{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook for MicropollDB data analysis\n",
    "\n",
    "For a start let's import all modules used in the analysis:\n",
    "- `mysql.connector` lets us connect to a MySql DB hosted on a server\n",
    "- `pandas` is used for data analysis (e.g. table operations)\n",
    "- `numpy` ...\n",
    "- `matplotlib` is for plotting various kind of scientific graphs, with *inline* command to plot figures directly into the notebook itself\n",
    "- `getpass` is used to request passwords interactively to avoid storing them in the notebook (e.g. for DB user login)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nibor/.local/share/virtualenvs/MPSchleiSediments-bSFBF4Xj/bin/python\r\n"
     ]
    },
    {
     "data": {
      "text/plain": "DataTransformerRegistry.enable('default')"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import getpass\n",
    "#pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, connect to the Database:\n",
    "- first I define a variable called `password`, that receives the output of the function `getpass`\n",
    "- then connect to the database using the `connect` function of `mysql.connector` and save it into a new variable called `connection`\n",
    "- the argument for `passwd` calls the content of the variable `password` which in turn calls the `getpass` function, which will open a prompt to type the password on execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = mysql.connector.connect(host = '192.124.245.26',\n",
    "                                    user = getpass.getpass(),\n",
    "                                    passwd = getpass.getpass(),\n",
    "                                    db = 'micropoll')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if the connection was established, just print the variable `connection`. It should give something like this:\n",
    "\n",
    "   ```<mysql.connector.connection.MySQLConnection at 0x..............>```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<mysql.connector.connection_cext.CMySQLConnection at 0x7f5060447e50>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the `particles` table using pandas `read_sql_query` function by giving it a SQL `SELECT ...` command together with the second required argument: the variable that contains our open connection to the DB: `connenction`. For better readability I define the SQL command beforehand and call it `query1` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = '''SELECT distinct\n",
    "        `p`.`Sample` AS `Sample`,\n",
    "        `p`.`IDParticles` AS `IDParticles`,\n",
    "        `s`.`Site_name` AS `Site_name`,\n",
    "        `s`.`GPS_LON` AS `GPS_LON`,\n",
    "        `s`.`GPS_LAT` AS `GPS_LAT`,\n",
    "        `s`.`Compartment` AS `Compartment`,\n",
    "        `s`.`Contributor` AS `Contributor`,\n",
    "        `s`.`Project` AS `Project`,\n",
    "        `p`.`Size_1_[µm]` AS `Size_1_[µm]`,\n",
    "        `p`.`Size_2_[µm]` AS `Size_2_[µm]`,\n",
    "        `p`.`Shape` AS `Shape`,\n",
    "        `p`.`Colour` AS `Colour`,\n",
    "        `pt`.`polymer_type` AS `polymer_type`,\n",
    "        `a`.`Library_entry` AS `library_entry`,\n",
    "        `s`.`Lab_blank` AS `lab_blank_ID`,\n",
    "        `s`.`IDSample` AS `sample_ID`,\n",
    "        `s`.`Sampling_weight_[kg]` AS `Sampling_weight_[kg]`,\n",
    "        `s`.`Fraction_analysed` AS `Fraction_analysed`      \n",
    "    FROM\n",
    "        ((((`particles` `p`\n",
    "        JOIN `samples` `s` ON ((`p`.`Sample` = `s`.`Sample_name`)))\n",
    "        JOIN `particles2analysis` `pa` ON ((`p`.`IDParticles` = `pa`.`IDParticles`)))\n",
    "        JOIN `analysis` `a` ON ((`pa`.`IDAnalysis` = `a`.`IDAnalysis`)))\n",
    "        JOIN `polymer_type` `pt` ON ((`a`.`Result` = `pt`.`IDPolymer_type`)))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MP = pd.read_sql_query(query1,connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop system caused contamination (PTFE, PV23, Parafilm) and certain dyes if they are no distinct indicators for synthetic polymers\n",
    "MP.drop(MP[(MP.polymer_type == 'Poly (tetrafluoro ethylene)')\n",
    "                        |(MP.library_entry == 'PV23')\n",
    "                        |(MP.library_entry == 'Parafilm')\n",
    "                        |(MP.library_entry == 'PR101')\n",
    "                        |(MP.library_entry == 'PB15')\n",
    "                        |(MP.library_entry == 'PW6')\n",
    "                        |(MP.library_entry == 'PBr29')\n",
    "                        |(MP.library_entry == 'PY17based')\n",
    "                        |(MP.library_entry == 'PY74')\n",
    "                        |(MP.library_entry == 'PB15 + PV23')\n",
    "                        |(MP.library_entry == 'PV23 + PB15')\n",
    "                        |(MP.library_entry == 'PB15 + TiO2')\n",
    "                        |(MP.library_entry == 'PB23 + PY17based')\n",
    "                        |(MP.library_entry == 'Parafilm/PE')\n",
    "                        |(MP.library_entry == 'PB15+PY17')\n",
    "                        |(MP.library_entry == 'PY17+PB15')\n",
    "                        |(MP.library_entry == 'PV23+PB15+TiO2')\n",
    "                        |(MP.library_entry == 'PB15+TiO2')\n",
    "                        |(MP.library_entry == 'TiO2+PB15')\n",
    "                        |(MP.library_entry == 'PB15+PV23')\n",
    "                        #|(MP.Sample == 'Blank_20.11.19')\n",
    "                        #|(MP.Sample == 'Blank_20.11.2019')\n",
    "                        #|(MP.Sample == 'Blank_20.11.19_IS')\n",
    "                     #   |(MP.Sample == 'Blank_5.11.19_IS_1')\n",
    "                      #  |(MP.Sample == 'Blank_5.11.19_IS_2')\n",
    "                      #  |(MP.Sample == 'Blank_6.11.19_1')\n",
    "                       # |(MP.Sample == 'Blank_6.11.19_2')\n",
    "].index, inplace=True)\n",
    "\n",
    "# Silicon was included here as it is not used during processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN with 1 in Fraction_analysed, assuming whole sample has been analysed when no value was provided\n",
    "MP.Fraction_analysed.fillna(1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 1 divided by Fraction_analysed (rounded to next integer) to get the factor each particle needs to be repeated by to extrapolate to whole sample\n",
    "# Then do the repetition using np. repeat and write it back into \"MP\"\n",
    "MP = MP.loc[np.repeat(MP.index.values,round(1 / MP.Fraction_analysed))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MP.IDParticles = MP.IDParticles.astype(str) + '_' + MP.groupby('IDParticles').cumcount().astype(str)  # .replace('0','') #--> replace 0 if not wanted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `particles` I also add a new column by simply addressing a column of it which does not exist yet and fill it the some calculation from 2 other columns (making the *geometric mean* from the size columns `Size_1_[µm]` and `Size_2_[µm]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MP['size_geom_mean']=np.sqrt(MP['Size_1_[µm]']*MP['Size_2_[µm]'])\n",
    "MP.set_index('IDParticles', inplace=True)\n",
    "MP = MP[MP['Size_1_[µm]']>=50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I combine certain values, if not needed for further analysis:\n",
    "- unspecific colours grouped in one ('unspecific')\n",
    "- shapes other than fibres grouped in one ('irregular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MP['Colour'].replace(['transparent', 'undetermined', 'white','non-determinable', 'grey', 'brown', 'black'],'unspecific',inplace=True) \n",
    "MP['Colour'].replace(['violet'],'blue',inplace=True) \n",
    "MP['Shape'].replace(['spherule','irregular','flake','foam','granule','undetermined'],'irregular',inplace=True) #non-fibrous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take env MP from dataset (without any blinds):\n",
    "env_MP = MP.loc[(MP.Compartment == 'sediment') & (MP.Site_name == 'Schlei') & (MP.Contributor == 27)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take IOW blinds from dataset:\n",
    "IOW_blind_MP = MP.loc[(MP.Site_name == \"lab\") & (MP.Project == \"BONUS MICROPOLL\") & (MP.Contributor == 27)]\n",
    "\n",
    "blinds_to_use = ['Blank_11.02.19',\n",
    "                 'Blank_5.11.19_IS_1',\n",
    "                 'Blank_5.11.19_IS_2',\n",
    "                 'Blank_6.11.19_1',\n",
    "                 'Blank_6.11.19_2',\n",
    "                 'Blank_20.11.19_IS',\n",
    "                 'Blank_20.11.19']\n",
    "\n",
    "IOW_blind_MP = IOW_blind_MP.loc[IOW_blind_MP.Sample.isin(blinds_to_use)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Samples from the environment and from blinds are for now handled together in `samples_MP`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                 Sample Site_name  GPS_LON  GPS_LAT Compartment  Contributor  \\\nIDParticles                                                                    \n46174_0      Schlei_S25    Schlei  9.88715  54.6143    sediment           27   \n45252_1       Schlei_S3    Schlei  9.60065  54.5087    sediment           27   \n\n                     Project  Size_1_[µm]  Size_2_[µm]      Shape      Colour  \\\nIDParticles                                                                     \n46174_0      BONUS MICROPOLL        615.0         26.0      fibre  unspecific   \n45252_1      BONUS MICROPOLL        238.0         51.0  irregular  unspecific   \n\n                              polymer_type                library_entry  \\\nIDParticles                                                               \n46174_0      Poly (ethylene terephthalate)  Poly(ethylene terephtalate)   \n45252_1                        Polystyrene                  Polystyrene   \n\n             lab_blank_ID  sample_ID  Sampling_weight_[kg]  Fraction_analysed  \\\nIDParticles                                                                     \n46174_0            1402.0       1215                  0.13                1.0   \n45252_1            1400.0       1190                  0.06                0.5   \n\n             size_geom_mean  \nIDParticles                  \n46174_0          126.451572  \n45252_1          110.172592  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sample</th>\n      <th>Site_name</th>\n      <th>GPS_LON</th>\n      <th>GPS_LAT</th>\n      <th>Compartment</th>\n      <th>Contributor</th>\n      <th>Project</th>\n      <th>Size_1_[µm]</th>\n      <th>Size_2_[µm]</th>\n      <th>Shape</th>\n      <th>Colour</th>\n      <th>polymer_type</th>\n      <th>library_entry</th>\n      <th>lab_blank_ID</th>\n      <th>sample_ID</th>\n      <th>Sampling_weight_[kg]</th>\n      <th>Fraction_analysed</th>\n      <th>size_geom_mean</th>\n    </tr>\n    <tr>\n      <th>IDParticles</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>46174_0</th>\n      <td>Schlei_S25</td>\n      <td>Schlei</td>\n      <td>9.88715</td>\n      <td>54.6143</td>\n      <td>sediment</td>\n      <td>27</td>\n      <td>BONUS MICROPOLL</td>\n      <td>615.0</td>\n      <td>26.0</td>\n      <td>fibre</td>\n      <td>unspecific</td>\n      <td>Poly (ethylene terephthalate)</td>\n      <td>Poly(ethylene terephtalate)</td>\n      <td>1402.0</td>\n      <td>1215</td>\n      <td>0.13</td>\n      <td>1.0</td>\n      <td>126.451572</td>\n    </tr>\n    <tr>\n      <th>45252_1</th>\n      <td>Schlei_S3</td>\n      <td>Schlei</td>\n      <td>9.60065</td>\n      <td>54.5087</td>\n      <td>sediment</td>\n      <td>27</td>\n      <td>BONUS MICROPOLL</td>\n      <td>238.0</td>\n      <td>51.0</td>\n      <td>irregular</td>\n      <td>unspecific</td>\n      <td>Polystyrene</td>\n      <td>Polystyrene</td>\n      <td>1400.0</td>\n      <td>1190</td>\n      <td>0.06</td>\n      <td>0.5</td>\n      <td>110.172592</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# append IOW blinds to environmental MP:\n",
    "samples_MP = pd.concat([env_MP,IOW_blind_MP],axis=0)\n",
    "samples_MP.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take IPF blinds from dataset:\n",
    "IPF_blank_MP = MP.loc[(MP.sample_ID.isin(samples_MP.lab_blank_ID))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For differentiation to env_MP their `size_geom_mean` is renamed to `blank_size_geom_mean`.\n",
    "IPF_blank_MP.rename(columns={'size_geom_mean':'blank_size_geom_mean'},inplace=True)\n",
    "IPF_blank_MP['Sample'] = IPF_blank_MP['Sample'].str.replace('Blank_','',1)  # the last option (called count, here \"1\") was added here because some of the IOW blinds have the sample name \"Blank_xxxxx\" and their corresponding IPF blanks have the sample name \"Blank_Blank_xxxxx\". So with count option set to \"1\", only the first occurence of \"Blank_\" is replace by \"\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IPF blanks\n",
    "\n",
    "The IPF blanks should be handled first, so I loop through each blank particle and identify the best matching counterpart in each sample and save its ID for later removal flagging. As this is a bit complicated, see the line by line comments..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_MP_copy = samples_MP.copy() #take copy to retain an unaltered version of samples_MP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For blank particle # 26681_0 :  Nothing to clean up.\n",
      "For blank particle # 26684_0 :  Nothing to clean up.\n",
      "For blank particle # 26686_0 :  Nothing to clean up.\n",
      "For blank particle # 26689_0 :  Nothing to clean up.\n",
      "For blank particle # 26690_0 :  Nothing to clean up.\n",
      "For blank particle # 26691_0 :  Nothing to clean up.\n",
      "For blank particle # 26692_0 :  Nothing to clean up.\n",
      "For blank particle # 26694_0 :  Nothing to clean up.\n",
      "For blank particle # 26709_0 :  Nothing to clean up.\n",
      "For blank particle # 26710_0 :  Nothing to clean up.\n",
      "For blank particle # 26712_0 :  Nothing to clean up.\n",
      "For blank particle # 26723_0 :  Nothing to clean up.\n",
      "For blank particle # 26725_0 :  Nothing to clean up.\n",
      "For blank particle # 26728_0 :  Nothing to clean up.\n",
      "For blank particle # 26733_0 :  Nothing to clean up.\n",
      "For blank particle # 28292_0 :  Nothing to clean up.\n",
      "For blank particle # 42143_0 :  Env. particle # 42107_0 was eliminated.\n",
      "For blank particle # 42158_0 :  Env. particle # 42107_1 was eliminated.\n",
      "For blank particle # 42160_0 :  Env. particle # 42106_0 was eliminated.\n",
      "For blank particle # 42162_0 :  Env. particle # 42097_0 was eliminated.\n",
      "For blank particle # 42728_0 :  Env. particle # 42819_0 was eliminated.\n",
      "For blank particle # 42940_0 :  Env. particle # 26498_0 was eliminated.\n",
      "For blank particle # 42941_0 :  Env. particle # 26579_0 was eliminated.\n",
      "For blank particle # 42946_0 :  Env. particle # 26534_0 was eliminated.\n",
      "For blank particle # 44097_0 :  Nothing to clean up.\n",
      "For blank particle # 44104_0 :  Nothing to clean up.\n",
      "For blank particle # 44105_0 :  Nothing to clean up.\n",
      "For blank particle # 44113_0 :  Env. particle # 43031_0 was eliminated.\n",
      "For blank particle # 44122_0 :  Env. particle # 44768_0 was eliminated.\n",
      "For blank particle # 44127_0 :  Env. particle # 44878_0 was eliminated.\n",
      "For blank particle # 44130_0 :  Env. particle # 44257_0 was eliminated.\n",
      "For blank particle # 44954_0 :  Env. particle # 44999_0 was eliminated.\n",
      "For blank particle # 46267_0 :  Env. particle # 46172_0 was eliminated.\n",
      "For blank particle # 46308_0 :  Env. particle # 45702_0 was eliminated.\n",
      "For blank particle # 46309_0 :  Env. particle # 45746_0 was eliminated.\n",
      "For blank particle # 46310_0 :  Nothing to clean up.\n"
     ]
    }
   ],
   "source": [
    "IPF_elimination_list = pd.DataFrame(columns = [ #create empty dataframe to collect particles-to-be-flagged in a loop\n",
    "    'ID_blank_particle',\n",
    "    'ID_sample_particle',\n",
    "    'Sample',\n",
    "    'polymer_type',\n",
    "    'Colour',\n",
    "    'Shape'])\n",
    "\n",
    "for label,row_content in IPF_blank_MP.iterrows(): #loop through the list of IPFblanks, particle by particle\n",
    "    #label contains the particle ID of the current blank particle, row_content contains the data of that particle\n",
    "    \n",
    "    current_blank_particle = samples_MP_copy.reset_index().merge(row_content.to_frame().transpose(), on=['Sample','polymer_type','Colour','Shape'], how='inner').set_index('IDParticles')\n",
    "    #current_blank_particle is basically extract of samples_MP_copy, where only particles which match the current blank particle in all fields after the \"on =\" are listed\n",
    "    #where a match is found, all fields of both lines are written as one long line.\n",
    "    #column names that exist in both of the merged dataframes get an appendix x or y\n",
    "    #we only need the entry of blank_size_geom_mean to be written as a new column in all lines (i.e. particles) that have the same phenotype as the current blank particle\n",
    "    #with this we can calculate the difference between the size_geom_mean of the particle and the blank_size_geom_mean \n",
    "    \n",
    "    if len(current_blank_particle) > 0: #there might be the case where no particles were found to match a specific blank particle, so we check for this with this if clause\n",
    "        \n",
    "        current_blank_particle['size_diff']=abs((current_blank_particle['size_geom_mean']-current_blank_particle['blank_size_geom_mean'])) #here we take the size difference as described above\n",
    "        \n",
    "        eliminee = pd.to_numeric(current_blank_particle['size_diff']).idxmin() #the particle that has the smallest difference in size_geom_mean to that of the current blank particle is our candidate for elimination, and we save its ID as 'eliminee'\n",
    "        \n",
    "        IPF_elimination_list = IPF_elimination_list.append(pd.DataFrame({ #now we keep a record entry of all details of the particle that gets eliminated and append it to the prepared data frame\n",
    "            'ID_blank_particle': label,\n",
    "            'ID_sample_particle': eliminee,\n",
    "            'Sample':  current_blank_particle.Sample.iloc[0],\n",
    "            'polymer_type':  current_blank_particle.polymer_type.iloc[0],\n",
    "            'Colour':  current_blank_particle.Colour.iloc[0],\n",
    "            'Shape':  current_blank_particle.Shape.iloc[0]\n",
    "        }, index=[0]), ignore_index=True)\n",
    "        \n",
    "        samples_MP_copy.drop([eliminee],inplace=True) #finally we drop the line of the eliminated particle from our particles dataframe, so we can't match it to another blank particle in the next round\n",
    "        \n",
    "        print('For blank particle #',label,': ','Env. particle #',eliminee,'was eliminated.')\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print('For blank particle #',label,': ','Nothing to clean up.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IOW blinds\n",
    "Now I separate the sample particles in blind MP particles as `IOWblind_MP` and environmental MP particles as `env_MP` and form an additional groupby object with them, grouped by sample as `env_MP_samplegrouped`. Note that the input here is the output of the blank particle matching process: `samples_MP_copy`. This is important, because we don't want to eliminate the same particles because of IPF blanks AND IOW blinds. Also, the IOW blind are samples themselves, so there might be particles eliminated because of IPF blanks as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "IOW_blind_MP = samples_MP_copy[samples_MP_copy.index.isin(IOW_blind_MP.index)].copy()\n",
    "# For differentiation to env_MP their `size_geom_mean` is renamed to `blind_size_geom_mean`.\n",
    "IOW_blind_MP.rename(columns={'size_geom_mean':'blind_size_geom_mean'},inplace=True)\n",
    "\n",
    "env_MP = samples_MP_copy[~samples_MP_copy.index.isin(IOW_blind_MP.index)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I form the blind phenotypes by grouping the particles by `polymer_type`,`Colour` and `Shape`. To know the number of `blinds` we are dealing with I count how many unique entries can be found in the column that shows the sample name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "blind_PhTs = IOW_blind_MP.groupby(['polymer_type','Colour','Shape'])\n",
    "blinds = pd.unique(IOW_blind_MP.Sample).size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might have several blinds that replicate each other and together represent the contamination that comes from the process that was used to treat the samples. However, having multiple replicates of blinds, means also a total of X times as many blind particles as there would be correct.\n",
    "\n",
    "To account for this, we need to combine the blinds into a \"synthesised blind\" `syn_blind`. To decide which particles are the chosen ones to enter that synthetic blind, we divide them first into their phenotypes, then for each phenotype sort them by size and only select every n<sup>th</sup> particle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_blind = IOW_blind_MP[0:0]\n",
    "\n",
    "for group_name, group_content in blind_PhTs:\n",
    "    current_group = group_content.sort_values(by=['blind_size_geom_mean'],ascending=False)\n",
    "    syn_blind = syn_blind.append(current_group[0::blinds]) #Why is there no inplace option for pandas append?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run a similar procedure as the one above for the IPF blanks, to find out which particles need to be eliminated because a IOW blind particle tell us to. For detailed comments please refer to the inline comments in the IPF blank elimination above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now handling sample:  Schlei_S10\n",
      "Now handling sample:  Schlei_S10_15cm\n",
      "Now handling sample:  Schlei_S11\n",
      "Now handling sample:  Schlei_S13\n",
      "Now handling sample:  Schlei_S14\n",
      "Now handling sample:  Schlei_S15\n",
      "Now handling sample:  Schlei_S17\n",
      "Now handling sample:  Schlei_S19\n",
      "Now handling sample:  Schlei_S1_15cm\n",
      "Now handling sample:  Schlei_S2\n",
      "Now handling sample:  Schlei_S22\n",
      "Now handling sample:  Schlei_S23\n",
      "Now handling sample:  Schlei_S24\n",
      "Now handling sample:  Schlei_S25\n",
      "Now handling sample:  Schlei_S26\n",
      "Now handling sample:  Schlei_S27\n",
      "Now handling sample:  Schlei_S3\n",
      "Now handling sample:  Schlei_S30\n",
      "Now handling sample:  Schlei_S31\n",
      "Now handling sample:  Schlei_S5\n",
      "Now handling sample:  Schlei_S8\n"
     ]
    }
   ],
   "source": [
    "env_MP_copy = env_MP.copy()\n",
    "IOW_elimination_list = pd.DataFrame(columns = ['ID_blind_particle','Blind_sample','ID_sample_particle','Sample','polymer_type','Colour','Shape'])\n",
    "\n",
    "for sample_name, sample_group in env_MP.groupby('Sample'):\n",
    "    print('Now handling sample: ', sample_name)\n",
    "    \n",
    "    for label,row_content in syn_blind.iterrows():\n",
    "        current_blind_particle = sample_group.reset_index().merge(row_content.to_frame().transpose(), on=['polymer_type','Colour','Shape'], how='inner').set_index('IDParticles')\n",
    "        \n",
    "        if len(current_blind_particle) > 0:\n",
    "            current_blind_particle['size_diff']=abs((current_blind_particle['size_geom_mean']-current_blind_particle['blind_size_geom_mean']))\n",
    "            eliminee = pd.to_numeric(current_blind_particle['size_diff']).idxmin()\n",
    "            sample_group.drop([eliminee],inplace=True)\n",
    "            \n",
    "            IOW_elimination_list = IOW_elimination_list.append(pd.DataFrame({\n",
    "            'ID_blind_particle': label,\n",
    "            'Blind_sample': current_blind_particle.Sample_y.iloc[0],\n",
    "            'ID_sample_particle': eliminee,\n",
    "            'Sample':  env_MP_copy.loc[eliminee, 'Sample'],\n",
    "            'polymer_type':  current_blind_particle.polymer_type.iloc[0],\n",
    "            'Colour':  current_blind_particle.Colour.iloc[0],\n",
    "            'Shape':  current_blind_particle.Shape.iloc[0]\n",
    "             }, index= [0]), ignore_index=True)\n",
    "            \n",
    "            env_MP_copy.drop([eliminee],inplace=True)\n",
    "                    \n",
    "            #print('For blind particle #',label,': ','Env. particle #',eliminee,'was eliminated.')\n",
    "        \n",
    "        #else:\n",
    "            #print('For blind particle #',label,': ','Nothing to clean up.')\n",
    "env_MP_copy.to_csv('../csv/env_MP_clean_list_SchleiSediments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the records we write what has been eliminated to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IPF_elimination_list.to_csv('../csv/IPF_elimination_list_SchleiSediments.csv')\n",
    "IOW_elimination_list.to_csv('../csv/IOW_elimination_list_SchleiSediments.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-41-3e55984d7416>:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  flagged_particles.IDFlag[0:len(flagged_particles_IPF_IDParticles)] = 3\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-41-3e55984d7416>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0mflagged_particles\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mIDFlag\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mflagged_particles_IPF_IDParticles\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m \u001B[0mflagged_particles\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mIDFlag\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mflagged_particles_IPF_IDParticles\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0mflagged_particles\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreset_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdrop\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minplace\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "flagged_particles_IPF_IDParticles = IPF_elimination_list.ID_sample_particle\n",
    "flagged_particles_IPF_IDblank_particle = IPF_elimination_list.ID_blank_particle\n",
    "\n",
    "flagged_particles_IOW_IDParticles = IOW_elimination_list.ID_sample_particle\n",
    "flagged_particles_IOW_IDblank_particle = IOW_elimination_list.ID_blind_particle\n",
    "\n",
    "flagged_particles = pd.DataFrame({ #create empty dataframe to collect particles-to-be-flagged in a loop\n",
    "    'IDParticles': flagged_particles_IPF_IDParticles.append(flagged_particles_IOW_IDParticles),\n",
    "    'IDFlag': np.nan,\n",
    "    'IDblank_particle': flagged_particles_IPF_IDblank_particle.append(flagged_particles_IOW_IDblank_particle),\n",
    "    'IDContributor': 27\n",
    "})\n",
    "\n",
    "flagged_particles.IDFlag[0:len(flagged_particles_IPF_IDParticles)] = 3\n",
    "flagged_particles.IDFlag[len(flagged_particles_IPF_IDParticles):] = x\n",
    "\n",
    "flagged_particles.reset_index(drop = True, inplace = True)\n",
    "\n",
    "flagged_particles.to_csv('flagged_particles_SchleiSediments.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}