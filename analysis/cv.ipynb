{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # ignore warnings to avoid flooding the gridsearch output with repetitive messages (works for single cpu)\n",
    "import os\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"  # \"ignore::FutureWarning\"  # ignore warnings to avoid flooding the gridsearch output with repetitive messages (works for parallel)\n",
    "import time\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.plotting.backend = 'holoviews'\n",
    "import geopandas as gpd\n",
    "\n",
    "import sklearn\n",
    "print(f'sklearn verion: {sklearn.__version__}')\n",
    "from sklearn import clone\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "# from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MaxAbsScaler, MinMaxScaler, OneHotEncoder, FunctionTransformer\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    # AdaBoostRegressor,\n",
    "    # GradientBoostingRegressor,\n",
    "    # HistGradientBoostingRegressor,\n",
    ")\n",
    "from xgboost import XGBRegressor\n",
    "# from sklearn.neighbors import KNeighborsRegressor, RadiusNeighborsRegressor\n",
    "# from sklearn.svm import SVR\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "\n",
    "try:  # if on phy-server local modules will not be found if their directory is not added to PATH\n",
    "    import sys\n",
    "    sys.path.append(\"/silod7/lenz/MPSchleiSediments/analysis/\")\n",
    "    import os\n",
    "    os.chdir(\"/silod7/lenz/MPSchleiSediments/analysis/\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import prepare_data\n",
    "import geo\n",
    "from components import PCOA\n",
    "#from helpers import PipelineHelper, SMWrapper\n",
    "from plots import repNCV_score_plots, ncv_pie, ensemble_pred_histograms, per_err_agg_bar, scatter_chart\n",
    "from cv import compete_rep_ncv, compara_rep_ncv, best_scored, append_agg_cv_scores, aggregation, aggregate_folds_only, transform_score_df, process_results, make_setup_dict, make_header, rensembling, augment_predictions, performance\n",
    "from cv_helpers import generate_feature_sets, SelectFeatures, unnegate, inter_rank, fix_feature_combis, ensemble_predict, aggregate_predictions, iqm, check_testset_duplicates\n",
    "\n",
    "## Create set of keys in locals() originating from kernel initialisation and imports\n",
    "_lokeys = [k for k in locals().keys()]\n",
    "## later this could be used to pickle all user defined variables for complete workspace serialisation\n",
    "\n",
    "from settings import Config, shortnames, target, featurelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% loading data\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# cell magic to supress output. Comment it out to see output of this cell.\n",
    "\n",
    "# What happened so far: DB extract and blank procedure. Now import resulting MP data from csv\n",
    "mp_pdd = prepare_data.get_pdd()\n",
    "\n",
    "# Also import sediment data (sediment frequencies per size bin from master sizer export)\n",
    "grainsize_iow, grainsize_cau = prepare_data.get_grainsizes()[0:2]\n",
    "scor_iow = PCOA(grainsize_iow, 2)[0]\n",
    "scor_cau = PCOA(grainsize_cau, 2)[0]\n",
    "\n",
    "# ...some data wrangling to prepare particle domain data and sample domain data for MP and combine with certain sediment aggregates.\n",
    "sdd_iow = prepare_data.aggregate_SDD(mp_pdd)\n",
    "sdd_iow = prepare_data.additional_sdd_merging(sdd_iow, how='outer')\n",
    "sdd_iow = sdd_iow.merge(scor_iow, right_index=True, left_on='Sample', how='outer')\n",
    "sdd_iow = sdd_iow.replace({'Sample': shortnames}).sort_values(by='Sample')\n",
    "\n",
    "sdd_cau = pd.read_csv('../data/Metadata_CAU_sampling_log.csv', index_col=0)\n",
    "sdd_cau = sdd_cau.join(prepare_data.fix_gradistat_names(pd.read_csv('../data/GRADISTAT_CAU_vol_log-cau_closed.csv', index_col=0)), how='outer')\n",
    "sdd_cau = sdd_cau.merge(scor_cau, right_index=True, left_on='Sample', how='outer').reset_index()\n",
    "sdd_cau['Dist_Land'] = geo.get_distance_to_shore(sdd_cau['LON'], sdd_cau['LAT'])\n",
    "sdd_cau = prepare_data.impute_cau(sdd_cau)\n",
    "sdd_cau = geo.get_wwtp_influence(sdd_cau, tracks_file='../data/BAW_tracer_simulations.zip', file_postfix='_CAU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Split data into samples used for building the model and samples used for predicting.\n",
    "\n",
    "samples_with_response_and_predictor_data = sdd_iow.loc[~sdd_iow[target].isna()].set_index('Sample')\n",
    "samples_with_only_predictor_data = sdd_iow.loc[sdd_iow[target].isna()]\n",
    "samples_with_only_predictor_data = pd.concat([samples_with_only_predictor_data, sdd_cau.drop('Date',axis=1)]).set_index('Sample')\n",
    "samples_with_only_predictor_data = samples_with_only_predictor_data.drop('S09')  # apparently there is no MP nor sediment data for S09, so drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Samples which are not suitable for (\"hydrodynamic outliers\") are moved from modelling data to prediction data\n",
    "droplist = ['S32','S05']\n",
    "samples_with_only_predictor_data = pd.concat([samples_with_only_predictor_data, samples_with_response_and_predictor_data.loc[droplist,:]])\n",
    "samples_with_response_and_predictor_data = samples_with_response_and_predictor_data.drop(droplist)\n",
    "all_samples = pd.concat([samples_with_response_and_predictor_data, samples_with_only_predictor_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Limit dataframes to features (predictors) to be used in the model. Beware: depending on the preprocessing steps not all features might be used.\n",
    "model_X = samples_with_response_and_predictor_data[featurelist]\n",
    "model_y = samples_with_response_and_predictor_data[target]\n",
    "pred_X = samples_with_only_predictor_data[featurelist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternatively to above four cells read samples data from the prediction output of a previous model run\n",
    "## This allows to replace the second model stage (originally geospatial interpolation),\n",
    "## by a re-run of the first NCV pipeline (using the ~200 predicted points as input and only\n",
    "## predictors which are available at the grid points of the geospatial raster for second prediction)\n",
    "# droplist = ['S32','S05']\n",
    "# output_from_1st_model = pd.read_csv('../data/exports/models/predictions/20230403_233901_Concentration_predictions.csv').set_index('Sample').drop(droplist)\n",
    "# model_X = output_from_1st_model[featurelist]\n",
    "# model_y = output_from_1st_model[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined test set optionally to be used instead of CV:\n",
    "# test_set = ('S30', 'S03', 'S15', 'S06', 'S31', 'S25', 'S20')  # possible samples to use as a predefined test set: ordered by relevance\n",
    "# test_set_size = 7  # Requires int, should be 0 < test_set_size <= len(test_set), for using the n first samples as test_set\n",
    "# test_set = test_set[0:test_set_size]\n",
    "# test_set = model_X.index.isin(test_set).astype(int) - 1  # returns array of len(model_X.index) with -1 for training samples and 0 for testing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = None  # this is just needed for reporting as long as scaler is manually switched outside the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data using a scaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# model_X.loc[:] = scaler.fit_transform(model_X)\n",
    "# pred_X.loc[:] = scaler.transform(pred_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Check some basic statistics of the target variable\n",
    "\n",
    "# model_y.describe()\n",
    "# model_y.hist()\n",
    "# model_X.info()\n",
    "# model_X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom preprocessing functions to be used in the model pipeline\n",
    "Create a custom feature selector which creates exhaustively all combinations\n",
    "of available features to be tested as individual feature sets.\n",
    "It respects constrictions on non-allowed combinations, which can be defined\n",
    "by the user in Config.mutual_exclusive and Config.exclusive_keywords to sace\n",
    "computation time by not testing meaningless combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feat = (2,5)  # allowed number of features:\n",
    "                  #     if int: all possible combinations of this length will be created\n",
    "                  #     if tuple (min, max): all possible combinations of length min upt to length max will be created\n",
    "                  #     if 'all' all possible combinations of all possible lengths will be created\n",
    "feature_candidates_list = generate_feature_sets(model_X, Config.mutual_exclusive, Config.exclusive_keywords, num_feat=num_feat, n_jobs=1, save=True)\n",
    "\n",
    "CustomFeatureSelector = FunctionTransformer(SelectFeatures)#, feature_names_out='one-to-one')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Pipeline and parameter grid for model selection, see here for inspiration: https://towardsdatascience.com/getting-the-most-out-of-scikit-learn-pipelines-c2afc4410f1a\n",
    "\n",
    "PreProcessor = ColumnTransformer([\n",
    "      ('selector', CustomFeatureSelector, model_X.columns),\n",
    "      # ('imputer', SimpleImputer(), make_column_selector(dtype_include=np.number)),\n",
    "      # ('scaler', StandardScaler(), make_column_selector(dtype_include=np.number)),\n",
    "      # ('encoder', OneHotEncoder(), make_column_selector(dtype_include=object)),\n",
    "      ])\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('preprocessor', PreProcessor),\n",
    "    ('regressor', DummyRegressor())\n",
    "    ])\n",
    "\n",
    "preprocessor_params = [\n",
    "    {\n",
    "    'preprocessor__selector': [CustomFeatureSelector],\n",
    "    'preprocessor__selector__kw_args': [{'feature_set': i,\n",
    "                                         'feature_sets': feature_candidates_list\n",
    "                                        } for i in range(len(feature_candidates_list))],\n",
    "    \n",
    "    # 'preprocessor__scaler': [StandardScaler()],#MaxAbsScaler(), MinMaxScaler(), RobustScaler(), QuantileTransformer(), Normalizer()],\n",
    "    #    'preprocessor__scaler__with_mean': [True],\n",
    "    #    'preprocessor__scaler__with_std': [True],\n",
    "    }\n",
    "]\n",
    "\n",
    "regressor_params = [\n",
    "    {\n",
    "     'regressor': [TweedieRegressor(max_iter=100000)],  # , warm_start': True)],\n",
    "         'regressor__power': [2],  # 'regressor__power': [0, 1, 1.2, 1.5, 1.9, 2, 3],\n",
    "         'regressor__alpha': [0], \n",
    "         'regressor__link': ['log'],#, 'identity', 'auto'],\n",
    "         # 'regressor__fit_intercept': [True, False],\n",
    "    },\n",
    "\n",
    "    {\n",
    "    'regressor': [XGBRegressor(random_state=np.random.RandomState(0), verbosity = 1)],\n",
    "        'regressor__booster': ['gblinear'],\n",
    "        'regressor__objective': ['reg:gamma'],\n",
    "        'regressor__n_estimators': [300],\n",
    "        'regressor__reg_alpha': [0], #[0, 0.1, 1],  # L1-regularisation\n",
    "        'regressor__reg_lambda': [0],  # L2-regularisation (OBS: it's called 'alpha' in sklearn GLM models)\n",
    "    },\n",
    "\n",
    "    {\n",
    "    'regressor': [XGBRegressor(random_state=np.random.RandomState(0), verbosity = 0)],\n",
    "        'regressor__booster': ['gbtree'],\n",
    "        'regressor__objective': ['reg:gamma'],\n",
    "        'regressor__n_estimators': [300],\n",
    "        'regressor__reg_alpha': [0],  # L1-regularisation\n",
    "        'regressor__reg_lambda': [0],  # L2-regularisation (OBS: it's called 'alpha' in sklearn GLM models)\n",
    "        'regressor__tree_method': ['exact'],  # OBS: 'exact' doesn't work with max_depth=0\n",
    "        'regressor__learning_rate': [0.1, 0.25], #alias for eta\n",
    "        'regressor__max_depth': [2, 3],\n",
    "        'regressor__min_child_weight': [5, 7],  # also a regularising parameter: higher values will stop splitting further, when childs have less then \n",
    "        # 'regressor__min_split_loss': [0], # alias for gamma\n",
    "        # 'regressor__grow_policy': ['depthwise', 'lossguide'],\n",
    "        # 'regressor__subsample': [0.5, 1.0],\n",
    "        # 'regressor__colsample_bytree': [1/3, 1.0],\n",
    "    # #     'regressor__colsample_bylevel': [0.5, 1.0],\n",
    "    # #     'regressor__colsample_bynode': [0.5, 1.S0],\n",
    "    },\n",
    "    \n",
    "    {\n",
    "     'regressor': [RandomForestRegressor(random_state=np.random.RandomState(0))],\n",
    "          'regressor__n_estimators': [10, 100, 300],\n",
    "          'regressor__max_depth': [None, 2, 4],\n",
    "          'regressor__max_features': [None, 1/3],\n",
    "          # 'regressor__min_samples_split': [2, 10],\n",
    "          # 'regressor__min_samples_leaf': [1, 3, 5],\n",
    "          # 'regressor__bootstrap': [True, False],\n",
    "          # 'regressor__oob_score': [True, False],\n",
    "          # 'regressor__warm_start': [True, False],\n",
    "    },\n",
    "]\n",
    "\n",
    "params = [{**dict_pre, **dict_reg} for dict_reg in regressor_params for dict_pre in preprocessor_params]\n",
    "\n",
    "# Replace the full featurset lists with:\n",
    "# -> (2,3)-combinations feature sets for linear models,\n",
    "# -> (5,5)-combinations feature sets for tree models\n",
    "Config.lin_combis = [2, 3]\n",
    "Config.tree_combis = [5]\n",
    "params = fix_feature_combis(params, feature_candidates_list, lin_combis=Config.lin_combis, tree_combis=Config.tree_combis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set options for NCV\n",
    "scoring, aggregation, repetitions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repetitions = 100  # number of times the whole NCV is repeated, each time using a different randon_state to make the train/test-splits. Enter 1 for single shuffled NCV, 0 (or False) for single non-shuffled NCV.\n",
    "inner_reps = 10 #repetitions/10  # repetitions of inner KFold: int or fracton of `repetitions` (will be cast to int and fixed to 1 if smaller)\n",
    "folds = (3, 3)  # tuple of n_folds (outer_folds, inner_folds) or int if number of folds for inner and out CVs should be the same.\n",
    "outer_strata = 3 #[0, 1000, 10000, 100000] #len(model_y)//folds[0]  # Number of strata for outer StratifiedKFold splitting. Useful values range between n_folds and n_samples//n_folds. Use False in either or both n_strata values for unstratified splitting. E.g. `strata=(len(model_y)//n_folds[0], False)`  --> only stratify outer folds not inner (as done by https://doi.org/10.1186/1758-2946-6-10).\n",
    "inner_strata = 3 #[0, 1000, 10000, 100000] #(len(model_y) - outer_strata)//folds[1]  # like outer strata but for inner folds. OBS: n_samples in inner CV is already reduced by outer_strata, so inner_strata is calculated from the remaining samples.\n",
    "stratification_mode = 'quantile'  # 'quantile', 'interval', or None\n",
    "setup = make_setup_dict(repeats=(repetitions,inner_reps), folds=folds, strata=(outer_strata, inner_strata), stratification_mode=stratification_mode, n_jobs=(1,-1), verbosity=(0,1))\n",
    "\n",
    "scorers = [dict(zip(Config.scorers,t)) for t in zip(*Config.scorers.values())][1]  # dict 'scorers' will contain the string representation of negated scorers used by GridSearchCV\n",
    "\n",
    "Config.refit_scorer = 'R2'  # must be one of the keys in Config.scorers\n",
    "Config.select_best = 'median'  # 'mean', 'median', 'iqm'\n",
    "Config.ncv_mode = 'comp'+'ara'+'tive'  # 'competitive' (run all activated models in grid against each other) or 'comparative' (run all activated models in separate sequential repeated NCV runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_savestamp = '20231004_153340' # '20230831_083952'  # enter savestamp of model run here, write None to train a new model\n",
    "\n",
    "if load_savestamp is not None:\n",
    "    ## Load serialised results instead of training anew\n",
    "\n",
    "    sp = Path(f'../data/exports/models/serialised/{load_savestamp}/')\n",
    "    NCV = pd.read_pickle(sp/f'NCV_{load_savestamp}.pkl')\n",
    "    with open(sp/f'starttime_{load_savestamp}.pkl', 'rb') as f:\n",
    "        starttime = pickle.load(f)\n",
    "    with open(sp/f'time_needed_{load_savestamp}.pkl', 'rb') as f:\n",
    "        time_needed = pickle.load(f)\n",
    "    with open(sp/f'setup_{load_savestamp}.pkl', 'rb') as f:\n",
    "        setup = pickle.load(f)\n",
    "    try:\n",
    "        with open(sp/f'Config_{load_savestamp}.pkl', 'rb') as f:\n",
    "            Config = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print('No Config file found. Adjust Config manually according to logs (e.g. Config.select_best, Config.refit_scorer, Config.ncv_mode)')\n",
    "        \n",
    "        \n",
    "else:\n",
    "    if repetitions < 1:\n",
    "        ## Run single NCV without shuffle split\n",
    "        raise NotImplementedError('Running single NCV without shuffle split corresponds to the old version before repeated NCV was included and was not re-implemented after rewriting codebase for the new rep_ncv function.')\n",
    "    if Config.ncv_mode == 'competitive':\n",
    "        NCV, setup, starttime, time_needed = compete_rep_ncv(pipe, params, model_X, model_y, scorers, setup)\n",
    "    elif Config.ncv_mode == 'comparative':\n",
    "        NCV, setup, starttime, time_needed = compara_rep_ncv(pipe, params, model_X, model_y, scorers, setup)\n",
    "    else: raise ValueError(f'No valid NCV mode: {Config.ncv_mode}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV results postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take an unaltered backup for pickling\n",
    "NCV_bak = NCV.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Add ranking to outer fold test scores, all together, not regarding the repetitions\n",
    "NCV = inter_rank(NCV, Config.refit_scorer)\n",
    "## Get additional scorings, get feature names, etc. and arrange into a dataframe\n",
    "NCV = process_results(NCV, model_X, model_y, refitOnAll=True)\n",
    "## When run in comparative mode: append results that would have come out of an equivalent run in competitive mode\n",
    "NCV = rensembling(NCV)  # OBS: the intra- and inter rep rankings are not created again for the new block of the competitive run\n",
    "## Reversing the higher-is-better sklearn negated scores\n",
    "NCV = unnegate(NCV, scorers)\n",
    "## Sort NCV\n",
    "NCV.sort_index(level=[0, 1, 3], sort_remaining=False, inplace=True)\n",
    "## Check for duplicates in test sets\n",
    "dup_testsets, testset_length_freq = check_testset_duplicates(NCV)  # TODO: add this output to the printed header\n",
    "## Generate report header\n",
    "savestamp = starttime.strftime(\"%Y%m%d_%H%M%S\")\n",
    "header = make_header(NCV, setup, savestamp, time_needed, droplist, model_X, num_feat, feature_candidates_list, scaler, regressor_params, dup_testsets)\n",
    "## Display results\n",
    "print(header)\n",
    "pd.set_option('display.max_colwidth', None, 'display.max_columns', None)\n",
    "NCV.drop(['fit_time', 'score_time', 'estimator', 'estimator_refit_on_all'], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulate shorter runs (fewer repetitions) and repeat score aggregation\n",
    "scored_multi = pd.DataFrame()\n",
    "for reps in range(1, setup['repeats'][0]+1):\n",
    "    scored_short = aggregation(NCV.query(f'NCV_repetition <= {reps-1}'), setup, r=reps)\n",
    "    scored_short = pd.concat({reps: scored_short}, names=['NCV_repetitions'])\n",
    "    scored_multi = pd.concat([scored_multi, scored_short])\n",
    "scored_long = transform_score_df(scored_multi)\n",
    "    \n",
    "## Also calculate the scores aggregated over folds within each repetition\n",
    "separate_reps_scored = aggregate_folds_only(NCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Plot score evolution\n",
    "score_chart = repNCV_score_plots(pd.concat([separate_reps_scored, scored_long]).reset_index())\n",
    "score_chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots above show, how the aggregated outer test scores are evolving while accumulating more and more repetitions.\n",
    "That means, that e.g. at value 5 on the x-axis, the corresponding y-values are aggregations of the repetitions 0 to 4.\n",
    "\n",
    "- `fold_aggregator`: aggregating (as median, iqm or mean, depending on color) the outer test scores of all outer folds within each repetition\n",
    "- `rep_aggregator`: aggregating (as median, iqm or mean, depending on stroke) pre-aggregated scores across all repetitions (up to the number of repetitions on the x-axis)\n",
    "\n",
    "Where `rep_aggregator` is 'none', outer test scores of all outer folds from all repetitions are aggregated together (up to the number of repetitions on the x-axis).\n",
    "Note that the aggregation type 'mean(mean)' is yielding the identical result as '(none(mean)' (i.e. per-repetition grouped aggregation is equal to the ungrouped aggregation), because the mean of means of equally sized groups is equal to the mean of all.\n",
    "Areas represent the span of one standard deviation among the per-repetition aggregated values.\n",
    "\n",
    "**The assembling of the ensemble should ressemble the assembling of the NCV generalisation score.** So, here you should decide which scorer and aggregation scheme seem reasonable to use.\n",
    "The chosen values will control which members will be elected into the prediction ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Control the way the ensemble is composed\n",
    "## `reduced_ensemble` allows for two different ways to put the ensemble together:\n",
    "##    reduced_ensemble = True:\n",
    "##        - selects the one model in each repetition, which comes closest (with its outer test score) to the aggregated score (using `fold_aggreagator`) of all models in this repetition\n",
    "##        - this will yield an ensemble of `n_outer_rep` members\n",
    "##        - when predicting, the `n_outer_rep` predictions are aggregated according to `rep_aggregator`\n",
    "##        - a reduced ensemble will result in lower cpu time needed for prediction\n",
    "##        - using a reduced ensemble indicates a higher level of trust that the target distribution in the dataset is repesentatitve for the unknown true distribution\n",
    "##    reduced_ensemble = False:\n",
    "##        - uses all \"best_estimator_\" models found\n",
    "##        - this will yield an ensemble of `n_outer_rep` * `n_outer_folds` members\n",
    "##        - when predicting, the `n_outer_folds` predictions in each outer repetition are aggregated first according to `folds_aggregator`\n",
    "##        - the `n_outer_rep` aggregates of the predictions are then aggregated according to `rep_aggregator`\n",
    "##        - using a full ensemble indicates a lower level of trust that the target distribution in the dataset is repesentatitve for the unknown true distribution\n",
    "reduced_ensemble = False  # boolean\n",
    "rep_aggregator = 'mean'  # Can be 'mean' or 'iqm' or 'median'. Will be used to aggregate the predictions\n",
    "fold_aggregator = 'median'  # Can be 'median', 'mean' or 'iqm'. Will be used to select models for the ensemble if reduced_ensemble=True, otherwise to aggregate the predictions within repetitions\n",
    "election_scorer = 'R2'  # Can be any of the scorers in the plot above. Will be used together with fold_aggregator to select one model per repetition if reduced_ensemble=True, neglected otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "When predicting, each member will cast its individual vote. Then these votes will be aggregated into a single final predicted value, using the `rep_aggregator` if not None, and the `fold_aggregator` otherwise.\n",
    "This procedure ensures that the ways of creating the final generalisation score and the final predictions are as identical as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assembling the ensemble...\n",
    "\n",
    "## select model run to use for prediction\n",
    "sel = 'Competitive'  # substring to be search for in the `run_with` index level (needs to distinguish the wanted run from the rest)\n",
    "selRun = NCV.filter(regex=re.compile(sel, re.IGNORECASE), axis=0) if Config.ncv_mode == 'comparative' else NCV\n",
    "\n",
    "if reduced_ensemble:  # if `reduced_ensemble` is True, use fold_aggregator and election_scorer to select one model per repetition\n",
    "    diff = selRun.groupby('NCV_repetition', as_index=False)[f'test_{election_scorer}'].apply(lambda x: np.abs(x - Config.aggregators[fold_aggregator](x))).droplevel(None)\n",
    "    idx = diff.groupby('NCV_repetition').idxmin()\n",
    "else:\n",
    "    idx = selRun.index\n",
    "esti = pd.concat([\n",
    "    selRun.loc[idx][['outer_test_set_samples', 'estimator_refit_on_all', 'features', 'regressor']],\n",
    "    selRun.loc[idx].filter(regex='regressor__')\n",
    "], axis=1)\n",
    "\n",
    "if 'regressor__booster' in esti.columns:\n",
    "        esti.regressor[~esti.regressor__booster.isna()] = esti.regressor[~esti.regressor__booster.isna()] + ' ' + esti.regressor__booster[~esti.regressor__booster.isna()]\n",
    "        esti.drop(columns='regressor__booster', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##Make pie charts to see what the ensemble is composed of\n",
    "pie_chart, pie_chart_df = ncv_pie(esti, show_top=5)\n",
    "pie_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the predictions...\n",
    "## (returning the individual predicted values of all members for all samples)\n",
    "pred_y_seenSamples_df = ensemble_predict(esti, model_X)  # ...for original MP samples\n",
    "pred_y_unseenSamples_df = ensemble_predict(esti, pred_X)  # ...for other samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Plot a histogram of how many members predicted what values (split into stacked colors for different model types)\n",
    "## First view shows all stations combined: change to another Sample on the dropdon below, to see single sample histograms\n",
    "ensemble_pred_histograms(esti, pred_y_seenSamples_df, model_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if reduced_ensemble:  # currently only implemented in a way which makes sense when the ensemble was already reduced to on member per repetition\n",
    "    ## Comparing different ways to aggregate the votes of the ensemble members\n",
    "    pred_agg_df = pd.DataFrame(  # makes a df with samples as rows and aggregators as columns\n",
    "\n",
    "        np.array([agg(s)\n",
    "                  for _, s in pred_y_seenSamples_df.items()\n",
    "                  for agg in Config.aggregators.values()\n",
    "                  ]\n",
    "                ).reshape(\n",
    "                    pred_y_seenSamples_df.shape[1],\n",
    "                    len(Config.aggregators)),\n",
    "       index=pred_y_seenSamples_df.columns,\n",
    "       columns=Config.aggregators.keys()\n",
    "      ).join(model_y)\n",
    "\n",
    "    ## Compare their performances in a table\n",
    "    pd.concat([\n",
    "        performance(pred_agg_df[target],\n",
    "                    pred_agg_df[agg_name]\n",
    "                   ).drop(columns='Info'\n",
    "                   ).rename(columns={'Value': agg_name})\n",
    "        for agg_name in Config.aggregators.keys()\n",
    "    ], axis=1).T.drop_duplicates().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if reduced_ensemble:  # currently only implemented in a way which makes sense when the ensemble was already reduced to on member per repetition\n",
    "    ## Make a plot of the percentage error of the aggregations\n",
    "    per_err_agg_bar(pred_agg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregate the ensemble votes into the final prediction.\n",
    "# rep_aggregator = ...  # only activate to try out prediction aggregations deviating from how the score aggregation was made\n",
    "\n",
    "pred_y_seenSamples, pred_y_unseenSamples = [aggregate_predictions(\n",
    "    df,\n",
    "    (Config.aggregators[rep_aggregator], Config.aggregators[fold_aggregator]) if reduced_ensemble else (Config.aggregators[rep_aggregator],),\n",
    "    target,\n",
    ") for df in [\n",
    "    pred_y_seenSamples_df,\n",
    "    pred_y_unseenSamples_df]\n",
    "                             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o = augment_predictions(pred_y_seenSamples, samples_with_response_and_predictor_data, target=target, kind='observed')\n",
    "df_p = augment_predictions(pred_y_unseenSamples, samples_with_only_predictor_data, kind='predicted')\n",
    "df_a = pd.concat([df_o, df_p])\n",
    "\n",
    "## For samples previously dropped (outliers with existing observed target value), replace the predicted value with the original observed one\n",
    "df_a.loc[df_a.Type=='observed', 'outlier_excl'] = False\n",
    "for o in droplist:\n",
    "    df_a.loc[o, f'{target}_observed'] = samples_with_only_predictor_data.loc[o, target]\n",
    "    df_a.loc[o, 'Type'] = 'observed'\n",
    "    df_a.loc[o, 'outlier_excl'] = True\n",
    "\n",
    "df_a.insert(0,target, df_a[f'{target}_observed'].combine_first(df_a[f'{target}_predicted']))\n",
    "\n",
    "if target == 'Concentration':  # ...add MassConcentration columns also\n",
    "    df_a.insert(3, 'MassConcentration_observed', all_samples.MassConcentration)\n",
    "    for k, v in Config.massConc_from_numConc.items():  # iterating through dict, currently only one key: 'MassConcentration_predicted'\n",
    "        df_a.insert(3, k, prepare_data.patsy_transform(v, df_a))  # OBS: MassConc is in µg/kg. Divide by 1e9 to get MassConc in kg MP per kg dry sediment!\n",
    "    df_a.insert(3, 'MassConcentration', df_a[k])  # alike 'Concentration': make combined column for 'MassConcentration'\n",
    "    df_a.loc[df_a.Type=='observed', 'MassConcentration'] = df_a.loc[df_a.Type=='observed', 'MassConcentration_observed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Quick check predicted vs. observed:\n",
    "id_max, id_min = df_a[target].idxmax(), df_a[target].idxmin()\n",
    "print('Maximum predicted value: ', f'Sample {id_max}: ', round(df_a.loc[id_max, f\"{target}_predicted\"]), f' (observed: {round(df_a.loc[id_max, f\"{target}_observed\"])}).')\n",
    "print('Minimum predicted value: ', f'Sample {id_min}: ', round(df_a.loc[id_min, f\"{target}_predicted\"]), f' (observed: {round(df_a.loc[id_min, f\"{target}_observed\"])}).')\n",
    "\n",
    "pred_vs_obs =scatter_chart(df_a.join(sdd_iow.set_index('Sample').regio_sep).dropna().drop(droplist),\n",
    "                     f'{target}_observed', f'{target}_predicted', 'regio_sep',\n",
    "                     xscale='log', yscale='log',\n",
    "                     # xtransform = 'log', ytransform= 'log', \n",
    "                     # equal_axes= True,\n",
    "                     identity = True,\n",
    "                     # reg= 'linear',\n",
    "                     labels = 'Sample',\n",
    "                     width=300, height=300\n",
    "                    )[0]\n",
    "pred_vs_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single model (for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Alternative to ensembling:\n",
    "## The classical approach to NCV is, that it is followed by a non-nested GS-CV on the same param grid.\n",
    "## The `best_estimator_` of this can be directly used for prediction. Its score is discarded as it is optimistically biased.\n",
    "singleGSCV = GridSearchCV(\n",
    "        pipe,\n",
    "        params,\n",
    "        scoring=scorers,\n",
    "        refit=best_scored,\n",
    "        cv=setup['cv_scheme'][1],\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        ).fit(model_X, model_y)\n",
    "\n",
    "singleGSCV_bak = clone(singleGSCV)\n",
    "append_agg_cv_scores(singleGSCV.cv_results_, 'median')\n",
    "append_agg_cv_scores(singleGSCV.cv_results_, 'iqm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singleGSCV_df = pd.DataFrame(singleGSCV.cv_results_)\n",
    "singleGSCV_df = unnegate(singleGSCV_df, scorers)\n",
    "\n",
    "sb = Config.select_best\n",
    "rf = Config.refit_scorer\n",
    "bi = singleGSCV_df[f'{sb}_test_{rf}'].idxmax()\n",
    "bi_ = singleGSCV.best_index_\n",
    "\n",
    "best_model_class = singleGSCV.best_estimator_.get_params()['regressor'].__class__.__name__\n",
    "best_model_hyperparams = singleGSCV_df.loc[bi_, [c for c in singleGSCV_df.columns if 'regressor__' in c]].dropna()#.reset_index()\n",
    "\n",
    "best_feat_sets_dict = singleGSCV.best_estimator_.get_params()['preprocessor__selector__kw_args']\n",
    "best_feat_set_idx = best_feat_sets_dict['feature_set']\n",
    "allowed_feat_sets = best_feat_sets_dict['feature_sets']\n",
    "best_feat_set = allowed_feat_sets[best_feat_set_idx]\n",
    "\n",
    "overoptimistic_scores = pd.DataFrame({\n",
    "    s: [singleGSCV_df.loc[bi_, f'{a}_test_{s}']\n",
    "        for a in Config.aggregators.keys()]\n",
    "    for s in Config.scorers.keys()\n",
    "}, index=Config.aggregators.keys())\n",
    "\n",
    "singleModel_predictions = pd.concat([\n",
    "    pd.Series(singleGSCV.best_estimator_.predict(model_X), index=model_X.index),\n",
    "    pd.Series(singleGSCV.best_estimator_.predict(pred_X), index=pred_X.index),\n",
    "]).rename(f'{target}_predictedBySingleModel')\n",
    "\n",
    "print(f'''\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------------------------\n",
    "                      Best single model configuration is:\n",
    "--------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Model class:      {best_model_class}\n",
    "Hyper parameters: { {k.split('__')[1]: v for k, v in best_model_hyperparams.to_dict().items()} }\n",
    "\n",
    "Feature set (index {best_feat_set_idx} of {len(allowed_feat_sets)}): {best_feat_set}\n",
    "\n",
    "    max       {sb}_test_{rf} (found at row index {bi} with rank {singleGSCV_df.loc[bi, f'rank_by_{sb}_test_{rf}']}): {singleGSCV_df.loc[bi, f'{sb}_test_{rf}']}\n",
    "\"best_index_\" {sb}_test_{rf} (found at row index {bi_} with rank {singleGSCV_df.loc[bi_, f'rank_by_{sb}_test_{rf}']}): {singleGSCV_df.loc[bi_, f'{sb}_test_{rf}']}\n",
    "\n",
    "\n",
    "Probably overoptimistic score of single best model (aggregated over all folds of the non-nested CV):\n",
    "\n",
    "{overoptimistic_scores}\n",
    "--------------------------------------------------------------------------------------------------------------------------------------\n",
    "      ''')\n",
    "\n",
    "print('    best_estimator_:')\n",
    "singleGSCV.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Include the single model predictions into the predictions df\n",
    "df_a.insert(2, singleModel_predictions.name, singleModel_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare prediction by \n",
    "PSM_vs_PEM =scatter_chart(df_a.join(sdd_iow.set_index('Sample').regio_sep).dropna().drop(droplist),\n",
    "                     f'{target}_predicted', f'{target}_predictedBySingleModel', 'regio_sep',\n",
    "                     # xscale='log', yscale='log',\n",
    "                     # xtransform = 'log', ytransform= 'log', \n",
    "                     equal_axes= True,\n",
    "                     identity = True,\n",
    "                     # reg= 'linear',\n",
    "                     labels = 'Sample',\n",
    "                     width=600, height=600\n",
    "                    )[0]\n",
    "PSM_vs_PEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### only save if not loaded from already serialised model:\n",
    "if load_savestamp is None:\n",
    "    ## Save model settings and results report\n",
    "    fp = '../data/exports/models/'\n",
    "    fn = fp + 'model_NCV_result.csv'\n",
    "    with open(fn, mode='a' if Path(fn).exists() else 'w', encoding='utf-8') as f:\n",
    "        f.write(header)\n",
    "    NCV.drop(['fit_time', 'score_time', 'estimator'], axis=1).to_csv(fn, mode='a', sep=';')\n",
    "    \n",
    "    ## Save figures\n",
    "    score_chart.save(f'../data/exports/plots/repNCV_score_evolution_{savestamp}.html')\n",
    "    pie_chart.save(f'../data/exports/plots/repNCV_ensemble_composition_{savestamp}.html')\n",
    "    pred_vs_obs.save(f'../data/exports/plots/repNCV_pred_vs_obs_{savestamp}.html')\n",
    "    \n",
    "    ## Save predictions\n",
    "    df_a.to_csv(f'{fp}/predictions/{savestamp}_{target}_predictions.csv')\n",
    "\n",
    "    ## Serialise results\n",
    "    sp = Path(fp)/'serialised'/savestamp\n",
    "    sp.mkdir(parents=True, exist_ok=True)\n",
    "    NCV_bak.to_pickle(sp/f'NCV_{savestamp}.pkl')\n",
    "    with open(sp/f'starttime_{savestamp}.pkl', 'wb') as f:\n",
    "        pickle.dump(starttime, f)\n",
    "    with open(sp/f'time_needed_{savestamp}.pkl', 'wb') as f:\n",
    "        pickle.dump(time_needed, f)\n",
    "    with open(sp/f'singleModelBestEstimator_{savestamp}.pkl', 'wb') as f:\n",
    "        pickle.dump(singleGSCV.best_estimator_, f)\n",
    "    with open(sp/f'setup_{savestamp}.pkl', 'wb') as f:\n",
    "        pickle.dump(setup, f)\n",
    "    with open(sp/f'Config_{savestamp}.pkl', 'wb') as f:\n",
    "        pickle.dump(Config, f)\n",
    "    with open(sp/f'locals_{savestamp}.pkl', 'wb') as f:\n",
    "        _lokeys.append('f')\n",
    "        key_list = [k for k in locals().keys() if k not in _lokeys and not k.startswith('_') ]\n",
    "        locdi = {}\n",
    "        for k in key_list:\n",
    "            locdi[k] = locals()[k]\n",
    "        pickle.dump(locdi, f)  # pickling might need to be done with dill instead..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(360000)  # to keep the kernel running for 100 h after model run, when no client is connected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MPSchleiSediments-venv",
   "language": "python",
   "name": "mpschleisediments-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
