{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.plotting.backend = 'holoviews'\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "from sklearn.utils import parallel_backend\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "# from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "from sklearn.model_selection import GridSearchCV, LeavePOut, LeaveOneOut, cross_validate, KFold\n",
    "\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MaxAbsScaler, MinMaxScaler, OneHotEncoder, FunctionTransformer\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    AdaBoostRegressor,\n",
    "    GradientBoostingRegressor,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsRegressor, RadiusNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn import set_config\n",
    "# set_config(transform_output='pandas')  # only works for sklearn >= 1.2\n",
    "\n",
    "import prepare_data\n",
    "from components import PCOA\n",
    "#from helpers import PipelineHelper, SMWrapper\n",
    "from settings import Config, shortnames, target\n",
    "from plots import scatter_chart\n",
    "from cv import generate_feature_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% loading data\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# cell magic to supress output. Comment it out to see output of this cell.\n",
    "\n",
    "# What happened so far: DB extract and blank procedure. Now import resulting MP data from csv\n",
    "mp_pdd = prepare_data.get_pdd()\n",
    "\n",
    "# Also import sediment data (sediment frequencies per size bin from master sizer export)\n",
    "grainsize_iow, grainsize_cau = prepare_data.get_grainsizes()[0:2]\n",
    "scor_iow = PCOA(grainsize_iow, 2)[0]\n",
    "scor_cau = PCOA(grainsize_cau, 2)[0]\n",
    "\n",
    "# ...some data wrangling to prepare particle domain data and sample domain data for MP and combine with certain sediment aggregates.\n",
    "sdd_iow = prepare_data.aggregate_SDD(mp_pdd)\n",
    "sdd_iow = prepare_data.additional_sdd_merging(sdd_iow, how='outer')\n",
    "sdd_iow = sdd_iow.merge(scor_iow, right_index=True, left_on='Sample', how='outer')\n",
    "sdd_iow = sdd_iow.replace({'Sample': shortnames}).sort_values(by='Sample')\n",
    "\n",
    "sdd_cau = pd.read_csv('../data/Metadata_CAU_sampling_log.csv', index_col=0).join(pd.read_csv('../data/GRADISTAT_CAU_vol_log-cau_closed.csv', index_col=0), how='outer')\n",
    "sdd_cau = sdd_cau.merge(scor_cau, right_index=True, left_on='Sample', how='outer').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Additional variable generation (e.g. predictor derivatives)\n",
    " \n",
    "# sdd_iow['Dist_WWTP_revsq'] = ((1/sdd_iow['Dist_WWTP'])**3)*10000000000  # calculates the squared of the reversed Distance \n",
    "# sdd_iow['Dist_WWTP_revsq'] = (((sdd_iow['Dist_WWTP'].max()-sdd_iow['Dist_WWTP'])+1)**3)/100000000000  # calculates the squared of the reversed Distance \n",
    "# sdd_iow['Dist_WWTP_revsq'] = ((sdd_iow['Dist_WWTP'].max()/sdd_iow['Dist_WWTP'])**3)/100  # calculates the squared of the reversed Distance\n",
    "# sdd_iow['Dist_WWTP_revsq'] = ((sdd_iow['Dist_WWTP'].max()/sdd_iow['Dist_WWTP'])**2)  # calculates the squared of the reversed Distance\n",
    "\n",
    "# sdd_iow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Split data into samples used for building the model and samples used for predicting.\n",
    "\n",
    "model_data = sdd_iow.loc[~sdd_iow.Concentration.isna()].set_index('Sample')\n",
    "pred_data = sdd_iow.loc[sdd_iow.Concentration.isna()]\n",
    "pred_data = pd.concat([pred_data, sdd_cau.drop('Date',axis=1)]).set_index('Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Potential outlier exclusion\n",
    "# model_data = model_data.drop(['S10d', 'S02d', 'S04d', 'S01d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of features (predictors) to be used in the model. Beware: depending on the preprocessing steps not all features might be used.\n",
    "\n",
    "featurelist = [\n",
    "    # 'Depth',\n",
    "    # 'LON', 'LAT',\n",
    "    # 'Dist_Land',\n",
    "    # 'Dist_Marina',\n",
    "    'Dist_WWTP',\n",
    "    # 'WWTP_influence_as_tracer_mean_dist',\n",
    "    # 'WWTP_influence_as_cumulated_residence',\n",
    "    # 'WWTP_influence_as_mean_time_travelled',\n",
    "    'WWTP_influence_as_tracer_mean_dist__sed_18µm_allseasons_444',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__sed_18µm_allseasons_444',\n",
    "    'WWTP_influence_as_cumulated_residence__sed_18µm_allseasons_444',\n",
    "    'WWTP_influence_as_mean_time_travelled__sed_18µm_allseasons_444',\n",
    "    'WWTP_influence_as_tracer_mean_dist__nosed_18µm_spring_444',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__nosed_18µm_spring_444',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_18µm_spring_444',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_18µm_spring_444',\n",
    "    'WWTP_influence_as_tracer_mean_dist__nosed_allsizes_spring_444',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__nosed_allsizes_spring_444',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_allsizes_spring_444',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_allsizes_spring_444',\n",
    "    'WWTP_influence_as_tracer_mean_dist__sed_18µm_autumn_222',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__sed_18µm_autumn_222',\n",
    "    'WWTP_influence_as_cumulated_residence__sed_18µm_autumn_222',\n",
    "    'WWTP_influence_as_mean_time_travelled__sed_18µm_autumn_222',\n",
    "    'WWTP_influence_as_tracer_mean_dist__sed_18µm_autumn_444',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__sed_18µm_autumn_444',\n",
    "    'WWTP_influence_as_cumulated_residence__sed_18µm_autumn_444',\n",
    "    'WWTP_influence_as_mean_time_travelled__sed_18µm_autumn_444',\n",
    "    'WWTP_influence_as_tracer_mean_dist__nosed_18µm_autumn_222',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__nosed_18µm_autumn_222',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_18µm_autumn_222',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_18µm_autumn_222',\n",
    "    'WWTP_influence_as_tracer_mean_dist__nosed_allsizes_autumn_222',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__nosed_allsizes_autumn_222',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_allsizes_autumn_222',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_allsizes_autumn_222',\n",
    "    'WWTP_influence_as_tracer_mean_dist__nosed_allsizes_allseasons_444',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__nosed_allsizes_allseasons_444',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_allsizes_allseasons_444',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_allsizes_allseasons_444',\n",
    "    'WWTP_influence_as_tracer_mean_dist__sed_allsizes_allseasons_444',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__sed_allsizes_allseasons_444',\n",
    "    'WWTP_influence_as_cumulated_residence__sed_allsizes_allseasons_444',\n",
    "    'WWTP_influence_as_mean_time_travelled__sed_allsizes_allseasons_444',\n",
    "    'WWTP_influence_as_tracer_mean_dist__sed_18µm_allseasons_222',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__sed_18µm_allseasons_222',\n",
    "    'WWTP_influence_as_cumulated_residence__sed_18µm_allseasons_222',\n",
    "    'WWTP_influence_as_mean_time_travelled__sed_18µm_allseasons_222',\n",
    "    'WWTP_influence_as_tracer_mean_dist__nosed_18µm_allseasons_222',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__nosed_18µm_allseasons_222',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_18µm_allseasons_222',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_18µm_allseasons_222',\n",
    "    'WWTP_influence_as_tracer_mean_dist__nosed_18µm_spring_222',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__nosed_18µm_spring_222',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_18µm_spring_222',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_18µm_spring_222',\n",
    "    'WWTP_influence_as_tracer_mean_dist__nosed_allsizes_allseasons_222',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__nosed_allsizes_allseasons_222',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_allsizes_allseasons_222',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_allsizes_allseasons_222',\n",
    "    'WWTP_influence_as_tracer_mean_dist__nosed_allsizes_autumn_444',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__nosed_allsizes_autumn_444',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_allsizes_autumn_444',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_allsizes_autumn_444',\n",
    "    'WWTP_influence_as_tracer_mean_dist__sed_18µm_spring_444',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__sed_18µm_spring_444',\n",
    "    'WWTP_influence_as_cumulated_residence__sed_18µm_spring_444',\n",
    "    'WWTP_influence_as_mean_time_travelled__sed_18µm_spring_444',\n",
    "    'WWTP_influence_as_tracer_mean_dist__nosed_18µm_autumn_444',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__nosed_18µm_autumn_444',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_18µm_autumn_444',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_18µm_autumn_444',\n",
    "    'WWTP_influence_as_tracer_mean_dist__sed_18µm_spring_222',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__sed_18µm_spring_222',\n",
    "    'WWTP_influence_as_cumulated_residence__sed_18µm_spring_222',\n",
    "    'WWTP_influence_as_mean_time_travelled__sed_18µm_spring_222',\n",
    "    'WWTP_influence_as_tracer_mean_dist__sed_allsizes_allseasons_222',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__sed_allsizes_allseasons_222',\n",
    "    'WWTP_influence_as_cumulated_residence__sed_allsizes_allseasons_222',\n",
    "    'WWTP_influence_as_mean_time_travelled__sed_allsizes_allseasons_222',\n",
    "    'WWTP_influence_as_tracer_mean_dist__nosed_allsizes_spring_222',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__nosed_allsizes_spring_222',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_allsizes_spring_222',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_allsizes_spring_222',\n",
    "    # 'Dist_WWTP2',\n",
    "    # 'Dist_WWTP_revsq',\n",
    "    # 'MODE 1 (µm)',\n",
    "    # 'D10 (µm)',\n",
    "    # 'D50 (µm)',\n",
    "    # 'D90 (µm)',\n",
    "    # 'perc GRAVEL',\n",
    "    # 'perc SAND',\n",
    "    # 'perc MUD',\n",
    "    # 'perc CLAY',\n",
    "    # 'OM_D50',\n",
    "    # 'TOC',\n",
    "    # 'Hg',\n",
    "    # 'TIC',\n",
    "    # 'regio_sep',\n",
    "    'PC1',\n",
    "    # 'PC2'\n",
    "    ]\n",
    "model_X = model_data[featurelist]\n",
    "model_y = model_data[target]\n",
    "pred_X = pred_data[featurelist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data using StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "model_X.values[:] = scaler.fit_transform(model_X)\n",
    "pred_X.values[:] = scaler.transform(pred_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mutual exclusive list (list of lists detailing predictors that are not allowed together in one model candidate)\n",
    "\n",
    "mutual_exclusive = [\n",
    "    ['D50 (µm)', 'PC1'],\n",
    "    ['D50 (µm)', 'perc MUD'],\n",
    "    ['perc MUD', 'PC1'],\n",
    "    ['perc MUD', 'TOC'],\n",
    "]\n",
    "exclusive_keywords = ['WWTP']  # only feature_candidates sets with max 1 feature containing each keyword will be considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Check some basic statistics of the target variable\n",
    "\n",
    "# model_y.describe()\n",
    "# model_y.hist()\n",
    "# model_X.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom preprocessing functions to be used in the model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create exhaustive feature selector, using leave-p-out on columns labels to generate a boolean matrix.\n",
    "\n",
    "# min_features = 2  # minimum number of features to be used in the model\n",
    "\n",
    "# feature_candidates_list = []\n",
    "# for i in tqdm(range(1,len(model_X.columns)+1-min_features)):\n",
    "#     lpo = LeavePOut(p=i)\n",
    "#     # base_index = [False] * len(model_X.columns)\n",
    "#     for candidate_indices, _ in lpo.split(model_X.columns):\n",
    "#         feature_candidates = model_X.columns[candidate_indices]\n",
    "#         if any(all(pd.Series(ex_feats).isin(feature_candidates)) for ex_feats in mutual_exclusive):\n",
    "#             continue  # if all entries of any row in mutual_exclusive list are present in the current feature_candidates, then don't put them in the feature_candidates_list\n",
    "#         feature_candidates_list.append(feature_candidates)\n",
    "# if not any(all(pd.Series(ex_feats).isin(model_X.columns)) for ex_feats in mutual_exclusive):\n",
    "#     feature_candidates_list.append(model_X.columns)  # also append the set of all possible features\n",
    "\n",
    "feature_candidates_list = generate_feature_sets(model_X, mutual_exclusive, exclusive_keywords, num_feat=2, n_jobs=-1)\n",
    "\n",
    "def SelectFeatures(model_X, feature_set, feature_sets=feature_candidates_list):\n",
    "    return model_X.loc[:, feature_sets[feature_set]]\n",
    "\n",
    "CustomFeatureSelector = FunctionTransformer(SelectFeatures)#, feature_names_out='one-to-one')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Pipeline and parameter grid for model selection, see here for inspiration: https://towardsdatascience.com/getting-the-most-out-of-scikit-learn-pipelines-c2afc4410f1a\n",
    "\n",
    "PreProcessor = ColumnTransformer([\n",
    "      ('selector', CustomFeatureSelector, model_X.columns),\n",
    "      # ('imputer', SimpleImputer(), make_column_selector(dtype_include=np.number)),\n",
    "      # ('scaler', StandardScaler(), make_column_selector(dtype_include=np.number)),\n",
    "      # ('encoder', OneHotEncoder(), make_column_selector(dtype_include=object)),\n",
    "      ])\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('preprocessor', PreProcessor),\n",
    "    ('regressor', DummyRegressor())\n",
    "    ])\n",
    "\n",
    "preprocessor_params = [{\n",
    "    # 'preprocessor__selector': [CustomFeatureSelector],\n",
    "      'preprocessor__selector__kw_args': [{'feature_set':i} for i in range(len(feature_candidates_list))],\n",
    "    \n",
    "    # 'preprocessor__scaler': [StandardScaler()],#MaxAbsScaler(), MinMaxScaler(), RobustScaler(), QuantileTransformer(), Normalizer()],\n",
    "    #    'preprocessor__scaler__with_mean': [True],\n",
    "    #    'preprocessor__scaler__with_std': [True],\n",
    "    }]\n",
    "\n",
    "regressor_params = [\n",
    "    # {\n",
    "    # 'regressor': [DummyRegressor()],\n",
    "    #     'regressor__strategy': ['median'],\n",
    "    # },\n",
    "    \n",
    "    # {\n",
    "    # 'regressor': [SVR()],\n",
    "    #    'regressor__C': [0.1, 1.5],\n",
    "    #    'regressor__kernel': ['linear', 'rbf', 'poly'],\n",
    "    #    'regressor__degree': [2, 3, 4, 5],\n",
    "    # },\n",
    "\n",
    "    {\n",
    "    'regressor': [TweedieRegressor(max_iter=100000)],\n",
    "    #   'regressor__power': [2],\n",
    "      # 'regressor__power': [0, 1, 1.25, 1.5, 1.6, 1.7, 1.8, 1.9, 1.95, 1.99, 2, 3],\n",
    "      # 'regressor__alpha': [0, 1], \n",
    "      'regressor__link': ['log'],#, 'identity', 'auto'],\n",
    "      # 'regressor__fit_intercept': [True, False],\n",
    "    #   'regressor__warm_start': [True, False],\n",
    "    #    'regressor__fit_params__sample_weights': [None, model_data.loc[model_X.index, 'Mass'].to_numpy()],  # FIXME: fit_params seem not to be accepted from gridsearch params, only as argument in fit method directly...\n",
    "    }\n",
    "  \n",
    "    # {\n",
    "    # 'regressor': [RadiusNeighborsRegressor()],\n",
    "    #     'regressor__radius': [1000, 10000, 100000],\n",
    "    #     'regressor__weights': ['uniform', 'distance'],\n",
    "    #     'regressor__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    #     'regressor__leaf_size': [10, 20, 30, 40, 50],\n",
    "    # },\n",
    "    \n",
    "    # {\n",
    "    # 'regressor': [RandomForestRegressor()],\n",
    "    #      'regressor__n_estimators': [10, 50], #[20, 50, 100, 150],\n",
    "    #      'regressor__max_features': [None, 'sqrt', 'log2'],\n",
    "    #      'regressor__min_samples_split': [2, 10],\n",
    "    #      'regressor__min_samples_leaf': [1, 5],\n",
    "    #      'regressor__bootstrap': [True],\n",
    "    #      'regressor__max_depth': [None, 5, 50],\n",
    "    #     #  'regressor__warm_start': [True, False],\n",
    "    # },\n",
    "    \n",
    "    # {\n",
    "    # 'regressor': [GradientBoostingRegressor()],\n",
    "        # 'regressor__loss': ['ls', 'lad', 'huber', 'quantile'],\n",
    "        # 'regressor__learning_rate': [0.1, 0.5],  \n",
    "        # 'regressor__n_estimators': [100, 200],\n",
    "        # 'regressor__subsample': [0.5, 1.0],\n",
    "        # 'regressor__criterion': ['friedman_mse', 'mse', 'mae'],\n",
    "        # 'regressor__min_samples_split': [2, 10],\n",
    "        # 'regressor__min_samples_leaf': [1, 5],\n",
    "        # 'regressor__max_depth': [3, 5],\n",
    "        # 'regressor__min_weight_fraction_leaf': [0.0, 0.1],\n",
    "        # 'regressor__max_features': [None, 'sqrt', 'log2'],\n",
    "        # 'regressor__max_leaf_nodes': [None, 5, 10],\n",
    "        # 'regressor__min_impurity_decrease': [0.0, 0.1],\n",
    "        # 'regressor__min_impurity_split': [None, 0.1],\n",
    "        # 'regressor__alpha': [0.9, 0.95, 0.99, 0.999],\n",
    "        # 'regressor__tol': [0.0001, 0.001, 0.01],\n",
    "        # 'regressor__validation_fraction': [0.1, 0.2],\n",
    "        # 'regressor__n_iter_no_change': [None, 5, 10],\n",
    "        # 'regressor__ccp_alpha': [0.0, 0.1],\n",
    "        # 'regressor__warm_start': [True, False],\n",
    "    # },\n",
    "]\n",
    "\n",
    "params = [{**dict_pre, **dict_reg} for dict_reg in regressor_params for dict_pre in preprocessor_params]\n",
    "# params = regressor_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## The pipeline is run by searching the provided paramter space using scorings of a crossvalidation technique to find out how each model candidate performs.\n",
    "\n",
    "scoring = {\n",
    "    'R2': 'r2',\n",
    "    'MAPE': 'neg_mean_absolute_percentage_error',\n",
    "    'MedAE': 'neg_median_absolute_error',\n",
    "    'MSLE': 'neg_mean_squared_log_error',\n",
    "  }\n",
    "refit_scorer = 'R2'\n",
    "\n",
    "with parallel_backend('loky', n_jobs=-1):\n",
    "    innerCV = GridSearchCV(\n",
    "        pipe,\n",
    "        params,\n",
    "        scoring= scoring,\n",
    "        refit= refit_scorer,\n",
    "        cv=10,\n",
    "        verbose=0,\n",
    "        # n_jobs=-1\n",
    "        )\n",
    "\n",
    "    outerCV = cross_validate(\n",
    "        innerCV,\n",
    "        model_X,\n",
    "        model_y,\n",
    "        scoring=scoring,\n",
    "        cv=10,\n",
    "        # return_train_score=True,\n",
    "        return_estimator=True,\n",
    "        verbose=1,\n",
    "        # n_jobs=-1\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a df of all outer cv results and show it sorted by the best scoring metric\n",
    "outerCV_df = pd.DataFrame(outerCV)\n",
    "outerCV_df.sort_values(by=f'test_{refit_scorer}', ascending=False, inplace=True)\n",
    "outerCV_df.rename_axis(index='outerCV_fold', inplace=True)\n",
    "\n",
    "## Get best model params for each of the outer cv folds:\n",
    "best_params_df = pd.DataFrame()\n",
    "for i, model in enumerate(outerCV['estimator']):\n",
    "    best_params = model.best_params_\n",
    "    # best_params_df = pd.concat([best_params_df, pd.DataFrame(best_params, index=[i])])  # this does not work when RandomForestRegressor is used, because some internals call len() on the values of the best_params dict, which raises AttributeError: 'RandomForestRegressor' object has no attribute 'estimators_'\n",
    "    # instead filling df with for-loop...:\n",
    "    current_best_params_df = pd.DataFrame()\n",
    "    for key, value in best_params.items():\n",
    "        current_best_params_df[key] = [value]\n",
    "        current_best_params_df.index = [i]\n",
    "    best_params_df = pd.concat([best_params_df, current_best_params_df])\n",
    "\n",
    "results = outerCV_df.join(best_params_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of coefficients in each outer folds best model: ', [outerCV['estimator'][i].best_estimator_.named_steps['regressor'].n_features_in_ for i in range(len(outerCV['estimator']))])\n",
    "print('Coeffs: ', *[outerCV['estimator'][i].best_estimator_.named_steps['regressor'].coef_ for i in range(len(outerCV['estimator']))], sep='\\n')\n",
    "# results.estimator[0].best_estimator_.named_steps['preprocessor'].transformers[0][1].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outerCV['estimator'][0].best_estimator_.named_steps['regressor'].n_features_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = results.preprocessor__selector__kw_args.apply(lambda x: [x['feature_set'], feature_candidates_list[x['feature_set']]])\n",
    "d = pd.DataFrame.from_dict(dict(zip(s.index, s.values))).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['estimator'].apply(lambda x: x.score(model_X, model_y))\n",
    "# pd.DataFrame.from_dict(dict(zip(s.values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_all_but_no_refit = results['estimator'].apply(lambda x: r2_score(model_y, x.predict(model_X)))\n",
    "r2_all_but_no_refit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_refitted_to_all = results.copy(deep=True)\n",
    "results_refitted_to_all['estimator'].apply(lambda x: r2_score(model_y, x.best_estimator_.fit(model_X, model_y).predict(model_X)))\n",
    "r2_refitted_to_all = results_refitted_to_all['estimator'].apply(lambda x: r2_score(model_y, x.predict(model_X)))\n",
    "r2_refitted_to_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing score of the best performing model candidate and its parameters.\n",
    "\n",
    "outer_fold = 9  # OBS: using only one of the outer-folds models here... e.g. [0]\n",
    "\n",
    "print(f'{scoring[refit_scorer]}: {outerCV[\"estimator\"][outer_fold].score(model_X, model_y)}')  \n",
    "print(outerCV['estimator'][outer_fold].best_params_)\n",
    "\n",
    "## Plotting the results of the best performing model candidate.\n",
    "df = pd.DataFrame(zip(model_y, outerCV['estimator'][outer_fold].predict(model_X)), index=model_X.index, columns=[target, 'predicted'])\n",
    "from plots import scatter_chart\n",
    "scatter_chart(df.reset_index(), target, 'predicted',\n",
    "                                 labels='Sample',\n",
    "                                 identity=True,\n",
    "                                 equal_axes=True,\n",
    "                                #  xscale='log', yscale='log',\n",
    "                                #  xtransform=True, ytransform=True,\n",
    "                                 width=800, height=800,\n",
    "                                 title='yhat vs. y')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get inner cv results of the outer cv fold which achieved the best scoring metric\n",
    "innerCV_df = pd.DataFrame(outerCV_df.loc[outerCV_df[f'test_score'].idxmax(), 'estimator'].cv_results_)\n",
    "innerCV_df.sort_values(by=f'rank_test_{scoring[0]}', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outerCV['estimator'][0].best_estimator_.named_steps['preprocessor'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting feature names\n",
    "# [grid.best_estimator_.named_steps['preprocessor'].named_transformers_['selector'].get_feature_names_out(input_features=model_X.columns.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show how the model performs on the training data\n",
    "\n",
    "train_pred_y = grid.predict(model_X)  # use the best model to predict the data on the same samples that were used to train the model\n",
    "print(f'R² = {r2_score(model_y, train_pred_y)}')  # adjusted R² = 1 - (1 - R²) * (n - 1) / (n - p) with n = number of samples, p = number of features\n",
    "\n",
    "df = pd.concat([\n",
    "    model_data.loc[model_y.index].regio_sep,\n",
    "    model_y,\n",
    "    pd.Series(\n",
    "        train_pred_y,\n",
    "        name='Prediction',\n",
    "        index=model_y.index)\n",
    "        ],\n",
    "    axis=1\n",
    "    ).reset_index()\n",
    "    \n",
    "scatter_chart(df, target, 'Prediction', color='regio_sep', labels='Sample', identity=True, equal_axes=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Take a look at all model candidates and their performance\n",
    "\n",
    "scores = pd.DataFrame(grid.cv_results_)\n",
    "# scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('MPSchleiSediments-z4CtktJ9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "82a31816b63c673b7463547b8d8376fc489f1e362f1a21f244a13819d6095661"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
