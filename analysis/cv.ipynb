{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # ignore warnings to avoid flooding the gridsearch output with repetitive messages (works for single cpu)\n",
    "import os\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"  # ignore warnings to avoid flooding the gridsearch output with repetitive messages (works for parallel)\n",
    "import time\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.plotting.backend = 'holoviews'\n",
    "import geopandas as gpd\n",
    "\n",
    "import sklearn\n",
    "print(f'sklearn verion: {sklearn.__version__}')\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "# from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MaxAbsScaler, MinMaxScaler, OneHotEncoder, FunctionTransformer\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    # AdaBoostRegressor,\n",
    "    # GradientBoostingRegressor,\n",
    "    # HistGradientBoostingRegressor,\n",
    ")\n",
    "from xgboost import XGBRegressor\n",
    "# from sklearn.neighbors import KNeighborsRegressor, RadiusNeighborsRegressor\n",
    "# from sklearn.svm import SVR\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "\n",
    "try:  # if on phy-server local modules will not be found if their directory is not added to PATH\n",
    "    import sys\n",
    "    sys.path.append(\"/silod7/lenz/MPSchleiSediments/analysis/\")\n",
    "    import os\n",
    "    os.chdir(\"/silod7/lenz/MPSchleiSediments/analysis/\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import prepare_data\n",
    "import geo\n",
    "from components import PCOA\n",
    "#from helpers import PipelineHelper, SMWrapper\n",
    "from plots import repNCV_score_plots\n",
    "from cv import compete_rep_ncv, compara_rep_ncv, aggregation, process_results, make_setup_dict, make_header, rensembling\n",
    "from cv_helpers import generate_feature_sets, SelectFeatures, unnegate, inter_rank, fix_feature_combis\n",
    "\n",
    "## Create set of keys in locals() originating from kernel initialisation and imports\n",
    "_lokeys = {k for k in locals().keys()}\n",
    "## later this could be used to pickle all user defined variables for complete workspace serialisation, using something like\n",
    "# d = locals()\n",
    "# with open('../data/exports/models/test.pkl', 'wb') as f:\n",
    "#     pickle.dump({k: d[k] for k in d.keys() - _lokeys}, f)  # pickling might need to be done with dill instead...\n",
    "\n",
    "from settings import Config, shortnames, target, featurelist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% loading data\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# cell magic to supress output. Comment it out to see output of this cell.\n",
    "\n",
    "# What happened so far: DB extract and blank procedure. Now import resulting MP data from csv\n",
    "mp_pdd = prepare_data.get_pdd()\n",
    "\n",
    "# Also import sediment data (sediment frequencies per size bin from master sizer export)\n",
    "grainsize_iow, grainsize_cau = prepare_data.get_grainsizes()[0:2]\n",
    "scor_iow = PCOA(grainsize_iow, 2)[0]\n",
    "scor_cau = PCOA(grainsize_cau, 2)[0]\n",
    "\n",
    "# ...some data wrangling to prepare particle domain data and sample domain data for MP and combine with certain sediment aggregates.\n",
    "sdd_iow = prepare_data.aggregate_SDD(mp_pdd)\n",
    "sdd_iow = prepare_data.additional_sdd_merging(sdd_iow, how='outer')\n",
    "sdd_iow = sdd_iow.merge(scor_iow, right_index=True, left_on='Sample', how='outer')\n",
    "sdd_iow = sdd_iow.replace({'Sample': shortnames}).sort_values(by='Sample')\n",
    "\n",
    "sdd_cau = pd.read_csv('../data/Metadata_CAU_sampling_log.csv', index_col=0)\n",
    "sdd_cau = sdd_cau.join(prepare_data.fix_gradistat_names(pd.read_csv('../data/GRADISTAT_CAU_vol_log-cau_closed.csv', index_col=0)), how='outer')\n",
    "sdd_cau = sdd_cau.merge(scor_cau, right_index=True, left_on='Sample', how='outer').reset_index()\n",
    "sdd_cau['Dist_Land'] = geo.get_distance_to_shore(sdd_cau['LON'], sdd_cau['LAT'])\n",
    "sdd_cau = prepare_data.impute_cau(sdd_cau)\n",
    "sdd_cau = geo.get_wwtp_influence(sdd_cau, tracks_file='../data/BAW_tracer_simulations.zip', file_postfix='_CAU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Split data into samples used for building the model and samples used for predicting.\n",
    "\n",
    "samples_with_response_and_predictor_data = sdd_iow.loc[~sdd_iow[target].isna()].set_index('Sample')\n",
    "samples_with_only_predictor_data = sdd_iow.loc[sdd_iow[target].isna()]\n",
    "samples_with_only_predictor_data = pd.concat([samples_with_only_predictor_data, sdd_cau.drop('Date',axis=1)]).set_index('Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Samples which are not suitable for (\"hydrodynamic outliers\") are moved from modelling data to prediction data\n",
    "droplist = ['S32','S05']\n",
    "samples_with_only_predictor_data = pd.concat([samples_with_only_predictor_data, samples_with_response_and_predictor_data.loc[droplist,:]])\n",
    "samples_with_response_and_predictor_data = samples_with_response_and_predictor_data.drop(droplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Limit dataframes to features (predictors) to be used in the model. Beware: depending on the preprocessing steps not all features might be used.\n",
    "model_X = samples_with_response_and_predictor_data[featurelist]\n",
    "model_y = samples_with_response_and_predictor_data[target]\n",
    "pred_X = samples_with_only_predictor_data[featurelist]\n",
    "pred_X = pred_X.drop('S09')  # TODO: no sediment data for S09???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined test set optionally to be used instead of CV:\n",
    "test_set = ('S30', 'S03', 'S15', 'S06', 'S31', 'S25', 'S20')  # possible samples to use as a predefined test set: ordered by relevance\n",
    "test_set_size = 7  # Requires int, should be 0 < test_set_size <= len(test_set), for using the n first samples as test_set\n",
    "test_set = test_set[0:test_set_size]\n",
    "test_set = model_X.index.isin(test_set).astype(int) - 1  # returns array of len(model_X.index) with -1 for training samples and 0 for testing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = None  # this is just needed for reporting as long as scaler is manually switched outside the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data using a scaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# model_X.values[:] = scaler.fit_transform(model_X)\n",
    "# pred_X.values[:] = scaler.transform(pred_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Check some basic statistics of the target variable\n",
    "\n",
    "# model_y.describe()\n",
    "# model_y.hist()\n",
    "# model_X.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom preprocessing functions to be used in the model pipeline\n",
    "Create a custom feature selector which creates exhaustively all combinations\n",
    "of available features to be tested as individual feature sets.\n",
    "It respects constrictions on non-allowed combinations, which can be defined\n",
    "by the user in Config.mutual_exclusive and Config.exclusive_keywords to sace\n",
    "computation time by not testing meaningless combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feat = (2,5)  # allowed number of features:\n",
    "                  #     if int: all possible combinations of this length will be created\n",
    "                  #     if tuple (min, max): all possible combinations of length min upt to length max will be created\n",
    "                  #     if 'all' all possible combinations of all possible lengths will be created\n",
    "feature_candidates_list = generate_feature_sets(model_X, Config.mutual_exclusive, Config.exclusive_keywords, num_feat=num_feat, n_jobs=1, save=True)\n",
    "\n",
    "CustomFeatureSelector = FunctionTransformer(SelectFeatures)#, feature_names_out='one-to-one')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Pipeline and parameter grid for model selection, see here for inspiration: https://towardsdatascience.com/getting-the-most-out-of-scikit-learn-pipelines-c2afc4410f1a\n",
    "\n",
    "PreProcessor = ColumnTransformer([\n",
    "      ('selector', CustomFeatureSelector, model_X.columns),\n",
    "      # ('imputer', SimpleImputer(), make_column_selector(dtype_include=np.number)),\n",
    "      # ('scaler', StandardScaler(), make_column_selector(dtype_include=np.number)),\n",
    "      # ('encoder', OneHotEncoder(), make_column_selector(dtype_include=object)),\n",
    "      ])\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('preprocessor', PreProcessor),\n",
    "    ('regressor', DummyRegressor())\n",
    "    ])\n",
    "\n",
    "preprocessor_params = [\n",
    "    {\n",
    "    'preprocessor__selector': [CustomFeatureSelector],\n",
    "    'preprocessor__selector__kw_args': [{'feature_set': i,\n",
    "                                         'feature_sets': feature_candidates_list\n",
    "                                        } for i in range(len(feature_candidates_list))],\n",
    "    \n",
    "    # 'preprocessor__scaler': [StandardScaler()],#MaxAbsScaler(), MinMaxScaler(), RobustScaler(), QuantileTransformer(), Normalizer()],\n",
    "    #    'preprocessor__scaler__with_mean': [True],\n",
    "    #    'preprocessor__scaler__with_std': [True],\n",
    "    }\n",
    "]\n",
    "\n",
    "regressor_params = [\n",
    "    {\n",
    "     'regressor': [TweedieRegressor(max_iter=100000)],  # , warm_start': True)],\n",
    "         'regressor__power': [2],  # 'regressor__power': [0, 1, 1.2, 1.5, 1.9, 2, 3],\n",
    "         'regressor__alpha': [1], \n",
    "         'regressor__link': ['log'],#, 'identity', 'auto'],\n",
    "         # 'regressor__fit_intercept': [True, False],\n",
    "    },\n",
    "\n",
    "    {\n",
    "    'regressor': [XGBRegressor(random_state=np.random.RandomState(0), verbosity = 1)],\n",
    "        'regressor__booster': ['gblinear'],\n",
    "        'regressor__objective': ['reg:gamma'],\n",
    "        'regressor__n_estimators': [300],\n",
    "        'regressor__reg_alpha': [0], #[0, 0.1, 1],  # L1-regularisation\n",
    "        'regressor__reg_lambda': [1],  # L2-regularisation (OBS: it's called 'alpha' in sklearn GLM models)\n",
    "    },\n",
    "\n",
    "    {\n",
    "    'regressor': [XGBRegressor(random_state=np.random.RandomState(0), verbosity = 0)],\n",
    "        'regressor__booster': ['gbtree'],\n",
    "        'regressor__objective': ['reg:gamma'],\n",
    "        'regressor__n_estimators': [300],\n",
    "        'regressor__reg_alpha': [0],  # L1-regularisation\n",
    "        'regressor__reg_lambda': [1],  # L2-regularisation (OBS: it's called 'alpha' in sklearn GLM models)\n",
    "        'regressor__tree_method': ['exact'],  # OBS: 'exact' doesn't work with max_depth=0\n",
    "        'regressor__learning_rate': [0.1, 0.25], #alias for eta\n",
    "        'regressor__max_depth': [2, 3],\n",
    "        'regressor__min_child_weight': [5, 7],  # also a regularising parameter: higher values will stop splitting further, when childs have less then \n",
    "        # 'regressor__min_split_loss': [0], # alias for gamma\n",
    "        # 'regressor__grow_policy': ['depthwise', 'lossguide'],\n",
    "        # 'regressor__subsample': [0.5, 1.0],\n",
    "        # 'regressor__colsample_bytree': [1/3, 1.0],\n",
    "    # #     'regressor__colsample_bylevel': [0.5, 1.0],\n",
    "    # #     'regressor__colsample_bynode': [0.5, 1.S0],\n",
    "    },\n",
    "    \n",
    "    {\n",
    "     'regressor': [RandomForestRegressor(random_state=np.random.RandomState(0))],\n",
    "          'regressor__n_estimators': [10, 100, 300],\n",
    "          'regressor__max_depth': [None, 2, 4],\n",
    "          'regressor__max_features': [None, 1/3],\n",
    "          # 'regressor__min_samples_split': [2, 10],\n",
    "          # 'regressor__min_samples_leaf': [1, 3, 5],\n",
    "          # 'regressor__bootstrap': [True, False],\n",
    "          # 'regressor__oob_score': [True, False],\n",
    "          # 'regressor__warm_start': [True, False],\n",
    "    },\n",
    "]\n",
    "\n",
    "params = [{**dict_pre, **dict_reg} for dict_reg in regressor_params for dict_pre in preprocessor_params]\n",
    "\n",
    "# Replace the full featurset lists with:\n",
    "# -> (2,3)-combinations feature sets for linear models,\n",
    "# -> (5,5)-combinations feature sets for tree models\n",
    "params = fix_feature_combis(params, feature_candidates_list, lin_combis=[2,3], tree_combis=[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set options for NCV\n",
    "scoring, aggregation, repetitions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repetitions = 100  # number of times the whole NCV is repeated, each time using a different randon_state to make the train/test-splits. Enter 1 for single shuffled NCV, 0 (or False) for single non-shuffled NCV.\n",
    "inner_reps = repetitions/10  # repetitions of inner KFold: int or fracton of `repetitions` (will be cast to int and fixed to 1 if smaller)\n",
    "folds = (5, 5)  # tuple (outer_folds, inner_folds) or int if number of folds for inner and out CVs should be the same.\n",
    "setup = make_setup_dict(repeats=(repetitions,inner_reps), folds=folds, n_jobs=(1,-1), verbosity=(0,1))\n",
    "\n",
    "scorers = [dict(zip(Config.scorers,t)) for t in zip(*Config.scorers.values())][1]  # dict 'scorers' will contain the string representation of negated scorers used by GridSearchCV\n",
    "negated = [  # saving a list of scores which will turn up with negated values in the results\n",
    "    key for key, val in scorers.items()\n",
    "    if any(sub in str(val)\n",
    "    for sub in ['neg_', 'greater_is_better=False'])\n",
    "]\n",
    "Config.refit_scorer = 'MAPE'  # must be one of the keys in Config.scorers\n",
    "Config.select_best = 'median'  # 'mean', 'median', 'iqm'\n",
    "Config.ncv_mode = 'comp'+'ara'+'tive'  # 'competitive' (run all activated models in grid against each other) or 'comparative' (run all activated models in separate sequential repeated NCV runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_savestamp = '20230403_233901'  # enter savestamp of model run here, write None to train a new model\n",
    "\n",
    "if load_savestamp is not None:\n",
    "    ## Load serialised results instead of training anew\n",
    "\n",
    "    sp = Path(f'../data/exports/models/serialised/{load_savestamp}/')\n",
    "    NCV = pd.read_pickle(sp/f'NCV_{load_savestamp}.pkl')\n",
    "    with open(sp/f'starttime_{load_savestamp}.pkl', 'rb') as f:\n",
    "        starttime = pickle.load(f)\n",
    "    with open(sp/f'time_needed_{load_savestamp}.pkl', 'rb') as f:\n",
    "        time_needed = pickle.load(f)\n",
    "    with open(sp/f'setup_{load_savestamp}.pkl', 'rb') as f:\n",
    "        setup = pickle.load(f)\n",
    "        \n",
    "else:\n",
    "    if repetitions < 1:\n",
    "        ## Run single NCV without shuffle split\n",
    "        raise NotImplementedError('Running single NCV without shuffle split corresponds to the old version before repeated NCV was included and was not re-implemented after rewriting codebase for the new rep_ncv function.')\n",
    "    if Config.ncv_mode == 'competitive':\n",
    "        NCV, setup, starttime, time_needed = compete_rep_ncv(pipe, params, model_X, model_y, scorers, setup)\n",
    "    elif Config.ncv_mode == 'comparative':\n",
    "        NCV, setup, starttime, time_needed = compara_rep_ncv(pipe, params, model_X, model_y, scorers, setup)\n",
    "    else: raise ValueError(f'No valid NCV mode: {Config.ncv_mode}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV results postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take an unaltered backup for pickling\n",
    "NCV_bak = NCV.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add ranking to outer fold test scores, all together, not regarding the repetitions\n",
    "NCV = inter_rank(NCV, Config.refit_scorer)\n",
    "## Get additional scorings, get feature names, etc. and arrange into a dataframe\n",
    "NCV = process_results(NCV, model_X, model_y, refitOnAll=True)\n",
    "## When run in comparative mode: append results that would have come out of an equivalent run in competitive mode\n",
    "NCV = rensembling(NCV)  # OBS: the intra- and inter rep rankings are not created again for the new block of the competitive run\n",
    "## Reversing the higher-is-better sklearn negated scores\n",
    "NCV = unnegate(NCV, negated)\n",
    "## Sort NCV\n",
    "NCV.sort_index(level=[0, 1, 3], sort_remaining=False, inplace=True)\n",
    "## Generate report header\n",
    "savestamp = starttime.strftime(\"%Y%m%d_%H%M%S\")\n",
    "header = make_header(NCV, setup, savestamp, time_needed, droplist, model_X, num_feat, feature_candidates_list, scaler, regressor_params)\n",
    "## Display results\n",
    "print(header)\n",
    "pd.set_option('display.max_colwidth', None, 'display.max_columns', None)\n",
    "NCV.drop(['fit_time', 'score_time', 'estimator', 'estimator_refit_on_all'], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simulate shorter runs (fewer repetitions) and repeat score aggregation\n",
    "scored_multi = pd.DataFrame()\n",
    "for reps in range(1, setup['repeats'][0]+1):\n",
    "    scored_short = aggregation(NCV.query(f'NCV_repetition <= {reps-1}'), setup, r=reps)\n",
    "    scored_short = pd.concat({reps: scored_short}, names=['NCV_repetitions'])\n",
    "    scored_multi = pd.concat([scored_multi, scored_short])\n",
    "    \n",
    "## Plot score evolution\n",
    "chart, chart_df = repNCV_score_plots(scored_multi, return_df=True, ncv_mode=Config.ncv_mode)\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select model run to use for prediction\n",
    "sel = 'Competitive'\n",
    "evalScore = 'R2'\n",
    "\n",
    "selRun = NCV.filter(regex=re.compile(sel, re.IGNORECASE), axis=0)\n",
    "diff = selRun.groupby('NCV_repetition')[f'test_{evalScore}'].apply(lambda x: np.abs(x - x.median()))\n",
    "idx = diff.groupby('NCV_repetition').idxmin()\n",
    "esti = selRun.loc[idx][['estimator_refit_on_all', 'features']]  # TODO: also include any columns starting with 'regressor' to check for unique models in ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y_seenSamples = pd.Series([\n",
    "    np.mean(\n",
    "        [est['estimator_refit_on_all'].predict(model_X.loc[[P], est.features])\n",
    "         for _, est in esti.iterrows()]\n",
    "    ) for P in model_X.index\n",
    "], index=model_X.index, name=f'{target}_predicted')\n",
    "\n",
    "\n",
    "pred_y = pd.Series([\n",
    "    np.mean(\n",
    "        [est['estimator_refit_on_all'].predict(pred_X.loc[[P], est.features])\n",
    "         for _, est in esti.iterrows()]\n",
    "    ) for P in pred_X.index\n",
    "], index=pred_X.index, name=f'{target}_predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_o = pred_y_seenSamples.to_frame().join(samples_with_response_and_predictor_data[[target, 'LON', 'LAT', 'SedDryBulkDensity'] + featurelist])\n",
    "df_o['Type'] = 'observed'\n",
    "df_o.rename(columns={target: f'{target}_observed'}, inplace=True)\n",
    "df_p = pred_y.to_frame().join(samples_with_only_predictor_data[['LON', 'LAT', 'SedDryBulkDensity'] + featurelist])\n",
    "df_p['Type'] = 'predicted'\n",
    "df_a = pd.concat([df_o, df_p])\n",
    "            \n",
    "## For samples previously dropped (outliers with existing observed target value), replace the predicted value with the original observed one\n",
    "df_a.insert(2, 'outlier_excl', False)\n",
    "for o in droplist:\n",
    "    df_a.loc[o, f'{target}_observed'] = samples_with_only_predictor_data.loc[o, target]\n",
    "    df_a.loc[o, 'Type'] = 'observed'\n",
    "    df_a.loc[o, 'outlier'] = True\n",
    "            \n",
    "df_a.insert(0,target, df_a[f'{target}_observed'].combine_first(df_a[f'{target}_predicted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.GeoDataFrame(df_a, geometry=gpd.points_from_xy(df_a.LON, df_a.LAT))\n",
    "\n",
    "gdf.plot(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## only save if not loaded from already serialised model:\n",
    "if load_savestamp is None:\n",
    "    ## Save model settings and results report\n",
    "    fp = '../data/exports/models/'\n",
    "    fn = fp + 'model_NCV_result.csv'\n",
    "    with open(fn, mode='a' if Path(fn).exists() else 'w', encoding='utf-8') as f:\n",
    "        f.write(header)\n",
    "    NCV.drop(['fit_time', 'score_time', 'estimator'], axis=1).to_csv(fn, mode='a', sep=';')\n",
    "\n",
    "    ## Serialise results\n",
    "    sp = Path(fp)/'serialised'/savestamp\n",
    "    sp.mkdir(parents=True, exist_ok=True)\n",
    "    NCV_bak.to_pickle(sp/f'NCV_{savestamp}.pkl')\n",
    "    with open(sp/f'starttime_{savestamp}.pkl', 'wb') as f:\n",
    "        pickle.dump(starttime, f)\n",
    "    with open(sp/f'time_needed_{savestamp}.pkl', 'wb') as f:\n",
    "        pickle.dump(time_needed, f)\n",
    "    with open(sp/f'setup_{savestamp}.pkl', 'wb') as f:\n",
    "        pickle.dump(setup, f)\n",
    "\n",
    "    ## Save figure\n",
    "    chart.save(f'../data/exports/plots/repNCV_score_evolution_{savestamp}.html')\n",
    "    \n",
    "    ## Save predictions\n",
    "    gdf.drop('geometry', axis=1).to_csv(f'{fp}/predictions/{savestamp}_{target}_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get top k results of inner CV sorted by best Config.refit_scorer \n",
    "\n",
    "# outer_fold = 0  # manuaklly chose which outer fold to look at\n",
    "# top_k = 10  # how many model candidates from the inner model to show?\n",
    "# pd.DataFrame(outerCV['estimator'][outer_fold].cv_results_).sort_values(f'rank_by_median_test_{Config.refit_scorer}', ascending=True).head(top_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing score of the best performing model candidate and its parameters.\n",
    "\n",
    "# outer_fold = 0\n",
    "\n",
    "# model = outerCV['estimator_refit_on_all'][outer_fold]\n",
    "# print(best_params_df.loc[outer_fold, 'regressor'])\n",
    "# print(f'R2 of model retrained with all samples, tested against all samples: {r2_score(model_y, model.predict(model_X[NCV.features.loc[outer_fold]]))}')\n",
    "# # if model was TweedieRegressor, print alpha and power\n",
    "# if 'Tweedie' in str(type(best_params_df.loc[outer_fold, 'regressor'])):\n",
    "#     print(f'Intercept: {model.intercept_}')\n",
    "#     print(f'Coeffs: {model.coef_}')\n",
    "# print(feature_candidates_list[best_params_df.loc[outer_fold, 'preprocessor__selector__kw_args']['feature_set']])\n",
    "# print(f'{Config.scorers[Config.refit_scorer]}: {outerCV[\"estimator\"][outer_fold].score(model_X, model_y)}')  \n",
    "#print(outerCV['estimator'][outer_fold].best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Number of coefficients in each outer folds best model: ', [outerCV['estimator'][i].best_estimator_.named_steps['regressor'].n_features_in_ for i in range(len(outerCV['estimator']))])\n",
    "# print('Coeffs: ', *[outerCV['estimator'][i].best_estimator_.named_steps['regressor'].coef_ for i in range(len(outerCV['estimator']))], sep='\\n')\n",
    "# results.estimator[0].best_estimator_.named_steps['preprocessor'].transformers[0][1].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [outerCV['estimator'][0].best_estimator_.named_steps['regressor'].estimators_[i].get_n_leaves() for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results['estimator'].apply(lambda x: x.score(model_X, model_y))\n",
    "# pd.DataFrame.from_dict(dict(zip(s.values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2_all_but_no_refit = results['estimator'].apply(lambda x: r2_score(model_y, x.predict(model_X)))\n",
    "# r2_all_but_no_refit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get inner cv results of the outer cv fold which achieved the best scoring metric\n",
    "# innerCV_df = pd.DataFrame(outerCV_df.loc[outerCV_df[f'test_score'].idxmax(), 'estimator'].cv_results_)\n",
    "# innerCV_df.sort_values(by=f'rank_test_{scoring[0]}', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outerCV['estimator'][0].best_estimator_.named_steps['preprocessor'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting feature names\n",
    "# [grid.best_estimator_.named_steps['preprocessor'].named_transformers_['selector'].get_feature_names_out(input_features=model_X.columns.tolist())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The chosen final model\n",
    "\n",
    "- evaluation by cross-validation\n",
    "- prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Activate this block to use same features, model type and hyperparameters a specific outer fold\n",
    "# #outer_fold = 1\n",
    "# #final_model_X = model_X[NCV.features.loc[outer_fold]]\n",
    "# #final_model = outerCV['estimator_refit_on_all'][outer_fold]\n",
    "\n",
    "# # Alternatively activate this block to use a manually defined model\n",
    "# final_model_X = model_X[['PC1', 'Depth', 'WWTP_influence_as_mean_time_travelled__sed_18Âµm_allseasons_444']]\n",
    "# final_model = TweedieRegressor(alpha=0.1, link='log', max_iter=100000, power=1.5, tol=0.0001)\n",
    "    \n",
    "# # for loop to run cross validation on the final model with leave-p-out iterating p from 2 to 10\n",
    "# allR2s = {}\n",
    "# allMedianR2s = {}\n",
    "# allMeanR2s = {}\n",
    "# R2calcDuration = {}\n",
    "# for P in range(2, 7):\n",
    "#     starttime = datetime.now()\n",
    "#     finalR2s = cross_val_score(final_model, final_model_X, model_y, cv=LeavePOut(P), scoring='r2', n_jobs=-1, verbose=0)\n",
    "#     duration = datetime.now() - starttime\n",
    "#     print(f'Cross-validation of final model with leave-{P}-out took {duration.seconds//3600} hours, {(duration.seconds//60)%60} minutes and {duration.seconds%60} seconds.')\n",
    "#     print(f'Cross-validated MEAN R2 of final model: {finalR2s.mean():.3f} (Standard deviation: {finalR2s.std():.3f})')\n",
    "#     print(f'Cross-validated MEDIAN R2 of final model: {np.median(finalR2s):.3f} (IQR {np.subtract(*np.percentile(finalR2s, [.75, .25])):.3f})')\n",
    "#     print()\n",
    "#     allR2s[P] = finalR2s\n",
    "#     allMedianR2s[P] = np.median(finalR2s)\n",
    "#     allMeanR2s[P] = finalR2s.mean()\n",
    "#     R2calcDuration[P] = duration\n",
    "\n",
    "# final_model.fit(final_model_X, model_y)\n",
    "# print(f'R2 of final model retrained with all samples, tested against all samples: {r2_score(model_y, final_model.predict(final_model_X))}')\n",
    "\n",
    "# # Predictions\n",
    "# #pred_X = pred_X[model_X.columns.tolist()]\n",
    "# #pred_y = final_model.predict(pred_X)\n",
    "# df1 = pd.DataFrame(zip(model_y, final_model.predict(final_model_X), ['IOW']*len(model_y)), index=model_X.index, columns=[target, 'predicted', 'group'])\n",
    "# #df2 = pd.DataFrame(zip([0]*len(pred_y), pred_y, ['CAU']*len(pred_y)), index=pred_X.index, columns=[target, 'predicted', 'group'])\n",
    "# df = pd.concat([df1])#, df2])\n",
    "\n",
    "# scatter_chart(df.reset_index(), target, 'predicted', 'group', labels='Sample', identity=True, equal_axes=True, width=800, height=800, title='yhat vs. y')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # combine dicts 'allMeanR2s' and 'allMedianR2s' into a dataframe\n",
    "# final_scores_df = pd.DataFrame.from_dict(allMeanR2s, orient='index', columns=['Mean R2']).join(pd.DataFrame.from_dict(allMedianR2s, orient='index', columns=['Median R2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_scores_df.plot.line(title='Mean and median R2 of final model with leave-p-out cross-validation', xlabel='p', ylabel='R2', width=800, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(360000)  # to keep the kernel running for 100 h after model run, when no client is connected"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
