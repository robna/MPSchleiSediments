{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataTransformerRegistry.enable('default')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import KDE_prepare_data\n",
    "import KDE_utils\n",
    "import correlations\n",
    "from KDE_settings import Config\n",
    "from scipy import stats\n",
    "alt.renderers.enable('altair_viewer')\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## MP particle size correlations\n",
    "\n",
    "*This calculates probability density distributions for MP sizes. These are used to estimate the abundances of MP of specific size ranges, wich can then be investigated for correlations to environmental paramteres such as TOC or sediment grain size properties.*\n",
    "**Before running, check the computational parameters in KDE_settings.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% loading data\n"
    }
   },
   "outputs": [],
   "source": [
    "# What happened so far: DB extract and blank procedure. Now import reulting MP data from csv\n",
    "pdd_MP = pd.read_csv('../csv/env_MP_clean_list_SchleiSediments.csv', index_col=0)\n",
    "\n",
    "# Also import sediment data (sediment frequencies per size bin from master sizer export)\n",
    "sed_size_freqs = pd.read_csv('/home/nibor/ownCloud/microSCHLEI/Sediment_K/Sediment analysis/KGA/KGA_IOW_Schlei_export/Enders_export_10µm_linear_noR_RL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the binning structure of the imported sediment data and optionally rebin it (make binning coarser) for faster computation\n",
    "sed_size_freqs, sed_x_d = KDE_prepare_data.sediment_preps(sed_size_freqs, rebinning=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% data wrangling\n"
    }
   },
   "outputs": [],
   "source": [
    "# ...some data wrangling to prepare particle domain data and sample domain data for MP and combine with certain sediment aggregates.\n",
    "sdd_MP = KDE_prepare_data.aggregate_SDD(pdd_MP)\n",
    "sdd_MP_sed = KDE_prepare_data.add_sediment(sdd_MP)\n",
    "pdd_sdd_MP = KDE_prepare_data.sdd2pdd(sdd_MP_sed, pdd_MP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% calculating probability densities\n"
    }
   },
   "outputs": [],
   "source": [
    "# KDEs for probability of finding a MP particle in a specific size bin are calculated.\n",
    "size_pdfs = KDE_utils.per_sample_kde(pdd_sdd_MP, x_d = sed_x_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%% size-ranged TOC correlations\n"
    },
    "tags": [
     "TOC"
    ]
   },
   "outputs": [],
   "source": [
    "# MP abundances per size bin are calculated from the KDEs and the total MP concentration in the samples.\n",
    "df_range_conc = KDE_utils.make_range_conc(size_pdfs, sdd_MP_sed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MP and sediment abundances per size bin are equalised (only samples and bins contained in both of the are kept).\n",
    "# They are then melted and merged into MPsedMelted.\n",
    "df_range_conc, sed_size_freqs, MPsedMelted = KDE_prepare_data.equalise_MP_and_Sed(df_range_conc, sed_size_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Displaying chart at <a href='http://localhost:15411/' target='_blank'>http://localhost:15411/</a>"
      ],
      "text/plain": [
       "Displaying chart at http://localhost:15411/"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brush1 = alt.selection(type=\"interval\", encodings=['x'])\n",
    "brush2 = alt.selection(type=\"interval\", encodings=['x'])\n",
    "\n",
    "\n",
    "scatter = alt.Chart(MPsedMelted).mark_point().encode(\n",
    "    x = 'freq_sum:Q',\n",
    "    y = 'conc_sum:Q',\n",
    "    tooltip = 'sample_name:N'\n",
    ").transform_filter(\n",
    "    brush1 | brush2\n",
    ").transform_aggregate(\n",
    "    conc_sum = 'sum(conc)',\n",
    "    freq_sum = 'sum(freq)',\n",
    "    sample_name = 'values(sample)',\n",
    "    groupby = ['sample']\n",
    "# ).transform_calculate(\n",
    "#     b1l='(isDefined(brush1.x) ? (brush1.x[0]) : 1)',\n",
    "#     b1u='(isDefined(brush1.x) ? (brush1.x[1]) : 1)',\n",
    "#     b2l='(isDefined(brush2.x) ? (brush2.x[0]) : 1)',\n",
    "#     b2u='(isDefined(brush2.x) ? (brush2.x[1]) : 1)',\n",
    "# ).transform_window(\n",
    "#     conc_sum = 'sum(conc)',\n",
    "#     frame = [brush1.x[0],brush1.x[1]],\n",
    "#     groupby=['sample']\n",
    "# ).transform_window(\n",
    "#     freq_sum = 'sum(freq)',\n",
    "#     frame = [brush2.x[0],brush2.x[1]],\n",
    "#     groupby=['sample']\n",
    ")\n",
    "\n",
    "\n",
    "RegLine = scatter.transform_regression(\n",
    "    'freq_sum', 'conc_sum', method=\"linear\",\n",
    ").mark_line(\n",
    "    color=\"red\"\n",
    ")\n",
    "\n",
    "\n",
    "RegParams = scatter.transform_regression(\n",
    "    'freq_sum', 'conc_sum', method=\"linear\", params=True\n",
    ").mark_text(align='left', lineBreak='\\n').encode(\n",
    "    x=alt.value(120),  # pixels from left\n",
    "    y=alt.value(20),  # pixels from top\n",
    "    text='params:N'\n",
    ").transform_calculate(\n",
    "    params='\"r² = \" + round(datum.rSquared * 100)/100 + \\\n",
    "    \"      y = \" + round(datum.coef[1] * 10)/10 + \"x\" + \" + \" + round(datum.coef[0] * 10)/10'\n",
    ")\n",
    "\n",
    "\n",
    "MP = alt.Chart(MPsedMelted).mark_line().encode(\n",
    "    x = alt.X('lower', scale=alt.Scale(type='linear')),\n",
    "    y = 'mean(conc)'\n",
    ").add_selection(\n",
    "    brush1\n",
    ").properties(\n",
    "    height = 100\n",
    ")\n",
    "\n",
    "\n",
    "sed = alt.Chart(MPsedMelted).mark_line().encode(\n",
    "    x = alt.X('lower', scale=alt.Scale(type='linear')),\n",
    "    y = 'mean(freq)'\n",
    ").add_selection(\n",
    "    brush2\n",
    ").properties(\n",
    "    height = 100\n",
    ")\n",
    "\n",
    "\n",
    "(scatter + RegLine + RegParams) & alt.vconcat(MP, sed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-4f1aa736e8b5479385adc9db4fac6735\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-4f1aa736e8b5479385adc9db4fac6735\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-4f1aa736e8b5479385adc9db4fac6735\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"vconcat\": [{\"mark\": \"point\", \"encoding\": {\"tooltip\": {\"type\": \"nominal\", \"field\": \"Origin\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"HP_sum\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"Dis_sum\"}}, \"transform\": [{\"filter\": {\"selection\": {\"or\": [\"selector003\", \"selector004\"]}}}, {\"aggregate\": [{\"op\": \"sum\", \"field\": \"Displacement\", \"as\": \"Dis_sum\"}, {\"op\": \"sum\", \"field\": \"Horsepower\", \"as\": \"HP_sum\"}], \"groupby\": [\"Origin\", \"HP_sum\"]}]}, {\"mark\": \"line\", \"encoding\": {\"x\": {\"type\": \"temporal\", \"field\": \"Year\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"Horsepower\"}}, \"height\": 100, \"selection\": {\"selector003\": {\"type\": \"interval\", \"encodings\": [\"x\"]}}}, {\"mark\": \"line\", \"encoding\": {\"x\": {\"type\": \"temporal\", \"field\": \"Year\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"mean\", \"field\": \"Displacement\"}}, \"height\": 100, \"selection\": {\"selector004\": {\"type\": \"interval\", \"encodings\": [\"x\"]}}}], \"data\": {\"url\": \"https://cdn.jsdelivr.net/npm/vega-datasets@v1.29.0/data/cars.json\"}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\"}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.VConcatChart(...)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import altair as alt\n",
    "from vega_datasets import data\n",
    "cars = data.cars.url\n",
    "\n",
    "brush1 = alt.selection(type=\"interval\", encodings=['x'])\n",
    "brush2 = alt.selection(type=\"interval\", encodings=['x'])\n",
    "\n",
    "\n",
    "scatter = alt.Chart(cars).mark_point().encode(\n",
    "    x = 'HP_sum:Q',\n",
    "    y = 'Dis_sum:Q',\n",
    "    tooltip = 'Origin:N'\n",
    ").transform_filter(  # Ok, I can filter the whole data set, but that always acts on both variables (HP and displacement) together... -> not what I want.\n",
    "    brush1 | brush2\n",
    ").transform_aggregate(\n",
    "    Dis_sum = 'sum(Displacement)',\n",
    "    HP_sum = 'sum(Horsepower)',\n",
    "    groupby = ['Origin', 'HP_sum']\n",
    "# ).transform_calculate(  # Can I extract the selection boundaries like that? And if yes: how can I use these extracts to calculate the aggregationsof HP and displacement?\n",
    "#     b1_lower='(isDefined(brush1.x) ? (brush1.x[0]) : 1)',\n",
    "#     b1_upper='(isDefined(brush1.x) ? (brush1.x[1]) : 1)',\n",
    "#     b2_lower='(isDefined(brush2.x) ? (brush2.x[0]) : 1)',\n",
    "#     b2_upper='(isDefined(brush2.x) ? (brush2.x[1]) : 1)',\n",
    "# ).transform_window(  # Maybe instead of calculate I can use two window transforms...??\n",
    "#     conc_sum = 'sum(conc)',\n",
    "#     frame = [brush1.x[0],brush1.x[1]],  # This will not work, as it sets the frame relative (back- and foreward) to each datum (i.e. sliding window), I need it to correspond to the entire data set\n",
    "#     groupby=['sample']\n",
    "# ).transform_window(\n",
    "#     freq_sum = 'sum(freq)',\n",
    "#     frame = [brush2.x[0],brush2.x[1]],  # ...same problem here\n",
    "#     groupby=['sample']\n",
    ")\n",
    "\n",
    "range_sel1 = alt.Chart(cars).mark_line().encode(\n",
    "    x = 'Year:T',\n",
    "    y = 'mean(Horsepower):Q'\n",
    ").add_selection(\n",
    "    brush1\n",
    ").properties(\n",
    "    height = 100\n",
    ")\n",
    "\n",
    "range_sel2 = alt.Chart(cars).mark_line().encode(\n",
    "    x = 'Year:T',\n",
    "    y = 'mean(Displacement):Q'\n",
    ").add_selection(\n",
    "    brush2\n",
    ").properties(\n",
    "    height = 100\n",
    ")\n",
    "\n",
    "\n",
    "scatter & range_sel1 & range_sel2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum abundances in size bins for all possible combinations. Basically create all possible rebinnings and stack them into one DF\n",
    "MPext = KDE_prepare_data.combination_sums(df_range_conc)\n",
    "SEDext= KDE_prepare_data.combination_sums(sed_size_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate a correlation matrix containing Pearson correlation coefficients for all combinations of any original or summed bins of MP and sediments.\n",
    "corrMat = np.corrcoef(MPext,SEDext)\n",
    "corrMat = corrMat[:len(MPext), len(SEDext):]  # only take upper right quadrant of correlation matrix\n",
    "\n",
    "corrMatDF = pd.DataFrame(corrMat, index=MPext.index, columns=SEDext.index)  # turn np array into df\n",
    "corrMatDF.rename('MP_{}'.format, axis=0, inplace=True)  # add a prefix for 'MP' to each row label\n",
    "corrMatDF.rename('SED_{}'.format, axis=1, inplace=True)  # add a prefix for 'sediment' to each column label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a cross-correlation for each sample between MP and sediment size distribution curve shapes.\n",
    "crosscorr_results = pd.DataFrame()\n",
    "\n",
    "for label, content in df_range_conc.items():\n",
    "    datax = content\n",
    "    datay = sed_size_freqs[label]\n",
    "    #best = correlations.crosscorr(datax, datay)\n",
    "        \n",
    "#     lags = range(-int(len(datax)/2), int(len(datax)/2)+1)\n",
    "    lags = range(-50,0)\n",
    "    df_r = pd.DataFrame(lags, columns = ['shifted'])\n",
    "    \n",
    "#     r_pear = [datax.corr(datay.shift(lag)) for lag in lags]\n",
    "#     df_r['pearson_r'] = r_pear\n",
    "    r_spear = [stats.spearmanr(np.array(datax),np.array(datay.shift(lag)), nan_policy = 'omit') for lag in lags]\n",
    "    df_r['spearman_r'] = r_spear\n",
    "    \n",
    "    \n",
    "    best = df_r.copy().loc[df_r['spearman_r'] == df_r['spearman_r'].max()]\n",
    "    best['Sample'] = label\n",
    "    crosscorr_results = crosscorr_results.append(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%% size-ranged TOC correlations\n"
    },
    "tags": [
     "TOC"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    lower_size upper_size         r         p990,        2000]                \n",
      "130       1300       1310  0.573008  0.006624\r"
     ]
    }
   ],
   "source": [
    "# Calculate correlation between all size bins (or their summed up combinations) of MP abundances and a scalar predictor.\n",
    "# Possible predictors are: 'TOC', 'Hg', 'Dist_WWTP' or different sediment grain size properties (D50, below 63, etc.). See column labels of sdd_MP_sed for a full list.\n",
    "bestLower, bestUpper, df_r = correlations.predictorcorr(df_range_conc, sdd_MP_sed.set_index('Sample'), 'TOC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "TOC"
    ]
   },
   "outputs": [],
   "source": [
    "# Heatmap correlation coefficients from the single predictor correlation\n",
    "alt.renderers.enable('html')\n",
    "\n",
    "alt.Chart(df_r).mark_point().encode(\n",
    "    x='lower_size',\n",
    "    y='upper_size',\n",
    "    color=alt.Color(\"r\", scale=alt.Scale(domain=[0.5, 0.6])),\n",
    "    tooltip=['r', 'lower_size', 'upper_size']\n",
    ").properties(\n",
    "    width=800,\n",
    "    height=800\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% size-ranged TOC correlations\n"
    },
    "tags": [
     "TOC"
    ]
   },
   "outputs": [],
   "source": [
    "#####USED only for testing external function inside NB: #####\n",
    "step = (Config.upper_size_limit - Config.lower_size_limit) / Config.kde_steps\n",
    "df_r = pd.DataFrame(columns=['lower_size', 'upper_size', 'r', 'p'])\n",
    "\n",
    "for i in [10]:\n",
    "    for j in [1000]:\n",
    "        size_sum = size_pdfs.loc[(size_pdfs.x_d >= i) & (size_pdfs.x_d < j)].sum()\n",
    "        size_sum.drop('x_d', inplace=True)\n",
    "        range_prob = size_sum * step\n",
    "        range_conc = range_prob * sdd_MP_sed.set_index('Sample').Concentration\n",
    "\n",
    "        r = stats.pearsonr(range_conc, sdd_MP_sed.set_index('Sample').TOC)\n",
    "        df_r.loc[len(df_r)] = [i, j, r[0], r[1]]\n",
    "        print(f'Correlating TOC with size range            [{i},        {j}]                ', end=\"\\r\", flush=True)\n",
    "\n",
    "print(df_r.loc[df_r.r == df_r.r.max()])\n",
    "bestLower, bestUpper = df_r.loc[df_r.r == df_r.r.max()].iloc[0, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "TOC"
    ]
   },
   "outputs": [],
   "source": [
    "# Property-property-plot with MP against single predictor incl. reg line \n",
    "kd = alt.Chart(pd.DataFrame([range_conc, sdd_MP_sed.set_index('Sample').TOC]).T.reset_index()).mark_point().encode(\n",
    "    x='TOC',\n",
    "    y='Unnamed 0',\n",
    "    color='index',\n",
    "    tooltip='index'\n",
    ")\n",
    "\n",
    "Reg_Line = kd.transform_regression('TOC', 'Unnamed 0', method=\"linear\",\n",
    "                                  ).mark_line(color=\"red\")\n",
    "\n",
    "Reg_Params = kd.transform_regression('TOC', 'Unnamed 0', method=\"linear\", params=True\n",
    "                                    ).mark_text(align='left', lineBreak='\\n').encode(\n",
    "        x=alt.value(120),  # pixels from left\n",
    "        y=alt.value(20),  # pixels from top\n",
    "        text='params:N'\n",
    "    ).transform_calculate(\n",
    "        params='\"r² = \" + round(datum.rSquared * 100)/100 + \\\n",
    "    \"      y = \" + round(datum.coef[0] * 10)/10 + \" + e ^ (\" + round(datum.coef[1] * 10000)/10000 + \"x\" + \")\" + \\n + \" \"'\n",
    "    )\n",
    "\n",
    "kd + Reg_Line + Reg_Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "daf = sed_size_freqs.reset_index().melt(id_vars='index', value_name='value')\n",
    "daf['size'] = daf['index'].str.split('_').str[0]\n",
    "daf['category'] = 'sediment'\n",
    "\n",
    "daff = df_range_conc.reset_index().melt(id_vars='index', value_name='value')\n",
    "daff['size'] = daf['index'].str.split('_').str[0]\n",
    "daff['category'] = 'MP'\n",
    "\n",
    "df = pd.concat([daf, daff]).drop(columns=['index'])\n",
    "# df = daf.merge(daff, on =['sample', 'size', 'index']).drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Displaying chart at <a href='http://localhost:23565/' target='_blank'>http://localhost:23565/</a>"
      ],
      "text/plain": [
       "Displaying chart at http://localhost:23565/"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dropdown = alt.binding_select(options=list(df['sample'].unique()), name='select sample ')\n",
    "selection = alt.selection_single(fields=['sample'],\n",
    "                                 bind=input_dropdown,\n",
    "                                 #init={'select_': 'Schlei_S8'}\n",
    "                                )\n",
    "\n",
    "base = alt.Chart(df).mark_line().encode(\n",
    "    x=alt.X('size:Q'),\n",
    "    y=alt.Y('value:Q'),\n",
    "    color='category',\n",
    "#     shape='category',\n",
    "#     detail='category'\n",
    "# ).transform_window(\n",
    "#     cuml='sum(value)',\n",
    "#     groupby=['sample']\n",
    ").transform_filter(\n",
    "    alt.FieldRangePredicate(field='size', range=[50, 990])\n",
    "# ).properties(\n",
    "#     width=180,\n",
    "#     height=180\n",
    "# ).facet(\n",
    "#     facet='sample',\n",
    "#     columns=6\n",
    "# ).resolve_scale(\n",
    "#     y='independent'\n",
    ")\n",
    "\n",
    "\n",
    "mps = base.mark_line(color='blue').transform_filter(\n",
    "    alt.FieldEqualPredicate(field='category', equal='MP')\n",
    ")\n",
    "seds = base.mark_line(color='yellow').transform_filter(\n",
    "    alt.FieldEqualPredicate(field='category', equal='sediment')\n",
    ")\n",
    "\n",
    "alt.layer(mps, seds).resolve_scale(\n",
    "    y = 'independent'\n",
    ").add_selection(\n",
    "    selection\n",
    ").transform_filter(\n",
    "    selection\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Displaying chart at <a href='http://localhost:23565/' target='_blank'>http://localhost:23565/</a>"
      ],
      "text/plain": [
       "Displaying chart at http://localhost:23565/"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = alt.Chart(df).encode(\n",
    "    alt.X('size:Q', axis=alt.Axis(title=None))\n",
    ").properties(\n",
    "    width=180,\n",
    "    height=180\n",
    ")\n",
    "\n",
    "mps = base.mark_line(stroke='#57A44C', interpolate='monotone').encode(\n",
    "    alt.Y('MP:Q', axis=alt.Axis(title='MP', titleColor='#57A44C'))\n",
    ")\n",
    "\n",
    "seds = base.mark_line(stroke='#5276A7', interpolate='monotone').encode(\n",
    "    alt.Y('sediment:Q', axis=alt.Axis(title='sediment', titleColor='#5276A7'))\n",
    ")\n",
    "\n",
    "alt.layer(mps, seds).resolve_scale(\n",
    "    y = 'independent'\n",
    ").properties(\n",
    "    width=180,\n",
    "    height=180\n",
    ").facet(\n",
    "    facet='sample',\n",
    "    columns=6\n",
    ").resolve_scale(\n",
    "    y='independent'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Displaying chart at <a href='http://localhost:23565/' target='_blank'>http://localhost:23565/</a>"
      ],
      "text/plain": [
       "Displaying chart at http://localhost:23565/"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = daf.shift(100).merge(daff, on =['sample', 'size', 'index']).drop(columns=['index'])\n",
    "alt.Chart(df).mark_point().encode(\n",
    "    x=alt.X('value_x', scale=alt.Scale(type='linear')),\n",
    "    y=alt.X('value_y', scale=alt.Scale(type='linear')),\n",
    "    color=alt.Color('size:Q', scale=alt.Scale(scheme=\"viridis\"))\n",
    ").properties(\n",
    "    width=150,\n",
    "    height=150\n",
    ").facet(\n",
    "    facet='sample',\n",
    "    columns=6\n",
    ").resolve_scale(\n",
    "    y='independent',\n",
    "    x='independent'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Displaying chart at <a href='http://localhost:19233/' target='_blank'>http://localhost:19233/</a>"
      ],
      "text/plain": [
       "Displaying chart at http://localhost:19233/"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame({\n",
    "    'x': range(9),\n",
    "    'y': [1, 2, 3, 4, 5, 4, 3, 2, 1]\n",
    "})\n",
    "\n",
    "brush = alt.selection_interval(name=\"brush\", encodings=['x'])\n",
    "alt.Chart(data).mark_line().add_selection(brush).transform_calculate(\n",
    "    scaled_by_brush_width='datum.y * (isDefined(brush.x) ? (brush.x[1] - brush.x[0]) : 1)'\n",
    ").encode(\n",
    "    x='x:Q',\n",
    "    y='scaled_by_brush_width:Q'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MPSchleiSediments-bSFBF4Xj",
   "language": "python",
   "name": "mpschleisediments-bsfbf4xj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": true,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}