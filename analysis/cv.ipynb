{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.plotting.backend = 'holoviews'\n",
    "\n",
    "from sklearn.utils import parallel_backend\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "# from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "from sklearn.model_selection import GridSearchCV, LeavePOut, LeaveOneOut, cross_validate, KFold\n",
    "\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler, OneHotEncoder, FunctionTransformer\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    AdaBoostRegressor,\n",
    "    GradientBoostingRegressor,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsRegressor, RadiusNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import prepare_data\n",
    "from components import PCOA\n",
    "#from helpers import PipelineHelper, SMWrapper\n",
    "from settings import Config, shortnames, target\n",
    "from plots import scatter_chart\n",
    "from geo import get_wwtp_influence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geo import get_BAW_traces\n",
    "from geo import tracer_sedimentation_points\n",
    "from geo import get_schlei\n",
    "from shapely.geometry import Point, LineString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%prun\n"
    }
   },
   "outputs": [],
   "source": [
    "%%prun\n",
    "tracks = get_BAW_traces('../data/BAW_tracer_simulations.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trac = tracks.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for group_name, group in tracks.groupby(['simPartID', 'season', 'tracer_ESD']):\n",
    "    i+=1\n",
    "    group_before = group.copy()\n",
    "    last_valid_x = group.loc[tracks.contact_count == Config.arrest_on_nth_sedimentation, 'geometry'].values[0].x\n",
    "    last_valid_y = group.loc[tracks.contact_count == Config.arrest_on_nth_sedimentation, 'geometry'].values[0].y\n",
    "    group.loc[tracks.contact_count >= Config.arrest_on_nth_sedimentation, 'geometry'] = Point(last_valid_x, last_valid_y)\n",
    "    # tracks.loc[group.index] = group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geo import load_zipped_grid\n",
    "dem = load_zipped_grid('../data/DGM_Schlei_1982_bis_2002_UTM32.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon = get_schlei()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_lines = tracks.groupby(['season', 'tracer_ESD', 'simPartID'])['geometry'].apply(lambda x: LineString(x.tolist()*2 if len(x.tolist())==1 else x.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_lines.head(1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks.loc[(tracks.season=='summer') & (tracks.tracer_ESD==300) & (tracks.simPartID==898)]['geometry'].tolist()*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% loading data\n"
    }
   },
   "outputs": [],
   "source": [
    "tracks.sample(20000).explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% loading data\n"
    }
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# cell magic to supress output. Comment it out to see output of this cell.\n",
    "\n",
    "# What happened so far: DB extract and blank procedure. Now import resulting MP data from csv\n",
    "mp_pdd = prepare_data.get_pdd()\n",
    "\n",
    "# Also import sediment data (sediment frequencies per size bin from master sizer export)\n",
    "grainsize_iow, grainsize_cau = prepare_data.get_grainsizes()[0:2]\n",
    "scor_iow = PCOA(grainsize_iow, 2)[0]\n",
    "scor_cau = PCOA(grainsize_cau, 2)[0]\n",
    "\n",
    "# ...some data wrangling to prepare particle domain data and sample domain data for MP and combine with certain sediment aggregates.\n",
    "sdd_iow = prepare_data.aggregate_SDD(mp_pdd)\n",
    "sdd_iow = prepare_data.additional_sdd_merging(sdd_iow, how='outer')\n",
    "sdd_iow = sdd_iow.merge(scor_iow, right_index=True, left_on='Sample', how='outer')\n",
    "sdd_iow = sdd_iow.replace({'Sample': shortnames}).sort_values(by='Sample')\n",
    "\n",
    "sdd_cau = pd.read_csv('../data/Metadata_CAU_sampling_log.csv', index_col=0).join(pd.read_csv('../data/GRADISTAT_CAU_vol_log-cau_closed.csv', index_col=0), how='outer')\n",
    "sdd_cau = sdd_cau.merge(scor_cau, right_index=True, left_on='Sample', how='outer').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Additional variable generation (e.g. predictor derivatives)\n",
    " \n",
    "# sdd_iow['Dist_WWTP_revsq'] = ((1/sdd_iow['Dist_WWTP'])**3)*10000000000  # calculates the squared of the reversed Distance \n",
    "# sdd_iow['Dist_WWTP_revsq'] = (((sdd_iow['Dist_WWTP'].max()-sdd_iow['Dist_WWTP'])+1)**3)/100000000000  # calculates the squared of the reversed Distance \n",
    "# sdd_iow['Dist_WWTP_revsq'] = ((sdd_iow['Dist_WWTP'].max()/sdd_iow['Dist_WWTP'])**3)/100  # calculates the squared of the reversed Distance\n",
    "# sdd_iow['Dist_WWTP_revsq'] = ((sdd_iow['Dist_WWTP'].max()/sdd_iow['Dist_WWTP'])**2)  # calculates the squared of the reversed Distance\n",
    "\n",
    "# sdd_iow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Split data into samples used for building the model and samples used for predicting.\n",
    "\n",
    "model_data = sdd_iow.loc[~sdd_iow.Concentration.isna()].set_index('Sample')\n",
    "pred_data = sdd_iow.loc[sdd_iow.Concentration.isna()]\n",
    "pred_data = pd.concat([pred_data, sdd_cau.drop('Date',axis=1)]).set_index('Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Potential outlier exclusion\n",
    "#model_data = model_data.drop(['S08','S10d','S05','S32'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of features (predictors) to be used in the model. Beware: depending on the preprocessing steps not all features might be used.\n",
    "\n",
    "featurelist = [\n",
    "    # 'Depth',\n",
    "    # 'LON', 'LAT',\n",
    "    # 'Dist_Land',\n",
    "    # 'Dist_Marina',\n",
    "    # 'Dist_WWTP',\n",
    "    # 'WWTP_influence_as_tracer_mean_dist',\n",
    "    'WWTP_influence_as_cumulated_residence',\n",
    "    # 'WWTP_influence_as_mean_time_travelled',\n",
    "    # 'Dist_WWTP2',\n",
    "    # 'Dist_WWTP_revsq',\n",
    "    # 'MODE 1 (µm)',\n",
    "    # 'D10 (µm)',\n",
    "    'D50 (µm)',\n",
    "    # 'D90 (µm)',\n",
    "    # 'perc GRAVEL',\n",
    "    # 'perc SAND',\n",
    "    # 'perc MUD',\n",
    "    # 'perc CLAY',\n",
    "    # 'OM_D50',\n",
    "    # 'TOC',\n",
    "    # 'Hg',\n",
    "    # 'TIC',\n",
    "    # 'regio_sep',\n",
    "    # 'PC1',\n",
    "    'PC2'\n",
    "    ]\n",
    "model_X = model_data[featurelist]\n",
    "model_y = model_data[target]\n",
    "pred_X = pred_data[featurelist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mutual exclusive list (list of lists detailing predictors that are not allowed together in one model candidate)\n",
    "\n",
    "mutual_exclusive = [\n",
    "    ['D50 (µm)','PC1','perc MUD'],\n",
    "    ['Dist_WWTP','Dist_WWTP2'],\n",
    "    ['TOC', 'perc MUD'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Check some basic statistics of the target variable\n",
    "\n",
    "# model_y.describe()\n",
    "# model_y.hist()\n",
    "# model_X.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom preprocessing functions to be used in the model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create exhaustive feature selector, using leave-p-out on columns labels to generate a boolean matrix.\n",
    "\n",
    "min_features = 3  # minimum number of features to be used in the model\n",
    "\n",
    "feature_candidates_list = []\n",
    "for i in range(1,len(model_X.columns)+1-min_features):\n",
    "    lpo = LeavePOut(p=i)\n",
    "    # base_index = [False] * len(model_X.columns)\n",
    "    for candidate_indices, _ in lpo.split(model_X.columns):\n",
    "        feature_candidates = model_X.columns[candidate_indices]\n",
    "        if any(all(pd.Series(ex_feats).isin(feature_candidates)) for ex_feats in mutual_exclusive):\n",
    "            continue  # if all entries of any row in mutual_exclusive list are present in the current feature_candidates, then don't put them in the feature_candidates_list\n",
    "        feature_candidates_list.append(feature_candidates)\n",
    "feature_candidates_list.append(model_X.columns)  # also append the set of all possible features\n",
    "\n",
    "def SelectFeatures(model_X, feature_set, feature_sets=feature_candidates_list):\n",
    "    return model_X.loc[:, feature_sets[feature_set]]\n",
    "\n",
    "CustomFeatureSelector = FunctionTransformer(SelectFeatures, feature_names_out='one-to-one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# scale the data\n",
    "# scaler = StandardScaler()\n",
    "# model_X = pd.DataFrame(scaler.fit_transform(model_X), columns=model_X.columns, index=model_X.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Pipeline and parameter grid for model selection, see here for inspiration: https://towardsdatascience.com/getting-the-most-out-of-scikit-learn-pipelines-c2afc4410f1a\n",
    "\n",
    "PreProcessor = ColumnTransformer([\n",
    "      ('selector', CustomFeatureSelector, model_X.columns),\n",
    "      # ('imputer', SimpleImputer(), make_column_selector(dtype_include=np.number)),\n",
    "      ('scaler', StandardScaler(), make_column_selector(dtype_include=np.number)),\n",
    "      # ('encoder', OneHotEncoder(), make_column_selector(dtype_include=object)),\n",
    "      ])\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('preprocessor', PreProcessor),\n",
    "    ('regressor', DummyRegressor())\n",
    "    ])\n",
    "\n",
    "preprocessor_params = {\n",
    "    'preprocessor__selector': [CustomFeatureSelector],\n",
    "      'preprocessor__selector__kw_args': [{'feature_set':i} for i in range(len(feature_candidates_list))],\n",
    "    \n",
    "    # 'preprocessor__scaler': [StandardScaler()],#MaxAbsScaler(), MinMaxScaler(), RobustScaler(), QuantileTransformer(), Normalizer()],\n",
    "    #    'preprocessor__scaler__with_mean': [True, False],\n",
    "    #    'preprocessor__scaler__with_std': [True, False],\n",
    "    }\n",
    "\n",
    "regressor_params = {\n",
    "    # 'regressor': [DummyRegressor()],\n",
    "    #     'regressor__strategy': ['mean', 'median'],\n",
    "    \n",
    "    # 'regressor': [SVR()],\n",
    "    #    'regressor__C': [0.1, 1.5],\n",
    "    #    'regressor__kernel': ['linear', 'rbf', 'poly'],\n",
    "    #    'regressor__degree': [2, 3, 4, 5],\n",
    "\n",
    "    'regressor': [TweedieRegressor(max_iter=1000)],\n",
    "      'regressor__power': [2],\n",
    "      # 'regressor__power': [0, 1, 1.25, 1.5, 1.6, 1.7, 1.8, 1.9, 1.95, 1.99, 2, 3],\n",
    "      'regressor__alpha': [1], \n",
    "      'regressor__link': ['log'],#, 'identity', 'auto'],\n",
    "       #'regressor__fit_params__sample_weights': [None, model_data.loc[model_X.index, 'Mass'].to_numpy()]  # FIXME: fit_params seem not to be accepted from gridsearch params, only as argument in fit method directly...\n",
    "  \n",
    "\n",
    "    # 'regressor': [RadiusNeighborsRegressor()],\n",
    "    #     'regressor__radius': [100000, 200000],\n",
    "    #     'regressor__weights': ['uniform', 'distance'],\n",
    "    #     'regressor__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    #     'regressor__leaf_size': [10, 20, 30, 40, 50],\n",
    "    # ,\n",
    "    \n",
    "    # 'regressor': [RandomForestRegressor()],\n",
    "    #      'regressor__n_estimators': [10, 50], #[20, 50, 100, 150],\n",
    "        #  'regressor__max_features': [None, 'sqrt', 'log2'],\n",
    "        #  'regressor__min_samples_split': [2, 10],\n",
    "        #  'regressor__min_samples_leaf': [1, 5],\n",
    "        #  'regressor__bootstrap': [True],\n",
    "        #  'regressor__max_depth': [None, 5, 50],\n",
    "    #     #  'regressor__warm_start': [True, False]\n",
    "    # \n",
    "}\n",
    "\n",
    "params = {**preprocessor_params, **regressor_params}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## The pipeline is run by searching the provided paramter space using scorings of a crossvalidation technique to find out how each model candidate performs.\n",
    "\n",
    "# Number of random trials\n",
    "# NUM_TRIALS = 1\n",
    "\n",
    "# Arrays to store scores\n",
    "# nested_cvs = np.zeros(NUM_TRIALS)\n",
    "scoring = ['neg_mean_absolute_error']  # possibilities: ‘neg_root_mean_squared_error’, ‘neg_mean_squared_error’, 'r2', 'neg_mean_absolute_error', 'neg_mean_squared_log_error'\n",
    "\n",
    "# Loop for each trial\n",
    "# for i in range(NUM_TRIALS):\n",
    "with parallel_backend('loky', n_jobs=-1):'):\n",
    "    innerCV = GridSearchCV(\n",
    "        pipe,\n",
    "        params,\n",
    "        scoring= scoring,\n",
    "        refit= scoring[0],\n",
    "        cv=LeaveOneOut(),\n",
    "        verbose=1,\n",
    "        # n_jobs=-1\n",
    "        )\n",
    "    #grid.fit(model_X, model_y)#, regressor__fit_params={'sample_weight': model_data.loc[model_X.index, 'Mass'].to_numpy()})\n",
    "    outerCV = cross_validate(\n",
    "        innerCV,\n",
    "        model_X,\n",
    "        model_y,\n",
    "        scoring=scoring,\n",
    "        cv=8,\n",
    "        return_train_score=True,\n",
    "        return_estimator=True,\n",
    "        verbose=1,\n",
    "        # n_jobs=-1\n",
    "        )\n",
    "    # nested_cvs[i] = nested_cv.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a df of all outer cv results and show it sorted by the best scoring metric\n",
    "outerCV_df = pd.DataFrame(outerCV)\n",
    "outerCV_df.sort_values(by=f'test_{scoring[0]}', ascending=False, inplace=True)\n",
    "outerCV_df.rename_axis(index='outerCV_fold', inplace=True)\n",
    "\n",
    "## Get best model params for each of the outer cv folds:\n",
    "best_params_df = pd.DataFrame()\n",
    "for i, model in enumerate(outerCV['estimator']):\n",
    "    best_params = model.best_params_\n",
    "    # best_params_df = pd.concat([best_params_df, pd.DataFrame(best_params, index=[i])])  # this does not work when RandomForestRegressor is used, because some internals call len() on the values of the best_params dict, which raises AttributeError: 'RandomForestRegressor' object has no attribute 'estimators_'\n",
    "    # instead filling df with for-loop...:\n",
    "    current_best_params_df = pd.DataFrame()\n",
    "    for key, value in best_params.items():\n",
    "        current_best_params_df[key] = [value]\n",
    "        current_best_params_df.index = [i]\n",
    "    best_params_df = pd.concat([best_params_df, current_best_params_df])\n",
    "\n",
    "outerCV_df.join(best_params_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outerCV_df.estimator[0].predict(model_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get inner cv results of the outer cv fold which achieved the best scoring metric\n",
    "innerCV_df = pd.DataFrame(outerCV_df.loc[outerCV_df.test_neg_mean_absolute_error.idxmax(), 'estimator'].cv_results_)\n",
    "innerCV_df.sort_values(by='rank_test_neg_mean_absolute_error', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing score of the best performing model candidate and its parameters.\n",
    "\n",
    "print(f'{scoring}: {outerCV[\"estimator\"][0].score(model_X, model_y)}')\n",
    "print(outerCV['estimator'][0].best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outerCV['estimator'][0].best_estimator_.named_steps['preprocessor'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting feature names\n",
    "# [grid.best_estimator_.named_steps['preprocessor'].named_transformers_['selector'].get_feature_names_out(input_features=model_X.columns.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show how the model performs on the training data\n",
    "\n",
    "train_pred_y = grid.predict(model_X)  # use the best model to predict the data on the same samples that were used to train the model\n",
    "print(f'R² = {r2_score(model_y, train_pred_y)}')  # adjusted R² = 1 - (1 - R²) * (n - 1) / (n - p) with n = number of samples, p = number of features\n",
    "\n",
    "df = pd.concat([\n",
    "    model_data.loc[model_y.index].regio_sep,\n",
    "    model_y,\n",
    "    pd.Series(\n",
    "        train_pred_y,\n",
    "        name='Prediction',\n",
    "        index=model_y.index)\n",
    "        ],\n",
    "    axis=1\n",
    "    ).reset_index()\n",
    "    \n",
    "scatter_chart(df, target, 'Prediction', color='regio_sep', labels='Sample', identity=True, equal_axes=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Take a look at all model candidates and their performance\n",
    "\n",
    "scores = pd.DataFrame(grid.cv_results_)\n",
    "# scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('MPSchleiSediments-z4CtktJ9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "82a31816b63c673b7463547b8d8376fc489f1e362f1a21f244a13819d6095661"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
