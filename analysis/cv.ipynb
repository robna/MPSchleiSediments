{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.plotting.backend = 'holoviews'\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sklearn\n",
    "print(f'sklearn verion: {sklearn.__version__}')\n",
    "from sklearn.utils import parallel_backend\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "# from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "from sklearn.model_selection import GridSearchCV, LeavePOut, LeaveOneOut, cross_validate, KFold, PredefinedSplit\n",
    "\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MaxAbsScaler, MinMaxScaler, OneHotEncoder, FunctionTransformer\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    AdaBoostRegressor,\n",
    "    GradientBoostingRegressor,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsRegressor, RadiusNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error, median_absolute_error\n",
    "\n",
    "from sklearn import clone, set_config\n",
    "# set_config(transform_output='pandas')  # only works for sklearn >= 1.2\n",
    "\n",
    "try:  # if on phy-server local modules will not be found if their directory is not added to PATH\n",
    "    import sys\n",
    "    sys.path.append(\"/silod7/lenz/MPSchleiSediments/analysis/\")\n",
    "    import os\n",
    "    os.chdir(\"/silod7/lenz/MPSchleiSediments/analysis/\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import prepare_data\n",
    "from components import PCOA\n",
    "#from helpers import PipelineHelper, SMWrapper\n",
    "from settings import Config, shortnames, target\n",
    "from plots import scatter_chart\n",
    "from cv import generate_feature_sets, best_scored, get_median_cv_scores, SelectFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% loading data\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# cell magic to supress output. Comment it out to see output of this cell.\n",
    "\n",
    "# What happened so far: DB extract and blank procedure. Now import resulting MP data from csv\n",
    "mp_pdd = prepare_data.get_pdd()\n",
    "\n",
    "# Also import sediment data (sediment frequencies per size bin from master sizer export)\n",
    "grainsize_iow, grainsize_cau = prepare_data.get_grainsizes()[0:2]\n",
    "scor_iow = PCOA(grainsize_iow, 2)[0]\n",
    "scor_cau = PCOA(grainsize_cau, 2)[0]\n",
    "\n",
    "# ...some data wrangling to prepare particle domain data and sample domain data for MP and combine with certain sediment aggregates.\n",
    "sdd_iow = prepare_data.aggregate_SDD(mp_pdd)\n",
    "sdd_iow = prepare_data.additional_sdd_merging(sdd_iow, how='outer')\n",
    "sdd_iow = sdd_iow.merge(scor_iow, right_index=True, left_on='Sample', how='outer')\n",
    "sdd_iow = sdd_iow.replace({'Sample': shortnames}).sort_values(by='Sample')\n",
    "\n",
    "sdd_cau = pd.read_csv('../data/Metadata_CAU_sampling_log.csv', index_col=0).join(pd.read_csv('../data/GRADISTAT_CAU_vol_log-cau_closed.csv', index_col=0), how='outer')\n",
    "sdd_cau = sdd_cau.merge(scor_cau, right_index=True, left_on='Sample', how='outer').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Additional variable generation (e.g. predictor derivatives)\n",
    " \n",
    "# sdd_iow['Dist_WWTP_revsq'] = ((1/sdd_iow['Dist_WWTP'])**3)*10000000000  # calculates the squared of the reversed Distance \n",
    "# sdd_iow['Dist_WWTP_revsq'] = (((sdd_iow['Dist_WWTP'].max()-sdd_iow['Dist_WWTP'])+1)**3)/100000000000  # calculates the squared of the reversed Distance \n",
    "# sdd_iow['Dist_WWTP_revsq'] = ((sdd_iow['Dist_WWTP'].max()/sdd_iow['Dist_WWTP'])**3)/100  # calculates the squared of the reversed Distance\n",
    "# sdd_iow['Dist_WWTP_revsq'] = ((sdd_iow['Dist_WWTP'].max()/sdd_iow['Dist_WWTP'])**2)  # calculates the squared of the reversed Distance\n",
    "\n",
    "# sdd_iow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Split data into samples used for building the model and samples used for predicting.\n",
    "\n",
    "model_data = sdd_iow.loc[~sdd_iow.Concentration.isna()].set_index('Sample')\n",
    "pred_data = sdd_iow.loc[sdd_iow.Concentration.isna()]\n",
    "pred_data = pd.concat([pred_data, sdd_cau.drop('Date',axis=1)]).set_index('Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Potential outlier exclusion\n",
    "droplist = []\n",
    "model_data = model_data.drop(droplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of features (predictors) to be used in the model. Beware: depending on the preprocessing steps not all features might be used.\n",
    "\n",
    "featurelist = [\n",
    "    'Depth',\n",
    "    # 'LON', 'LAT',\n",
    "    'Dist_Land',\n",
    "    # 'Dist_Marina',\n",
    "    'Dist_WWTP',\n",
    "    # 'WWTP_influence_as_tracer_mean_dist',\n",
    "    # 'WWTP_influence_as_cumulated_residence',\n",
    "    # 'WWTP_influence_as_mean_time_travelled',\n",
    "    #'WWTP_influence_as_tracer_mean_dist__sed_18µm_allseasons_444',\n",
    "    #'WWTP_influence_as_endpoints_mean_dist__sed_18µm_allseasons_444',\n",
    "    #'WWTP_influence_as_cumulated_residence__sed_18µm_allseasons_444',\n",
    "    #'WWTP_influence_as_mean_time_travelled__sed_18µm_allseasons_444',\n",
    "    #'WWTP_influence_as_tracer_mean_dist__nosed_18µm_spring_444',\n",
    "    #'WWTP_influence_as_endpoints_mean_dist__nosed_18µm_spring_444',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_18µm_spring_444',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_18µm_spring_444',\n",
    "    #'WWTP_influence_as_tracer_mean_dist__nosed_allsizes_spring_444',\n",
    "    #'WWTP_influence_as_endpoints_mean_dist__nosed_allsizes_spring_444',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_allsizes_spring_444',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_allsizes_spring_444',\n",
    "    #'WWTP_influence_as_tracer_mean_dist__sed_18µm_autumn_222',\n",
    "    #'WWTP_influence_as_endpoints_mean_dist__sed_18µm_autumn_222',\n",
    "    'WWTP_influence_as_cumulated_residence__sed_18µm_autumn_222',\n",
    "    'WWTP_influence_as_mean_time_travelled__sed_18µm_autumn_222',\n",
    "    #'WWTP_influence_as_tracer_mean_dist__sed_18µm_autumn_444',\n",
    "    #'WWTP_influence_as_endpoints_mean_dist__sed_18µm_autumn_444',\n",
    "    #'WWTP_influence_as_cumulated_residence__sed_18µm_autumn_444',\n",
    "    #'WWTP_influence_as_mean_time_travelled__sed_18µm_autumn_444',\n",
    "    #'WWTP_influence_as_tracer_mean_dist__nosed_18µm_autumn_222',\n",
    "    #'WWTP_influence_as_endpoints_mean_dist__nosed_18µm_autumn_222',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_18µm_autumn_222',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_18µm_autumn_222',\n",
    "    #'WWTP_influence_as_tracer_mean_dist__nosed_allsizes_autumn_222',\n",
    "    #'WWTP_influence_as_endpoints_mean_dist__nosed_allsizes_autumn_222',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_allsizes_autumn_222',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_allsizes_autumn_222',\n",
    "    #'WWTP_influence_as_tracer_mean_dist__nosed_allsizes_allseasons_444',            # *\n",
    "    #'WWTP_influence_as_endpoints_mean_dist__nosed_allsizes_allseasons_444',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_allsizes_allseasons_444',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_allsizes_allseasons_444',         # *\n",
    "    #'WWTP_influence_as_tracer_mean_dist__sed_allsizes_allseasons_444',              # *\n",
    "    #'WWTP_influence_as_endpoints_mean_dist__sed_allsizes_allseasons_444',\n",
    "    #'WWTP_influence_as_cumulated_residence__sed_allsizes_allseasons_444',\n",
    "    #'WWTP_influence_as_mean_time_travelled__sed_allsizes_allseasons_444',           # *\n",
    "    #'WWTP_influence_as_tracer_mean_dist__sed_18µm_allseasons_222',\n",
    "    #'WWTP_influence_as_endpoints_mean_dist__sed_18µm_allseasons_222',\n",
    "    #'WWTP_influence_as_cumulated_residence__sed_18µm_allseasons_222',\n",
    "    #'WWTP_influence_as_mean_time_travelled__sed_18µm_allseasons_222',\n",
    "    #'WWTP_influence_as_tracer_mean_dist__nosed_18µm_allseasons_222',\n",
    "    #'WWTP_influence_as_endpoints_mean_dist__nosed_18µm_allseasons_222',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_18µm_allseasons_222',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_18µm_allseasons_222',\n",
    "    #'WWTP_influence_as_tracer_mean_dist__nosed_18µm_spring_222',\n",
    "    #'WWTP_influence_as_endpoints_mean_dist__nosed_18µm_spring_222',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_18µm_spring_222',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_18µm_spring_222',\n",
    "    #'WWTP_influence_as_tracer_mean_dist__nosed_allsizes_allseasons_222',\n",
    "    #'WWTP_influence_as_endpoints_mean_dist__nosed_allsizes_allseasons_222',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_allsizes_allseasons_222',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_allsizes_allseasons_222',\n",
    "    #'WWTP_influence_as_tracer_mean_dist__nosed_allsizes_autumn_444',\n",
    "    #'WWTP_influence_as_endpoints_mean_dist__nosed_allsizes_autumn_444',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_allsizes_autumn_444',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_allsizes_autumn_444',\n",
    "    #'WWTP_influence_as_tracer_mean_dist__sed_18µm_spring_444',\n",
    "    #'WWTP_influence_as_endpoints_mean_dist__sed_18µm_spring_444',\n",
    "    #'WWTP_influence_as_cumulated_residence__sed_18µm_spring_444',\n",
    "    #'WWTP_influence_as_mean_time_travelled__sed_18µm_spring_444',\n",
    "    #'WWTP_influence_as_tracer_mean_dist__nosed_18µm_autumn_444',\n",
    "    #'WWTP_influence_as_endpoints_mean_dist__nosed_18µm_autumn_444',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_18µm_autumn_444',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_18µm_autumn_444',\n",
    "    #'WWTP_influence_as_tracer_mean_dist__sed_18µm_spring_222',\n",
    "    #'WWTP_influence_as_endpoints_mean_dist__sed_18µm_spring_222',\n",
    "    #'WWTP_influence_as_cumulated_residence__sed_18µm_spring_222',\n",
    "    #'WWTP_influence_as_mean_time_travelled__sed_18µm_spring_222',\n",
    "    #'WWTP_influence_as_tracer_mean_dist__sed_allsizes_allseasons_222',\n",
    "    #'WWTP_influence_as_endpoints_mean_dist__sed_allsizes_allseasons_222',\n",
    "    #'WWTP_influence_as_cumulated_residence__sed_allsizes_allseasons_222',\n",
    "    #'WWTP_influence_as_mean_time_travelled__sed_allsizes_allseasons_222',\n",
    "    #'WWTP_influence_as_tracer_mean_dist__nosed_allsizes_spring_222',\n",
    "    #'WWTP_influence_as_endpoints_mean_dist__nosed_allsizes_spring_222',\n",
    "    \n",
    "    'WWTP_influence_as_cumulated_residence__nosed_allsizes_spring_222',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_allsizes_spring_222',\n",
    "    'WWTP_influence_as_tracer_mean_dist__nosed_0µm_autumn_222',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__nosed_0µm_autumn_222',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_0µm_autumn_222',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_0µm_autumn_222',\n",
    "    'WWTP_influence_as_tracer_mean_dist__sed_0µm_allseasons_222',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__sed_0µm_allseasons_222',\n",
    "    'WWTP_influence_as_cumulated_residence__sed_0µm_allseasons_222',\n",
    "    'WWTP_influence_as_mean_time_travelled__sed_0µm_allseasons_222',\n",
    "    'WWTP_influence_as_tracer_mean_dist__nosed_0µm_summer_444',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__nosed_0µm_summer_444',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_0µm_summer_444',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_0µm_summer_444',\n",
    "    'WWTP_influence_as_tracer_mean_dist__nosed_0µm_autumnspring_222',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__nosed_0µm_autumnspring_222',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_0µm_autumnspring_222',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_0µm_autumnspring_222',\n",
    "    'WWTP_influence_as_tracer_mean_dist__nosed_0µm_autumn_444',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__nosed_0µm_autumn_444',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_0µm_autumn_444',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_0µm_autumn_444',\n",
    "    'WWTP_influence_as_tracer_mean_dist__nosed_0µm_spring_222',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__nosed_0µm_spring_222',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_0µm_spring_222',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_0µm_spring_222',\n",
    "    'WWTP_influence_as_tracer_mean_dist__nosed_0µm_autumnspring_444',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__nosed_0µm_autumnspring_444',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_0µm_autumnspring_444',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_0µm_autumnspring_444',\n",
    "    'WWTP_influence_as_tracer_mean_dist__sed_0µm_allseasons_444',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__sed_0µm_allseasons_444',\n",
    "    'WWTP_influence_as_cumulated_residence__sed_0µm_allseasons_444',\n",
    "    'WWTP_influence_as_mean_time_travelled__sed_0µm_allseasons_444',\n",
    "    'WWTP_influence_as_tracer_mean_dist__nosed_0µm_summer_222',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__nosed_0µm_summer_222',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_0µm_summer_222',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_0µm_summer_222',\n",
    "    'WWTP_influence_as_tracer_mean_dist__nosed_0µm_spring_444',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__nosed_0µm_spring_444',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_0µm_spring_444',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_0µm_spring_444',\n",
    "    'WWTP_influence_as_tracer_mean_dist__nosed_0µm_allseasons_444',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__nosed_0µm_allseasons_444',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_0µm_allseasons_444',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_0µm_allseasons_444',\n",
    "    'WWTP_influence_as_tracer_mean_dist__nosed_0µm_allseasons_222_',\n",
    "    'WWTP_influence_as_endpoints_mean_dist__nosed_0µm_allseasons_222_',\n",
    "    'WWTP_influence_as_cumulated_residence__nosed_0µm_allseasons_222_',\n",
    "    'WWTP_influence_as_mean_time_travelled__nosed_0µm_allseasons_222_',\n",
    "    \n",
    "    # 'Dist_WWTP2',\n",
    "    # 'Dist_WWTP_revsq',\n",
    "    #'MODE 1 (µm)',\n",
    "    # 'D10 (µm)',\n",
    "    'D50 (µm)',\n",
    "    # 'D90 (µm)',\n",
    "    # 'perc GRAVEL',\n",
    "    # 'perc SAND',\n",
    "    'perc MUD',\n",
    "    # 'perc CLAY',\n",
    "    # 'OM_D50',\n",
    "    'TOC',\n",
    "    # 'Hg',\n",
    "    # 'TIC',\n",
    "    # 'regio_sep',\n",
    "    'PC1',\n",
    "    'PC2'\n",
    "    ]\n",
    "model_X = model_data[featurelist]\n",
    "model_y = model_data[target]\n",
    "pred_X = pred_data[featurelist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = None  # this is just needed for reporting as long as scaler is manually switched outside the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data using StandardScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# model_X.values[:] = scaler.fit_transform(model_X)\n",
    "# pred_X.values[:] = scaler.transform(pred_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Check some basic statistics of the target variable\n",
    "\n",
    "# model_y.describe()\n",
    "# model_y.hist()\n",
    "# model_X.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom preprocessing functions to be used in the model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create exhaustive feature selector, using leave-p-out on columns labels to generate a boolean matrix.\n",
    "num_feat = (2,3)  # allowed number of features:\n",
    "                  #     if int: all possible combinations of this length will be created\n",
    "                  #     if tuple (min, max): all possible combinations of length min upt to length max will be created \n",
    "feature_candidates_list = generate_feature_sets(model_X, Config.mutual_exclusive, Config.exclusive_keywords, num_feat=num_feat, n_jobs=1, save=True)\n",
    "\n",
    "CustomFeatureSelector = FunctionTransformer(SelectFeatures)#, feature_names_out='one-to-one')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Pipeline and parameter grid for model selection, see here for inspiration: https://towardsdatascience.com/getting-the-most-out-of-scikit-learn-pipelines-c2afc4410f1a\n",
    "\n",
    "PreProcessor = ColumnTransformer([\n",
    "      ('selector', CustomFeatureSelector, model_X.columns),\n",
    "      # ('imputer', SimpleImputer(), make_column_selector(dtype_include=np.number)),\n",
    "      # ('scaler', StandardScaler(), make_column_selector(dtype_include=np.number)),\n",
    "      # ('encoder', OneHotEncoder(), make_column_selector(dtype_include=object)),\n",
    "      ])\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('preprocessor', PreProcessor),\n",
    "    ('regressor', DummyRegressor())\n",
    "    ])\n",
    "\n",
    "preprocessor_params = [{\n",
    "    # 'preprocessor__selector': [CustomFeatureSelector],\n",
    "      'preprocessor__selector__kw_args': [{'feature_set': i,\n",
    "                                           'feature_sets': feature_candidates_list\n",
    "                                          } for i in range(len(feature_candidates_list))],\n",
    "    \n",
    "    # 'preprocessor__scaler': [StandardScaler()],#MaxAbsScaler(), MinMaxScaler(), RobustScaler(), QuantileTransformer(), Normalizer()],\n",
    "    #    'preprocessor__scaler__with_mean': [True],\n",
    "    #    'preprocessor__scaler__with_std': [True],\n",
    "    }]\n",
    "\n",
    "regressor_params = [\n",
    "    # {\n",
    "    # 'regressor': [DummyRegressor()],\n",
    "    #     'regressor__strategy': ['median'],\n",
    "    # },\n",
    "    \n",
    "    # {\n",
    "    # 'regressor': [SVR()],\n",
    "    #    'regressor__C': [0.1, 1, 1.5, 10, 20],\n",
    "    #    'regressor__kernel': ['linear', 'rbf', 'poly'],\n",
    "    #    'regressor__degree': [2, 3, 4, 5],\n",
    "    # },\n",
    "\n",
    "    {\n",
    "    'regressor': [TweedieRegressor(max_iter=100000)],\n",
    "      'regressor__power': [2],\n",
    "      # 'regressor__power': [0, 1, 1.2, 1.5, 1.9, 2, 3],\n",
    "      # 'regressor__alpha': [0, 0.2, 1, 2, 5, 10], \n",
    "      'regressor__link': ['log'],#, 'identity', 'auto'],\n",
    "      # 'regressor__fit_intercept': [True, False],\n",
    "      # 'regressor__warm_start': [True, False],\n",
    "      ## 'regressor__fit_params__sample_weights': [None, model_data.loc[model_X.index, 'Mass'].to_numpy()],  # FIXME: fit_params seem not to be accepted from gridsearch params, only as argument in fit method directly...\n",
    "    },\n",
    "  \n",
    "#     {\n",
    "#     'regressor': [RadiusNeighborsRegressor()],\n",
    "#         'regressor__radius': [1000, 10000, 100000],\n",
    "#         'regressor__weights': ['uniform', 'distance'],\n",
    "#         'regressor__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "#         'regressor__leaf_size': [10, 20, 30, 40, 50],\n",
    "#     },\n",
    "    \n",
    "    # {\n",
    "    # 'regressor': [RandomForestRegressor()],\n",
    "    #      'regressor__random_state': [0],\n",
    "    #      'regressor__n_estimators': [20, 150, 500],\n",
    "    #      'regressor__max_depth': [None, 3, 7],\n",
    "    #      'regressor__max_features': [None, 'sqrt', 'log2'],\n",
    "    #      'regressor__min_samples_split': [2, 4, 10],\n",
    "    #      'regressor__min_samples_leaf': [1, 3, 5],\n",
    "    #      'regressor__bootstrap': [True, False],\n",
    "    #      'regressor__oob_score': [True, False],\n",
    "    #      'regressor__warm_start': [True, False],\n",
    "    # },\n",
    "    \n",
    "    # {\n",
    "    # 'regressor': [GradientBoostingRegressor()],\n",
    "    #     'regressor__loss': ['squared_error', 'huber', 'quantile'],\n",
    "    #     'regressor__learning_rate': [0.01, 0.1, 0.5],  \n",
    "    #     'regressor__n_estimators': [100, 200, 500],\n",
    "    #     'regressor__subsample': [0.5, 1.0],\n",
    "    #     'regressor__criterion': ['squared_error', 'friedman_mse'],\n",
    "    #     'regressor__min_samples_split': [2, 10],\n",
    "    #     'regressor__min_samples_leaf': [1, 5],\n",
    "    #     'regressor__max_depth': [2, 3, 5],\n",
    "    #     'regressor__min_weight_fraction_leaf': [0.0, 0.1],\n",
    "    #     'regressor__max_features': [None, 'sqrt', 'log2'],\n",
    "#         'regressor__max_leaf_nodes': [None, 5, 10],\n",
    "#         'regressor__min_impurity_decrease': [0.0, 0.1],\n",
    "#         'regressor__min_impurity_split': [None, 0.1],\n",
    "#         'regressor__alpha': [0.9, 0.95, 0.99, 0.999],\n",
    "#         'regressor__tol': [0.0001, 0.001, 0.01],\n",
    "#         'regressor__validation_fraction': [0.1, 0.2],\n",
    "#         'regressor__n_iter_no_change': [None, 5, 10],\n",
    "#         'regressor__ccp_alpha': [0.0, 0.1],\n",
    "#         'regressor__warm_start': [True, False],\n",
    "    # },\n",
    "]\n",
    "\n",
    "params = [{**dict_pre, **dict_reg} for dict_reg in regressor_params for dict_pre in preprocessor_params]\n",
    "# params = regressor_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## The pipeline is run by searching the provided parameter space using scorings of a crossvalidation technique to find out how each model candidate performs.\n",
    "\n",
    "warnings.filterwarnings('ignore')  # ignore warnings to avoid flooding the gridsearch output with repetitive messages (works for single cpu)\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"  # ignore warnings to avoid flooding the gridsearch output with repetitive messages (works for parallel)\n",
    "\n",
    "Config.scoring = {  # this dict is defined in settings.Config, but may be overwritten here for convenience\n",
    "    'R2': 'r2',\n",
    "    'MAPE': 'neg_mean_absolute_percentage_error',\n",
    "    'MedAE': 'neg_median_absolute_error',\n",
    "    # 'MSLE': 'neg_mean_squared_log_error',\n",
    "  }\n",
    "Config.refit_scorer = 'MedAE'\n",
    "Config.select_best = 'median'\n",
    "\n",
    "# Predefined test set instead of CV:\n",
    "test_set = ('S30', 'S03', 'S15', 'S06', 'S31', 'S25', 'S20')  # possible samples to use as a predefined test set: ordered by relevance\n",
    "test_set_size = 7  # Requires int, should be 0 < test_set_size <= len(test_set), for using the n first samples as test_set\n",
    "test_set = test_set[0:test_set_size]\n",
    "test_set = model_X.index.isin(test_set).astype(int) - 1  # returns array of len(model_X.index) with -1 for training samples and 0 for testing samples\n",
    "\n",
    "cv_scheme_inner = LeaveOneOut()\n",
    "cv_scheme_outer = 10  # use `PredefinedSplit(test_set)` for a single fold with test set as defined above\n",
    "\n",
    "# with parallel_backend('loky', n_jobs=-1):\n",
    "innerCV = GridSearchCV(\n",
    "    pipe,\n",
    "    params,\n",
    "    scoring=Config.scoring,\n",
    "    refit=best_scored,\n",
    "    cv=cv_scheme_inner,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    "    )\n",
    "\n",
    "outerCV = cross_validate(\n",
    "    innerCV,\n",
    "    model_X,\n",
    "    model_y,\n",
    "    scoring=Config.scoring,\n",
    "    cv=cv_scheme_outer,\n",
    "    return_train_score=True,\n",
    "    return_estimator=True,\n",
    "    verbose=1,\n",
    "    # n_jobs=-1\n",
    "    )\n",
    "\n",
    "get_median_cv_scores(outerCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model\n",
    "savelist = [\n",
    "    'model_X',\n",
    "    'model_y',\n",
    "    'feature_candidates_list',\n",
    "    'outerCV',\n",
    "    'params'    \n",
    "]\n",
    "savedict = {key: eval(key) for key in savelist}\n",
    "savestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "fp = '../data/exports/models/'\n",
    "with open(fp+f'{savestamp}.pkl', 'wb') as f:\n",
    "    # pickle.dump(outerCV, f)\n",
    "    joblib.dump(savedict, f, compress=1)\n",
    "\n",
    "# write settings of exported run to csv\n",
    "header = f'''\n",
    "\n",
    "\n",
    "---------------------------------------------------------------------------------------------\n",
    "---------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "Model run: {savestamp}\n",
    "Outliers excluded: {droplist}\n",
    "Scaler: {scaler}\n",
    "n feature combinations: {num_feat}\n",
    "Regressors: {regressor_params}\n",
    "\n",
    "Scorer      for evaluation: {Config.refit_scorer}  (outer fold results are sorted by this!)\n",
    "Aggregation for evaluation: {Config.select_best}\n",
    "\n",
    "CV schemes:\n",
    "    inner: {cv_scheme_inner}\n",
    "    outer: {cv_scheme_outer}\n",
    "    \n",
    "---------------------------------------------------------------------------------------------\n",
    "\n",
    "'''\n",
    "\n",
    "fn = fp + 'model_results.csv'\n",
    "with open(fn, mode='a' if Path(fn).exists() else 'w', encoding='utf-8') as f:\n",
    "    f.write(header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a df of all outer cv results and show it sorted by the best scoring metric\n",
    "outerCV_df = pd.DataFrame(outerCV)\n",
    "outerCV_df.rename_axis(index='outerCV_fold', inplace=True)\n",
    "\n",
    "## Get best model params for each of the outer cv folds:\n",
    "best_params_df = pd.DataFrame()\n",
    "for i, model in enumerate(outerCV['estimator']):\n",
    "    best_params = model.best_params_\n",
    "    # best_params_df = pd.concat([best_params_df, pd.DataFrame(best_params, index=[i])])  # this does not work when RandomForestRegressor is used, because some internals call len() on the values of the best_params dict, which raises AttributeError: 'RandomForestRegressor' object has no attribute 'estimators_'\n",
    "    # instead filling df with for-loop...:\n",
    "    current_best_params_df = pd.DataFrame()\n",
    "    for key, value in best_params.items():\n",
    "        current_best_params_df[key] = [value]\n",
    "        current_best_params_df.index = [i]\n",
    "    best_params_df = pd.concat([best_params_df, current_best_params_df])\n",
    "\n",
    "results = outerCV_df.join(best_params_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_summary = results.copy().drop(['estimator', 'fit_time', 'score_time'], axis=1)\n",
    "\n",
    "# get names of features used by the models\n",
    "if 'preprocessor__selector__kw_args' in results.columns:\n",
    "    results_summary.rename(columns={'preprocessor__selector__kw_args': 'features'}, inplace=True)\n",
    "    s = results_summary.features.apply(lambda x: [x['feature_set'], feature_candidates_list[x['feature_set']]])\n",
    "    d = pd.DataFrame.from_dict(dict(zip(s.index, s.values))).T\n",
    "    results_summary.features, results_summary['feature_combi_ID'] = d[1], d[0]\n",
    "results_summary.drop(list(results_summary.filter(regex='regressor__')), axis=1, inplace=True)\n",
    "\n",
    "# calculate scores of the best model for each outer cv fold against all data\n",
    "results_summary['allSamples_R2'] = [r2_score(model_y, outerCV['estimator'][i].predict(model_X)) for i in range(len(results_summary))]\n",
    "results_summary['allSamples_MAPE'] = [mean_absolute_percentage_error(model_y, outerCV['estimator'][i].predict(model_X)) for i in range(len(results_summary))]\n",
    "results_summary['allSamples_MedAE'] = [median_absolute_error(model_y, outerCV['estimator'][i].predict(model_X)) for i in range(len(results_summary))]\n",
    "\n",
    "# now refit all models in outerCV on all data\n",
    "outerCV['estimator_refit_on_all'] = [clone(outerCV['estimator'][i].best_estimator_.named_steps['regressor']).fit(model_X[results_summary.features.loc[i]], model_y) for i, _ in enumerate(outerCV['estimator'])]\n",
    "\n",
    "# calculate scores against all data again after refitting\n",
    "results_summary['allSamples_R2_refit'] = [r2_score(model_y, outerCV['estimator_refit_on_all'][i].predict(model_X[results_summary.features.loc[i]])) for i in range(len(results_summary))]\n",
    "results_summary['allSamples_MAPE_refit'] = [mean_absolute_percentage_error(model_y, outerCV['estimator_refit_on_all'][i].predict(model_X[results_summary.features.loc[i]])) for i in range(len(results_summary))]\n",
    "results_summary['allSamples_MedAE_refit'] = [median_absolute_error(model_y, outerCV['estimator_refit_on_all'][i].predict(model_X[results_summary.features.loc[i]])) for i in range(len(results_summary))]\n",
    "\n",
    "# Sort results\n",
    "results_summary.sort_values(by=f'test_{Config.refit_scorer}', ascending=False, inplace=True)\n",
    "\n",
    "# Save results\n",
    "results_summary.to_csv(fn, mode='a', sep=';')\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top k results of inner CV sorted by best Config.refit_scorer \n",
    "\n",
    "# outer_fold = 0  # manuaklly chose which outer fold to look at\n",
    "# top_k = 10  # how many model candidates from the inner model to show?\n",
    "# pd.DataFrame(outerCV['estimator'][outer_fold].cv_results_).sort_values(f'rank_by_median_test_{Config.refit_scorer}', ascending=True).head(top_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing score of the best performing model candidate and its parameters.\n",
    "\n",
    "outer_fold = 1  # OBS: using only one of the outer-folds models here... e.g. [0]\n",
    "\n",
    "# print(f'{Config.scoring[Config.refit_scorer]}: {outerCV[\"estimator\"][outer_fold].score(model_X, model_y)}')  \n",
    "#print(outerCV['estimator'][outer_fold].best_params_)\n",
    "\n",
    "## Plotting the results of the best performing model candidate.\n",
    "df = pd.DataFrame(zip(model_y, outerCV['estimator_refit_on_all'][outer_fold].predict(model_X[results_summary.features.loc[outer_fold]])), index=model_X.index, columns=[target, 'predicted'])\n",
    "from plots import scatter_chart\n",
    "scatter_chart(df.reset_index(), target, 'predicted',\n",
    "                                 labels='Sample',\n",
    "                                 identity=True,\n",
    "                                 equal_axes=True,\n",
    "                                 # xscale='log', yscale='log',\n",
    "                                #  xtransform=True, ytransform=True,\n",
    "                                 width=800, height=800,\n",
    "                                 title='yhat vs. y')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_fold = 0\n",
    "\n",
    "model = outerCV['estimator_refit_on_all'][outer_fold]\n",
    "print(best_params_df.loc[outer_fold, 'regressor'])\n",
    "print(f'R2 of model retrained with all samples, tested against all samples: {r2_score(model_y, model.predict(model_X[results_summary.features.loc[outer_fold]]))}')\n",
    "print(f'Intercept: {model.intercept_}')\n",
    "print(f'Coeffs: {model.coef_}')\n",
    "print(feature_candidates_list[best_params_df.loc[outer_fold, 'preprocessor__selector__kw_args']['feature_set']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Number of coefficients in each outer folds best model: ', [outerCV['estimator'][i].best_estimator_.named_steps['regressor'].n_features_in_ for i in range(len(outerCV['estimator']))])\n",
    "# print('Coeffs: ', *[outerCV['estimator'][i].best_estimator_.named_steps['regressor'].coef_ for i in range(len(outerCV['estimator']))], sep='\\n')\n",
    "# results.estimator[0].best_estimator_.named_steps['preprocessor'].transformers[0][1].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [outerCV['estimator'][0].best_estimator_.named_steps['regressor'].estimators_[i].get_n_leaves() for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results['estimator'].apply(lambda x: x.score(model_X, model_y))\n",
    "# pd.DataFrame.from_dict(dict(zip(s.values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2_all_but_no_refit = results['estimator'].apply(lambda x: r2_score(model_y, x.predict(model_X)))\n",
    "# r2_all_but_no_refit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get inner cv results of the outer cv fold which achieved the best scoring metric\n",
    "# innerCV_df = pd.DataFrame(outerCV_df.loc[outerCV_df[f'test_score'].idxmax(), 'estimator'].cv_results_)\n",
    "# innerCV_df.sort_values(by=f'rank_test_{scoring[0]}', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outerCV['estimator'][0].best_estimator_.named_steps['preprocessor'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting feature names\n",
    "# [grid.best_estimator_.named_steps['preprocessor'].named_transformers_['selector'].get_feature_names_out(input_features=model_X.columns.tolist())]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('3.9.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9283b71a9fb260974dcb1b3d32e79d34b5da211280b6af559776f5f59ea44de2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
