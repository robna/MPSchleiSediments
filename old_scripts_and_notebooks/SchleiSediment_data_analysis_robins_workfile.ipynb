{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0be933",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "import altair as alt\n",
    "alt.data_transformers.disable_max_rows()\n",
    "import altair_transform\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1223fd4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import quality checked and blank substracted list of MP from Micropoll_SchleiSediment_blank_subtract.ipynb\n",
    "env_MP = pd.read_csv('../csv/env_MP_clean_list_SchleiSediments.csv',index_col=0)\n",
    "#rename column name of Size_1\n",
    "env_MP.rename(columns = {'Size_1_[µm]':'Size_1_µm'}, inplace = True)\n",
    "#env_MP_a500 = env_MP.loc[env_MP.size_geom_mean >= 500]\n",
    "#env_MP_b500 = env_MP.loc[env_MP.size_geom_mean < 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11092641",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mp_station = env_MP.groupby(['Sample']).agg(\n",
    "        Frequency=('Site_name', 'count'),  # using 'Site_name' here for count, could use any other column too... Is there a way to count entries in groups without using a column?\n",
    "        FrequencyA500=('size_geom_mean', lambda x: (x>=500).sum()),  # using 'Site_name' here for count, could use any other column too... Is there a way to count entries in groups without using a column?\n",
    "        FrequencyB500=('size_geom_mean', lambda x: (x<500).sum()),  # using 'Site_name' here for count, could use any other column too... Is there a way to count entries in groups without using a column?\n",
    "        Mass=('Sampling_weight_[kg]', np.mean),  # using \"mean\" here is actually weird as all entries are the same. Is there something like \"first\"?\n",
    "        GPS_LONs = ('GPS_LON', np.mean),\n",
    "        GPS_LATs = ('GPS_LAT', np.mean),\n",
    "        Split = ('Fraction_analysed', np.mean),\n",
    "        MP_D50 = ('size_geom_mean',np.median)\n",
    "        ##MP_D50_A500 = ('size_geom_mean' >= 500.median()),\n",
    "        #MP_D50_B500 = ('size_geom_mean', lambda x: (x<500).median())\n",
    " ).reset_index()\n",
    "\n",
    "mp_station['Concentration'] =  round(mp_station['Frequency']/ (mp_station['Mass'] * mp_station['Split']))\n",
    "mp_station['ConcentrationA500'] =  round(mp_station['FrequencyA500']/ (mp_station['Mass'] * mp_station['Split']))\n",
    "mp_station['ConcentrationB500'] =  round(mp_station['FrequencyB500']/ (mp_station['Mass'] * mp_station['Split']))\n",
    "mp_station.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b155d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mp_poly_station = env_MP.groupby(['Sample','polymer_type']).agg(\n",
    "        Frequency=('Site_name', 'count'),  # using 'Site_name' here for count, could use any other column too... Is there a way to count entries in groups without using a column?\n",
    "        Mass=('Sampling_weight_[kg]', np.mean),  # using \"mean\" here is actually weird as all entries are the same. Is there something like \"first\"?\n",
    "        GPS_LONs = ('GPS_LON', np.mean),\n",
    "        GPS_LATs = ('GPS_LAT', np.mean),\n",
    "        Split = ('Fraction_analysed', np.mean)\n",
    " ).reset_index()\n",
    "\n",
    "mp_poly_station['Concentration'] =  round(mp_poly_station['Frequency']/ (mp_poly_station['Mass'] * mp_poly_station['Split']))\n",
    "\n",
    "mp_poly_station.head(1)#.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a55a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make plot for rel. Polymer type distribution per station\n",
    "\n",
    "Sample_order = ['Schlei_S1_15cm','Schlei_S2','Schlei_S3','Schlei_S5','Schlei_S8', 'Schlei_S10','Schlei_S10_15cm', 'Schlei_S11','Schlei_S13', 'Schlei_S14', 'Schlei_S15',\n",
    "       'Schlei_S17', 'Schlei_S19', 'Schlei_S22', 'Schlei_S23', 'Schlei_S24', 'Schlei_S25','Schlei_S26','Schlei_S27', 'Schlei_S30', 'Schlei_S31']\n",
    "\n",
    "selection = alt.selection_multi(fields=['polymer_type'], bind='legend')\n",
    "\n",
    "Poly_Dist = alt.Chart(mp_poly_station).mark_bar().encode(\n",
    "    x= alt.X('Sample',sort = Sample_order),#'polymer_type' == 'Polyamide',# , #'polymer_type' == \"Polyamide\", #df_new = df[df['Pid'] == 'p01']\n",
    "    y= alt.Y('Concentration'),\n",
    "    color= 'polymer_type',\n",
    "    tooltip = ['polymer_type', 'Concentration']\n",
    ").add_selection(\n",
    "    selection\n",
    ").transform_filter(\n",
    "    selection\n",
    "#).interactive(\n",
    ")\n",
    "\n",
    "Poly_Dist #| Poly_Dist.encode(y=alt.Y('Concentration',stack='normalize'))\n",
    "\n",
    "## look at specific polymer types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef6516f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import d50 values \n",
    "sed_d50 = pd.read_csv('../csv/Schlei_Sed_D50_new.csv',index_col=0)\n",
    "sed_63 = pd.read_csv('../csv/Schlei_Sed_D50_new.csv',index_col=0)\n",
    "\n",
    "#import organic matter size, TOC, Hg data\n",
    "sed_OM = pd.read_csv('../csv/Schlei_OM.csv',index_col=0)\n",
    "\n",
    "#import sampling log data\n",
    "slogs= pd.read_csv('../csv/Schlei_sed_sampling_log.csv',index_col=0)\n",
    "\n",
    "Dist_WWTP = pd.read_csv('../csv/Schlei_Sed_Dist_WWTP.csv',index_col=0)\n",
    "\n",
    "#merge with mp per station\n",
    "mp_sedStats = pd.merge(mp_station,slogs.reset_index(),on=['Sample'], how='left')\n",
    "mp_sedStats = pd.merge(mp_sedStats,sed_d50.reset_index(),on=['Sample'], how='left')\n",
    "mp_sedStats = pd.merge(mp_sedStats,sed_OM.reset_index(),on=['Sample'], how='left')\n",
    "mp_sedStats = pd.merge(mp_sedStats,Dist_WWTP.reset_index(),on=['Sample'], how='left')\n",
    "\n",
    "mp_sedStats.to_csv('../csv/MP_Stats_SchleiSediments.csv')\n",
    "mp_sedStats.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d5f984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dictionary\n",
    "Regio_Sep =  {'Schlei_S1_15cm': 'inner',\n",
    "              'Schlei_S2': 'inner',\n",
    "              'Schlei_S3': 'inner',\n",
    "              'Schlei_S5': 'river',\n",
    "              'Schlei_S8': 'inner',\n",
    "              'Schlei_S10': 'inner',\n",
    "              'Schlei_S10_15cm': 'inner',\n",
    "              'Schlei_S11': 'inner',\n",
    "              'Schlei_S13': 'inner',\n",
    "              'Schlei_S14': 'outlier',\n",
    "              'Schlei_S15': 'inner',\n",
    "              'Schlei_S17': 'inner',\n",
    "              'Schlei_S19': 'outlier',\n",
    "              'Schlei_S22': 'outer',\n",
    "              'Schlei_S23': 'outer',\n",
    "              'Schlei_S24': 'outer', \n",
    "              'Schlei_S25': 'outer',\n",
    "              'Schlei_S26': 'outer',\n",
    "              'Schlei_S27': 'outer', \n",
    "              'Schlei_S30': 'outer', \n",
    "              'Schlei_S31': 'outer'}\n",
    "\n",
    "mp_sedStats = mp_sedStats.merge(pd.DataFrame.from_dict(Regio_Sep,orient='index',columns=['Regio_Sep']),left_on='Sample',right_index=True)\n",
    "#mp_sedStats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccb7a14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MP_SED_Plot = alt.Chart(mp_sedStats).mark_point().encode(\n",
    "    x='D50',\n",
    "    y= alt.Y('Concentration',scale = alt.Scale(type= 'linear')),\n",
    "    color= 'Regio_Sep',\n",
    "    tooltip='Sample'\n",
    ")\n",
    "\n",
    "Reg_Line = MP_SED_Plot.transform_regression('D50', 'Concentration',\n",
    "                                            method=\"exp\",\n",
    "                                            groupby=[\"Regio_Sep\"],\n",
    "                                           ).mark_line(color=\"red\")\n",
    "\n",
    "Reg_Params = MP_SED_Plot.transform_regression('D50', 'Concentration',\n",
    "                                              method=\"pow\",\n",
    "                                              groupby=[\"Regio_Sep\"],\n",
    "                                              params=True                                             \n",
    "    ).mark_text(align='left', lineBreak='\\n').encode(\n",
    "        x=alt.value(120),  # pixels from left\n",
    "        y=alt.value(20),  # pixels from top\n",
    "        text='params:N'\n",
    "    ).transform_calculate(\n",
    "        params='\"r² = \" + round(datum.rSquared * 100)/100 + \\\n",
    "    \"      y = \" + round(datum.coef[0] * 10)/10 + \" + e ^ (\" + round(datum.coef[1] * 10000)/10000 + \"x\" + \")\" + \\n + \" \"'\n",
    "    )\n",
    "\n",
    "#ParamsDF = altair_transform.extract_data(Reg_Params)\n",
    "\n",
    "MP_SED_Plot + Reg_Line + Reg_Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50c35a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_MP = env_MP.merge(mp_sedStats[['Sample', 'TOC', 'Regio_Sep']], on='Sample')\n",
    "env_MP.rename(columns={'TOC': 'TOCs', 'Sampling_weight_[kg]': 'Sampling_weight'}, inplace=True)\n",
    "env_MP.drop(['Site_name', 'GPS_LON', 'GPS_LAT', 'Compartment',\n",
    "                      'Contributor', 'Project', 'Size_1_µm', 'Size_2_[µm]', 'Shape', 'Colour',\n",
    "                      'polymer_type', 'library_entry', 'lab_blank_ID', 'sample_ID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54fe906",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_size_limit = 0\n",
    "upper_size_limit = 1000\n",
    "\n",
    "brush = alt.selection(type='interval', encodings=['x'], resolve='global')\n",
    "\n",
    "PDFs = alt.Chart(\n",
    "    env_MP,\n",
    "#    width=100,\n",
    "#    height=80\n",
    ").transform_density(\n",
    "    'size_geom_mean', #Size_1_µm\n",
    "    extent=[lower_size_limit,upper_size_limit],\n",
    "    as_=['size','density'],\n",
    "    groupby=['Sample'],\n",
    "    steps=200,\n",
    "    bandwidth=50,\n",
    "    cumulative=False,\n",
    "    counts=False\n",
    ").mark_line().encode(\n",
    "    x='size:Q',\n",
    "    y=alt.Y('density:Q', stack=None),\n",
    "    color='Sample',\n",
    "    tooltip= ['Sample']\n",
    "#).facet(\n",
    "#    'Sample:N',\n",
    "#    columns=7\n",
    ").add_selection(\n",
    "    brush\n",
    ")\n",
    "\n",
    "MPconcTOC = alt.Chart(env_MP\n",
    ").transform_aggregate(\n",
    "    Frequency = 'count(*)',\n",
    "    Mass = 'mean(Sampling_weight)',\n",
    "    Split = 'mean(Fraction_analysed)',\n",
    "    TOC = 'mean(TOCs)',\n",
    "    groupby = ['Sample']\n",
    ").transform_calculate(\n",
    "    Conc = 'datum.Frequency / (datum.Mass * datum.Split)'\n",
    ").mark_point().encode(\n",
    "    x = 'TOC:Q',\n",
    "    y = 'Conc:Q',\n",
    "    color='Regio_Sep',\n",
    "    tooltip= ['Sample']\n",
    ").transform_filter(\n",
    "    brush\n",
    ")\n",
    "\n",
    "PDFs & MPconcTOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f65061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_bandwidth(data, kernel, bandwidths=10**np.linspace(0,2,100)):\n",
    "    grid = GridSearchCV(KernelDensity(kernel=kernel),\n",
    "                        {'bandwidth': bandwidths},\n",
    "                        cv=LeaveOneOut())\n",
    "    grid.fit(data[:, None]);\n",
    "    bw = grid.best_params_['bandwidth']\n",
    "    return bw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac881de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kde(data, x_d=np.linspace(0,999,1000), optimise_bw=True , kernel='gaussian'):  # data should be 1D np-array, x_d is the discrete values where the probability density is evaluated, bw is the bandwidth to be used for the kernels\n",
    "    \n",
    "    bw = optimise_bandwidth(data, kernel) if optimise_bw else 50\n",
    "    \n",
    "    # instantiate and fit the KDE model\n",
    "    kde = KernelDensity(bandwidth=bw, kernel=kernel)\n",
    "    kde.fit(data[:, None])\n",
    "    # score_samples returns the log of the probability density\n",
    "    logprob = kde.score_samples(x_d[:, None])\n",
    "    kde_result = np.exp(logprob)\n",
    "    \n",
    "    return kde_result, bw\n",
    "\n",
    "#plt.fill_between(x_d, kde_result, alpha=0.5)\n",
    "#plt.plot(x, np.full_like(x, -0.001), '|k', markeredgewidth=1)\n",
    "#plt.xlim(0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4de58e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = env_MP.loc[env_MP.Sample == 'Schlei_S10', 'size_geom_mean'].values\n",
    "\n",
    "kde_results = pd.DataFrame({'x_d': np.linspace(0,999,1000)})\n",
    "\n",
    "for SampleName, SampleGroup in env_MP.groupby(['Sample']):\n",
    "    x = SampleGroup.size_geom_mean.values\n",
    "    kde_result, bw = calculate_kde(x, optimise_bw=False)\n",
    "    \n",
    "    kde_results[SampleName] = kde_result\n",
    "    \n",
    "    print(f'{SampleName}:    bandwidth is {round(bw,2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a10f973",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(kde_results.melt(id_vars=['x_d'])).mark_line().encode(\n",
    "    x='x_d',\n",
    "    y='value',\n",
    "    color='variable',\n",
    "    tooltip='variable'\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e3ee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "slider_low = alt.binding_range(min=0, max=20000, step=10)\n",
    "select_low = alt.selection_single(name=\"lower_size\", fields=['size_geom_mean'],\n",
    "                                  bind=slider_low, init={'size_geom_mean': 0})\n",
    "slider_up = alt.binding_range(min=0, max=20000, step=10)\n",
    "select_up = alt.selection_single(name=\"upper_size\", fields=['size_geom_mean'],\n",
    "                                  bind=slider_up, init={'size_geom_mean': 20000})\n",
    "\n",
    "MPconcTOC = alt.Chart(env_MP\n",
    ").transform_aggregate(\n",
    "    Frequency = 'count(*)',\n",
    "    Mass = 'mean(Sampling_weight)',\n",
    "    Split = 'mean(Fraction_analysed)',\n",
    "    TOC = 'mean(TOCs)',\n",
    "    groupby = ['Sample']\n",
    ").transform_calculate(\n",
    "    Conc = 'datum.Frequency / (datum.Mass * datum.Split)'\n",
    ").mark_point().encode(\n",
    "    x = 'TOC:Q',\n",
    "    y = 'Conc:Q',\n",
    "    color='Regio_Sep'\n",
    ").add_selection(\n",
    "    select_low\n",
    ").add_selection(\n",
    "    select_up\n",
    ").transform_filter(\n",
    "    'datum.size_geom_mean >= lower_size_size_geom_mean'\n",
    ").transform_filter(\n",
    "    'datum.size_geom_mean <= upper_size_size_geom_mean'\n",
    ")\n",
    "\n",
    "MPconcTOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9f2f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "from vega_datasets import data\n",
    "source = data.iris()\n",
    "\n",
    "#brush = alt.selection(type='interval', encodings=['x'])\n",
    "slider_low = alt.binding_range(min=0, max=10, step=0.1)\n",
    "select_low = alt.selection_single(name=\"lowerSize\", fields=['sepalWidth'],\n",
    "                                  bind=slider_low, init={'sepalWidth': 0})\n",
    "slider_up = alt.binding_range(min=0, max=10, step=0.1)\n",
    "select_up = alt.selection_single(name=\"upperSize\", fields=['sepalWidth'],\n",
    "                                  bind=slider_up, init={'sepalWidth': 10})\n",
    "\n",
    "\n",
    "PDFs = alt.Chart(source\n",
    ").transform_density(\n",
    "    'sepalWidth',\n",
    "    as_=['size','density'],\n",
    "    groupby=['species']\n",
    ").mark_line().encode(\n",
    "    x='size:Q',\n",
    "    y='density:Q',\n",
    "    color='species'\n",
    "#).add_selection(\n",
    "#    brush\n",
    ")\n",
    "\n",
    "\n",
    "Scatter = alt.Chart(source\n",
    ").transform_aggregate(\n",
    "    Frequency = 'count()',\n",
    "    petalL_mean = 'mean(petalLength)',\n",
    "    petalW_mean = 'mean(petalWidth)',\n",
    "    sepalL_mean = 'mean(sepalLength)',\n",
    "    groupby = ['species']\n",
    ").transform_calculate(\n",
    "    Value = 'datum.Frequency / (datum.petalL_mean * datum.petalW_mean)'\n",
    ").mark_point().encode(\n",
    "    x = 'sepalWidth:Q',\n",
    "    y = 'sepalWidth:Q',\n",
    "    color='species'\n",
    ").add_selection(\n",
    "    select_low\n",
    ").add_selection(\n",
    "    select_up\n",
    ").transform_filter(\n",
    "    'datum.sepalWidth >= lowerSize_sepalWidth'\n",
    ").transform_filter(\n",
    "    'datum.sepalWidth <= upperSize_sepalWidth'\n",
    ")\n",
    "\n",
    "PDFs | Scatter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
