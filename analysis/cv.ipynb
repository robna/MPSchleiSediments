{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # ignore warnings to avoid flooding the gridsearch output with repetitive messages (works for single cpu)\n",
    "import os\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"  # ignore warnings to avoid flooding the gridsearch output with repetitive messages (works for parallel)\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.plotting.backend = 'holoviews'\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sklearn\n",
    "print(f'sklearn verion: {sklearn.__version__}')\n",
    "from sklearn.utils import parallel_backend\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "# from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "from sklearn.model_selection import GridSearchCV, LeavePOut, LeaveOneOut, cross_validate, cross_val_score, KFold, PredefinedSplit\n",
    "\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MaxAbsScaler, MinMaxScaler, OneHotEncoder, FunctionTransformer\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    AdaBoostRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    HistGradientBoostingRegressor,\n",
    ")\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor, RadiusNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import TweedieRegressor\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error, median_absolute_error, make_scorer\n",
    "\n",
    "from sklearn import clone, set_config\n",
    "# set_config(transform_output='pandas')  # only works for sklearn >= 1.2\n",
    "\n",
    "try:  # if on phy-server local modules will not be found if their directory is not added to PATH\n",
    "    import sys\n",
    "    sys.path.append(\"/silod7/lenz/MPSchleiSediments/analysis/\")\n",
    "    import os\n",
    "    os.chdir(\"/silod7/lenz/MPSchleiSediments/analysis/\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import prepare_data\n",
    "from components import PCOA\n",
    "#from helpers import PipelineHelper, SMWrapper\n",
    "from settings import Config, shortnames, target, featurelist\n",
    "from plots import scatter_chart\n",
    "from cv import generate_feature_sets, best_scored, get_median_cv_scores, get_iqm_cv_scores, SelectFeatures, median_absolute_percentage_error, iqm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% loading data\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# cell magic to supress output. Comment it out to see output of this cell.\n",
    "\n",
    "# What happened so far: DB extract and blank procedure. Now import resulting MP data from csv\n",
    "mp_pdd = prepare_data.get_pdd()\n",
    "\n",
    "# Also import sediment data (sediment frequencies per size bin from master sizer export)\n",
    "grainsize_iow, grainsize_cau = prepare_data.get_grainsizes()[0:2]\n",
    "scor_iow = PCOA(grainsize_iow, 2)[0]\n",
    "scor_cau = PCOA(grainsize_cau, 2)[0]\n",
    "\n",
    "# ...some data wrangling to prepare particle domain data and sample domain data for MP and combine with certain sediment aggregates.\n",
    "sdd_iow = prepare_data.aggregate_SDD(mp_pdd)\n",
    "sdd_iow = prepare_data.additional_sdd_merging(sdd_iow, how='outer')\n",
    "sdd_iow = sdd_iow.merge(scor_iow, right_index=True, left_on='Sample', how='outer')\n",
    "sdd_iow = sdd_iow.replace({'Sample': shortnames}).sort_values(by='Sample')\n",
    "\n",
    "sdd_cau = pd.read_csv('../data/Metadata_CAU_sampling_log.csv', index_col=0).join(prepare_data.fix_gradistat_names(pd.read_csv('../data/GRADISTAT_CAU_vol_log-cau_closed.csv', index_col=0)), how='outer')\n",
    "sdd_cau = sdd_cau.merge(scor_cau, right_index=True, left_on='Sample', how='outer').reset_index()\n",
    "# sdd_cau = geo.get_wwtp_influence(sdd_cau, tracks_file='../data/BAW_tracer_simulations.zip', file_postfix='_CAU')  # TODO: activate to get WWTP_influence features at CAU stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Split data into samples used for building the model and samples used for predicting.\n",
    "\n",
    "samples_with_response_and_predictor_data = sdd_iow.loc[~sdd_iow[target].isna()].set_index('Sample')\n",
    "samples_with_only_predictor_data = sdd_iow.loc[sdd_iow[target].isna()]\n",
    "samples_with_only_predictor_data = pd.concat([samples_with_only_predictor_data, sdd_cau.drop('Date',axis=1)]).set_index('Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Samples which are not suitable for (\"hydrodynamic outliers\") are moved from modelling data to prediction data\n",
    "droplist = ['S32','S05']\n",
    "samples_with_only_predictor_data = pd.concat([samples_with_only_predictor_data, samples_with_response_and_predictor_data.loc[droplist,:]])\n",
    "samples_with_response_and_predictor_data = samples_with_response_and_predictor_data.drop(droplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Limit dataframes to features (predictors) to be used in the model. Beware: depending on the preprocessing steps not all features might be used.\n",
    "model_X = samples_with_response_and_predictor_data[featurelist]\n",
    "model_y = samples_with_response_and_predictor_data[target]\n",
    "pred_X = samples_with_only_predictor_data[featurelist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined test set optionally to be used instead of CV:\n",
    "test_set = ('S30', 'S03', 'S15', 'S06', 'S31', 'S25', 'S20')  # possible samples to use as a predefined test set: ordered by relevance\n",
    "test_set_size = 7  # Requires int, should be 0 < test_set_size <= len(test_set), for using the n first samples as test_set\n",
    "test_set = test_set[0:test_set_size]\n",
    "test_set = model_X.index.isin(test_set).astype(int) - 1  # returns array of len(model_X.index) with -1 for training samples and 0 for testing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = None  # this is just needed for reporting as long as scaler is manually switched outside the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data using StandardScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# model_X.values[:] = scaler.fit_transform(model_X)\n",
    "# pred_X.values[:] = scaler.transform(pred_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Check some basic statistics of the target variable\n",
    "\n",
    "# model_y.describe()\n",
    "# model_y.hist()\n",
    "# model_X.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom preprocessing functions to be used in the model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create exhaustive feature selector, using leave-p-out on columns labels to generate a boolean matrix.\n",
    "num_feat = (2,2)  # allowed number of features:\n",
    "                  #     if int: all possible combinations of this length will be created\n",
    "                  #     if tuple (min, max): all possible combinations of length min upt to length max will be created \n",
    "feature_candidates_list = generate_feature_sets(model_X, Config.mutual_exclusive, Config.exclusive_keywords, num_feat=num_feat, n_jobs=1, save=True)\n",
    "\n",
    "CustomFeatureSelector = FunctionTransformer(SelectFeatures)#, feature_names_out='one-to-one')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Pipeline and parameter grid for model selection, see here for inspiration: https://towardsdatascience.com/getting-the-most-out-of-scikit-learn-pipelines-c2afc4410f1a\n",
    "\n",
    "PreProcessor = ColumnTransformer([\n",
    "      ('selector', CustomFeatureSelector, model_X.columns),\n",
    "      # ('imputer', SimpleImputer(), make_column_selector(dtype_include=np.number)),\n",
    "      # ('scaler', StandardScaler(), make_column_selector(dtype_include=np.number)),\n",
    "      # ('encoder', OneHotEncoder(), make_column_selector(dtype_include=object)),\n",
    "      ])\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('preprocessor', PreProcessor),\n",
    "    ('regressor', DummyRegressor())\n",
    "    ])\n",
    "\n",
    "preprocessor_params = [{\n",
    "    'preprocessor__selector': [CustomFeatureSelector],\n",
    "    'preprocessor__selector__kw_args': [{'feature_set': i,\n",
    "                                         'feature_sets': feature_candidates_list\n",
    "                                        } for i in range(len(feature_candidates_list))],\n",
    "    \n",
    "    # 'preprocessor__scaler': [StandardScaler()],#MaxAbsScaler(), MinMaxScaler(), RobustScaler(), QuantileTransformer(), Normalizer()],\n",
    "    #    'preprocessor__scaler__with_mean': [True],\n",
    "    #    'preprocessor__scaler__with_std': [True],\n",
    "    }]\n",
    "\n",
    "regressor_params = [\n",
    "    # {\n",
    "    #    'regressor': [DummyRegressor()],\n",
    "    #    'regressor__strategy': ['median'],\n",
    "    # },\n",
    "    \n",
    "    # {\n",
    "    # 'regressor': [SVR()],\n",
    "    #    'regressor__C': [0.1, 1, 1.5, 10, 20],\n",
    "    #    'regressor__kernel': ['linear', 'rbf', 'poly'],\n",
    "    #    'regressor__degree': [2, 3, 4, 5],\n",
    "    # },\n",
    "\n",
    "    # {\n",
    "    #  'regressor': [TweedieRegressor(max_iter=100000)],\n",
    "    #  'regressor__power': [2],\n",
    "    #   # 'regressor__power': [0, 1, 1.2, 1.5, 1.9, 2, 3],\n",
    "    #  'regressor__alpha': [0], \n",
    "    #  'regressor__link': ['log'],#, 'identity', 'auto'],\n",
    "    #   # 'regressor__fit_intercept': [True, False],\n",
    "    #   # 'regressor__warm_start': [True, False],\n",
    "    #  },\n",
    "  \n",
    "    # {\n",
    "    # 'regressor': [RadiusNeighborsRegressor()],\n",
    "    #     'regressor__radius': [1000, 10000, 100000],\n",
    "    #     'regressor__weights': ['uniform', 'distance'],\n",
    "    #     'regressor__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    #     'regressor__leaf_size': [10, 20, 30, 40, 50],\n",
    "    # },\n",
    "    \n",
    "    # {\n",
    "    # 'regressor': [RandomForestRegressor(random_state=np.random.RandomState(0))],\n",
    "    #   'regressor__n_estimators': [10, 100],\n",
    "    #   'regressor__max_depth': [None, 2, 4],\n",
    "    #   'regressor__max_features': [None, 1/3],\n",
    "    #  'regressor__min_samples_split': [2, 10],\n",
    "    #      'regressor__min_samples_leaf': [1, 3, 5],\n",
    "    #      'regressor__bootstrap': [True, False],\n",
    "    #      'regressor__oob_score': [True, False],\n",
    "    #      'regressor__warm_start': [True, False],\n",
    "    # },\n",
    "    \n",
    "    #{\n",
    "    # 'regressor': [GradientBoostingRegressor(random_state=np.random.RandomState(0))],\n",
    "    #     'regressor__loss': ['squared_error', 'huber', 'quantile'],\n",
    "    #     'regressor__learning_rate': [0.01, 0.1, 0.5],  \n",
    "    #     'regressor__n_estimators': [100, 200, 500],\n",
    "    #     'regressor__subsample': [0.5, 1.0],\n",
    "    #     'regressor__criterion': ['squared_error', 'friedman_mse'],\n",
    "    #     'regressor__min_samples_split': [2, 10],\n",
    "    #     'regressor__min_samples_leaf': [1, 5],\n",
    "    #     'regressor__max_depth': [2, 3, 5],\n",
    "    #     'regressor__min_weight_fraction_leaf': [0.0, 0.1],\n",
    "    #     'regressor__max_features': [None, 'sqrt', 'log2'],\n",
    "    #     'regressor__max_leaf_nodes': [None, 5, 10],\n",
    "    #     'regressor__min_impurity_decrease': [0.0, 0.1],\n",
    "    #     'regressor__min_impurity_split': [None, 0.1],\n",
    "    #     'regressor__alpha': [0.9, 0.95, 0.99, 0.999],\n",
    "    #     'regressor__tol': [0.0001, 0.001, 0.01],\n",
    "    #     'regressor__validation_fraction': [0.1, 0.2],\n",
    "    #     'regressor__n_iter_no_change': [None, 5, 10],\n",
    "    #     'regressor__ccp_alpha': [0.0, 0.1],\n",
    "    #     'regressor__warm_start': [True, False],\n",
    "    # },\n",
    "\n",
    "    # {\n",
    "    # 'regressor': [HistGradientBoostingRegressor(random_state=np.random.RandomState(0))],\n",
    "    #     'regressor__loss': ['squared_error', 'poisson', 'quantile'],\n",
    "    #     'regressor__quantile': [0.1, 0.5, 0.9],\n",
    "    #     'regressor__learning_rate': [0.01, 0.1, 0.5],  \n",
    "    #     'regressor__max_iter': [50, 100, 500],  \n",
    "    #     'regressor__max_depth': [2, 3, 5],\n",
    "    #     'regressor__min_samples_leaf': [1, 5],\n",
    "    #     'regressor__l2_regularization': [0.9, 0.95, 0.99, 0.999],\n",
    "    #     'regressor__max_bins': [None, 0.1],\n",
    "    #     'regressor__validation_fraction': [0.1, 0.2],\n",
    "    #     'regressor__tol': [0.0001, 0.001, 0.01],\n",
    "    #     'regressor__n_iter_no_change': [None, 5, 10],\n",
    "    # },\n",
    "\n",
    "    {\n",
    "    'regressor': [XGBRegressor(random_state=np.random.RandomState(0), verbosity = 1)],\n",
    "        'regressor__objective': ['reg:squarederror', 'reg:gamma', 'reg:tweedie'],\n",
    "        'regressor__booster': ['gblinear'],\n",
    "        'regressor__n_estimators': [100, 500],\n",
    "        'regressor__reg_alpha': [0.0, 0.1, 1, 10],\n",
    "        'regressor__reg_lambda': [0.0, 0.1, 1, 10],\n",
    "    },\n",
    "\n",
    "    # {\n",
    "    # 'regressor': [XGBRegressor(random_state=np.random.RandomState(0), verbosity = 0)],\n",
    "    #     'regressor__objective': ['reg:squarederror', 'reg:gamma', 'reg:tweedie'],\n",
    "    #     'regressor__booster': ['gbtree'],\n",
    "    #     'regressor__n_estimators': [100, 500], #\n",
    "    #     'regressor__tree_method': ['exact', 'hist'],\n",
    "    #     'regressor__grow_policy': ['depthwise', 'lossguide'],\n",
    "    #     'regressor__learning_rate': [0.05, 0.2], #alias for eta\n",
    "    # #     'regressor__min_split_loss': [0.0, 0.1], # alias for gamma\n",
    "    # #     'regressor__max_depth': [0, 1, 3, 6],\n",
    "    # #     'regressor__min_child_weight': [0.5, 1, 5],\n",
    "    # #     'regressor__subsample': [0.5, 0.8, 1.0],\n",
    "    # #     'regressor__colsample_bytree': [1/3, 0.5, 1.0],\n",
    "    # # #     'regressor__colsample_bylevel': [0.5, 1.0],\n",
    "    # # #     'regressor__colsample_bynode': [0.5, 1.0],\n",
    "    # #     'regressor__reg_alpha': [0.0, 0.1, 1],\n",
    "    # #     'regressor__reg_lambda': [0.0, 0.1, 1],\n",
    "    # },\n",
    "]\n",
    "\n",
    "params = [{**dict_pre, **dict_reg} for dict_reg in regressor_params for dict_pre in preprocessor_params]\n",
    "# params = regressor_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## The pipeline is run by searching the provided parameter space using scorings of a crossvalidation technique to find out how each model candidate performs.\n",
    "\n",
    "Config.scoring = {  # this dict is defined in settings.Config, but may be overwritten here for convenience\n",
    "    'R2': 'r2',\n",
    "    'MAPE': 'neg_mean_absolute_percentage_error',\n",
    "    'MedAE': 'neg_median_absolute_error',\n",
    "    'MedAPE': make_scorer(median_absolute_percentage_error, greater_is_better=False),\n",
    "    # 'MSLE': 'neg_mean_squared_log_error',\n",
    "  }\n",
    "Config.refit_scorer = 'R2'\n",
    "Config.select_best = 'iqm'  # 'mean', 'median'\n",
    "\n",
    "cv_scheme_inner = 10 #LeaveOneOut()\n",
    "cv_scheme_outer = 10 #LeaveOneOut()  # use `PredefinedSplit(test_set)` for a single fold with test set as defined above\n",
    "\n",
    "starttime = datetime.now()\n",
    "# with parallel_backend('loky', n_jobs=-1):\n",
    "innerCV = GridSearchCV(\n",
    "    pipe,\n",
    "    params,\n",
    "    scoring=Config.scoring,\n",
    "    refit=best_scored,\n",
    "    cv=cv_scheme_inner,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    "    )\n",
    "\n",
    "outerCV = cross_validate(\n",
    "    innerCV,\n",
    "    model_X,\n",
    "    model_y,\n",
    "    scoring=Config.scoring,\n",
    "    cv=cv_scheme_outer,\n",
    "    return_train_score=True,\n",
    "    return_estimator=True,\n",
    "    verbose=2,\n",
    "    # n_jobs=-1\n",
    "    )\n",
    "\n",
    "get_median_cv_scores(outerCV)\n",
    "get_iqm_cv_scores(outerCV)\n",
    "\n",
    "time_needed = datetime.now() - starttime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a df of all outer cv results and show it sorted by the best scoring metric\n",
    "outerCV_df = pd.DataFrame(outerCV)\n",
    "outerCV_df.rename_axis(index='outerCV_fold', inplace=True)\n",
    "\n",
    "## Get best model params for each of the outer cv folds:\n",
    "best_params_df = pd.DataFrame()\n",
    "for i, model in enumerate(outerCV['estimator']):\n",
    "    best_params = model.best_params_\n",
    "    # best_params_df = pd.concat([best_params_df, pd.DataFrame(best_params, index=[i])])  # this does not work when RandomForestRegressor is used, because some internals call len() on the values of the best_params dict, which raises AttributeError: 'RandomForestRegressor' object has no attribute 'estimators_'\n",
    "    # instead filling df with for-loop...:\n",
    "    current_best_params_df = pd.DataFrame()\n",
    "    for key, value in best_params.items():\n",
    "        current_best_params_df[key] = [value]\n",
    "        current_best_params_df.index = [i]\n",
    "    best_params_df = pd.concat([best_params_df, current_best_params_df])\n",
    "\n",
    "results = outerCV_df.join(best_params_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_summary = results.copy().drop(['preprocessor__selector', 'estimator', 'fit_time', 'score_time'], axis=1)\n",
    "\n",
    "# get names of features used by the models\n",
    "if 'preprocessor__selector__kw_args' in results.columns:\n",
    "    results_summary.rename(columns={'preprocessor__selector__kw_args': 'features'}, inplace=True)\n",
    "    s = results_summary.features.apply(lambda x: [x['feature_set'], feature_candidates_list[x['feature_set']]])\n",
    "    d = pd.DataFrame.from_dict(dict(zip(s.index, s.values))).T\n",
    "    results_summary.features, results_summary['feature_combi_ID'] = d[1], d[0]\n",
    "results_summary.drop(list(results_summary.filter(regex='regressor__')), axis=1, inplace=True)\n",
    "\n",
    "# calculate scores of the best model for each outer cv fold against all data\n",
    "scorer_dict = {\n",
    "    'R2': r2_score,\n",
    "    'MAPE': mean_absolute_percentage_error,\n",
    "    'MedAE': median_absolute_error,\n",
    "    'MedAPE': median_absolute_percentage_error\n",
    "}\n",
    "for key in Config.scoring:\n",
    "    results_summary[f'allSamples_{key}'] = [scorer_dict[key](model_y, outerCV['estimator'][i].predict(model_X)) for i in range(len(results_summary))]\n",
    "\n",
    "# now refit all models in outerCV on all data\n",
    "outerCV['estimator_refit_on_all'] = [clone(outerCV['estimator'][i].best_estimator_.named_steps['regressor']).fit(model_X[results_summary.features.loc[i]], model_y) for i, _ in enumerate(outerCV['estimator'])]\n",
    "\n",
    "# calculate scores against all data again after refitting\n",
    "for key in Config.scoring:\n",
    "    results_summary[f'allSamples_{key}_refit'] = [scorer_dict[key](model_y, outerCV['estimator_refit_on_all'][i].predict(model_X[results_summary.features.loc[i]])) for i in range(len(results_summary))]\n",
    "\n",
    "# Sort results\n",
    "results_summary.sort_values(by=f'test_{Config.refit_scorer}', ascending=False, inplace=True)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savestamp = starttime.strftime(\"%Y%m%d_%H%M%S\")\n",
    "fp = '../data/exports/models/'\n",
    "header = f'''\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Model started:;{savestamp}\n",
    "Duration:;{round(time_needed.seconds / 3600, 3)} h on {joblib.cpu_count()} cpu cores\n",
    "Outliers excluded:;{droplist}\n",
    "Samples:;{model_X.index.to_list()}\n",
    "Vertical merge:;{Config.vertical_merge}\n",
    "Scaler:;{scaler}\n",
    "Number of features per candidate (min, max):;{num_feat}\n",
    "Total number of feature combinations tested:;{len(feature_candidates_list)}\n",
    "Available features: {len(featurelist)}; {featurelist}\n",
    "\n",
    "Regressors:;{regressor_params}\n",
    "Scorer for evaluation (outer fold results are sorted by this!):;{Config.refit_scorer}\n",
    "Aggregation for evaluation:;{Config.select_best}\n",
    "\n",
    "CV schemes:\n",
    "    inner:;{cv_scheme_inner}\n",
    "    outer:;{cv_scheme_outer}\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Median outer CV test set {Config.refit_scorer}:; {np.median(results_summary[f'test_{Config.refit_scorer}'])}\n",
    "IQM outer CV test set {Config.refit_scorer}:; {iqm(results_summary[f'test_{Config.refit_scorer}'])}\n",
    "Mean outer CV test set {Config.refit_scorer}:; {np.mean(results_summary[f'test_{Config.refit_scorer}'])}\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "'''\n",
    "\n",
    "# Save model settings\n",
    "fn = fp + 'model_NCV_result.csv'\n",
    "with open(fn, mode='a' if Path(fn).exists() else 'w', encoding='utf-8') as f:\n",
    "    f.write(header)\n",
    "\n",
    "# Save results\n",
    "results_summary.to_csv(fn, mode='a', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top k results of inner CV sorted by best Config.refit_scorer \n",
    "\n",
    "# outer_fold = 0  # manuaklly chose which outer fold to look at\n",
    "# top_k = 10  # how many model candidates from the inner model to show?\n",
    "# pd.DataFrame(outerCV['estimator'][outer_fold].cv_results_).sort_values(f'rank_by_median_test_{Config.refit_scorer}', ascending=True).head(top_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing score of the best performing model candidate and its parameters.\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "model = outerCV['estimator_refit_on_all'][outer_fold]\n",
    "print(best_params_df.loc[outer_fold, 'regressor'])\n",
    "print(f'R2 of model retrained with all samples, tested against all samples: {r2_score(model_y, model.predict(model_X[results_summary.features.loc[outer_fold]]))}')\n",
    "# if model was TweedieRegressor, print alpha and power\n",
    "if 'Tweedie' in str(type(best_params_df.loc[outer_fold, 'regressor'])):\n",
    "    print(f'Intercept: {model.intercept_}')\n",
    "    print(f'Coeffs: {model.coef_}')\n",
    "print(feature_candidates_list[best_params_df.loc[outer_fold, 'preprocessor__selector__kw_args']['feature_set']])\n",
    "# print(f'{Config.scoring[Config.refit_scorer]}: {outerCV[\"estimator\"][outer_fold].score(model_X, model_y)}')  \n",
    "#print(outerCV['estimator'][outer_fold].best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Number of coefficients in each outer folds best model: ', [outerCV['estimator'][i].best_estimator_.named_steps['regressor'].n_features_in_ for i in range(len(outerCV['estimator']))])\n",
    "# print('Coeffs: ', *[outerCV['estimator'][i].best_estimator_.named_steps['regressor'].coef_ for i in range(len(outerCV['estimator']))], sep='\\n')\n",
    "# results.estimator[0].best_estimator_.named_steps['preprocessor'].transformers[0][1].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [outerCV['estimator'][0].best_estimator_.named_steps['regressor'].estimators_[i].get_n_leaves() for i in range(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results['estimator'].apply(lambda x: x.score(model_X, model_y))\n",
    "# pd.DataFrame.from_dict(dict(zip(s.values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r2_all_but_no_refit = results['estimator'].apply(lambda x: r2_score(model_y, x.predict(model_X)))\n",
    "# r2_all_but_no_refit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get inner cv results of the outer cv fold which achieved the best scoring metric\n",
    "# innerCV_df = pd.DataFrame(outerCV_df.loc[outerCV_df[f'test_score'].idxmax(), 'estimator'].cv_results_)\n",
    "# innerCV_df.sort_values(by=f'rank_test_{scoring[0]}', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outerCV['estimator'][0].best_estimator_.named_steps['preprocessor'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting feature names\n",
    "# [grid.best_estimator_.named_steps['preprocessor'].named_transformers_['selector'].get_feature_names_out(input_features=model_X.columns.tolist())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The chosen final model\n",
    "\n",
    "- evaluation by cross-validation\n",
    "- prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate this block to use same features, model type and hyperparameters a specific outer fold\n",
    "#outer_fold = 1\n",
    "#final_model_X = model_X[results_summary.features.loc[outer_fold]]\n",
    "#final_model = outerCV['estimator_refit_on_all'][outer_fold]\n",
    "\n",
    "# Alternatively activate this block to use a manually defined model\n",
    "final_model_X = model_X[['PC1', 'Depth', 'WWTP_influence_as_mean_time_travelled__sed_18Âµm_allseasons_444']]\n",
    "final_model = TweedieRegressor(alpha=0.1, link='log', max_iter=100000, power=1.5, tol=0.0001)\n",
    "    \n",
    "# for loop to run cross validation on the final model with leave-p-out iterating p from 2 to 10\n",
    "allR2s = {}\n",
    "allMedianR2s = {}\n",
    "allMeanR2s = {}\n",
    "R2calcDuration = {}\n",
    "for P in range(2, 7):\n",
    "    starttime = datetime.now()\n",
    "    finalR2s = cross_val_score(final_model, final_model_X, model_y, cv=LeavePOut(P), scoring='r2', n_jobs=-1, verbose=0)\n",
    "    duration = datetime.now() - starttime\n",
    "    print(f'Cross-validation of final model with leave-{P}-out took {duration.seconds//3600} hours, {(duration.seconds//60)%60} minutes and {duration.seconds%60} seconds.')\n",
    "    print(f'Cross-validated MEAN R2 of final model: {finalR2s.mean():.3f} (Standard deviation: {finalR2s.std():.3f})')\n",
    "    print(f'Cross-validated MEDIAN R2 of final model: {np.median(finalR2s):.3f} (IQR {np.subtract(*np.percentile(finalR2s, [.75, .25])):.3f})')\n",
    "    print()\n",
    "    allR2s[P] = finalR2s\n",
    "    allMedianR2s[P] = np.median(finalR2s)\n",
    "    allMeanR2s[P] = finalR2s.mean()\n",
    "    R2calcDuration[P] = duration\n",
    "\n",
    "final_model.fit(final_model_X, model_y)\n",
    "print(f'R2 of final model retrained with all samples, tested against all samples: {r2_score(model_y, final_model.predict(final_model_X))}')\n",
    "\n",
    "# Predictions\n",
    "#pred_X = pred_X[model_X.columns.tolist()]\n",
    "#pred_y = final_model.predict(pred_X)\n",
    "df1 = pd.DataFrame(zip(model_y, final_model.predict(final_model_X), ['IOW']*len(model_y)), index=model_X.index, columns=[target, 'predicted', 'group'])\n",
    "#df2 = pd.DataFrame(zip([0]*len(pred_y), pred_y, ['CAU']*len(pred_y)), index=pred_X.index, columns=[target, 'predicted', 'group'])\n",
    "df = pd.concat([df1])#, df2])\n",
    "\n",
    "scatter_chart(df.reset_index(), target, 'predicted', 'group', labels='Sample', identity=True, equal_axes=True, width=800, height=800, title='yhat vs. y')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine dicts 'allMeanR2s' and 'allMedianR2s' into a dataframe\n",
    "final_scores_df = pd.DataFrame.from_dict(allMeanR2s, orient='index', columns=['Mean R2']).join(pd.DataFrame.from_dict(allMedianR2s, orient='index', columns=['Median R2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores_df.plot.line(title='Mean and median R2 of final model with leave-p-out cross-validation', xlabel='p', ylabel='R2', width=800, height=400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9283b71a9fb260974dcb1b3d32e79d34b5da211280b6af559776f5f59ea44de2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
